id,Name,Score,Model version,Shots,Source,Source link,Notes
rec2xXtaYsETkAIic,,70.0%,mpt-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
reczTSlKRTLtNSENB,ChatGLM2,54.3%,chatglm2-6b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec7doikBryvY92fX,,67.0%,internlm-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
reclSAZPhymLQvV01,,71.8%,internlm-20b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recsrCl1BLlwQCvo0,XVerse,48.2%,,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recYHhnjZgZIXdZsp,,73.3%,Baichuan-2-7B-Base,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recNL96PBikW3geVY,,74.0%,Baichuan-2-13B-Base,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recXxy43JUGK2jPzL,,73.3%,LLaMA-7B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recWRJ7s4P8mK4b3y,,75.2%,LLaMA-13B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec1P4zL04qfPtmrX,,77.2%,LLaMA-33B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recfGUja0cWwBFXaA,,77.7%,LLaMA-65B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recU0DvIGj0Cx7FUc,,73.3%,Llama-2-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rechMH4HPGuohVfC6,,76.5%,Llama-2-13b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recoEvP3kjsU0RhfV,,78.9%,Llama-2-70b-hf ,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recaCNICEk1UnsEbl,,71.3%,StableBeluga2,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recC7agcItthoBTMw,,58.4%,Qwen-1_8B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rechdx8dakBhmNq77,,67.9%,Qwen-7B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recNTDzKb9IjM2007,,71.1%,Qwen-14B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recqoZjJbdXrF2PwC,GLaM (MoE) 0.1B/64E,41.4%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recby3N2kRZeuMQlq,GLaM (MoE) 1.7B/64E,63.7%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recwEt4RvCBSpNHc7,GLaM (MoE) 8B/64E,67.3%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recm72dsLSpBhD686,GLaM (MoE) 64B/64E,64.2%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recKxbD6So5wI5RpC,GLaM (Dense) 0.1B,37.8%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rectyMNzZ73ekSEpx,GLaM (Dense) 1.7B,60.1%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recrN7m2feVNAvJwh,GLaM (Dense) 8B,69.3%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recXiTnJTbpaZIHJf,GLaM (Dense) 137B,70.9%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recl6DERB2pCXr7Qm,GPT3 175B,76.2%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recTkq6GtFWtLIJbc,GLaM (MoE) 0.1B/64E,36.9%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recYvRgs3RIectytK,GLaM (MoE) 1.7B/64E,57.4%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recYyfhRzSGZRLjkf,GLaM (MoE) 8B/64E,64.1%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recc9tH2r1uYpwqoB,GLaM (MoE) 64B/64E,80.9%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recNmhCHM0OADXHm1,GLaM (Dense) 0.1B,21.8%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recTNlGCoJjsgUHuF,GLaM (Dense) 1.7B,52.3%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rechkVoiO7q7tAe60,GLaM (Dense) 8B,64.7%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recsoWWbAhIUwXAgw,GLaM (Dense) 137B,68.5%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recJMbsvJvQxBVWGL,GPT-3 (175B),72.5%,text-davinci-001,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rec3x2a9YCcrwIEYd,GLaM (MoE) 0.1B/64E,36.9%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
reclp1vVVIvkEJIxf,GLaM (MoE) 1.7B/64E,64.3%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recfzfHehI3kdkLoK,GLaM (MoE) 8B/64E,79.0%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recD1qk2bGjjgZkik,GLaM (MoE) 64B/64E,86.6%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recFGGbVEHE2m561T,GLaM (Dense) 0.1B,21.8%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recQKYZNOKPzVtQRc,GLaM (Dense) 1.7B,63.0%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recn0Nina4jBT63Q0,GLaM (Dense) 8B,77.1%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recxi0JTs60mpjDWE,GLaM (Dense) 137B,84.2%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recKkMcRrA3QYrqcl,GPT-3 (175B),86.4%,text-davinci-001,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recam11tENdCcJOOT,GPT-3         ,76.2%,text-davinci-001,0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recnJL8PNsKurWZQJ,Gopher        ,74.5%,Gopher (280B),0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
rectiOvDVbt6MvVp1,MT-NLG (ours) ,76.6%,Megatron-Turing NLG 530B,0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
rec9y9ui3cTNMI7fL,GPT-3         ,72.5%,text-davinci-001,1,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recsoiPkr7WH9pRDy,MT-NLG (ours) ,73.1%,Megatron-Turing NLG 530B,1,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
rec0YBtMZPv6PvP3L,GPT-3         ,86.4%,text-davinci-001,few,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
reckRUXCbqhiNITwm,MT-NLG (ours) ,87.2%,Megatron-Turing NLG 530B,few,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
rec6TGPjtqdj0scyj,Gopher (k-Shot),74.5%,Gopher (280B),0,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/abs/2112.11446,
recu9bbbL7HC6GIN7,MPT 7B             ,70.0%,mpt-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec27gUWwQKDPiwAW,ChatGLM2 6B        ,54.3%,chatglm2-6b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recHfcdJyV48YnXwf,InternLM 7B        ,67.0%,internlm-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recLdEoATQGGlAACE,InternLM 20B       ,71.8%,internlm-20b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recSrXtswfaU3egAZ,XVERSE 13B         ,48.2%,,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recIy3Bb5HqxiMKfB,Baichuan2 7B       ,73.3%,Baichuan-2-7B-Base,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recJIddONftE4eWpk,Baichuan2 13B      ,74.0%,Baichuan-2-13B-Base,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec874oZp2QZs6G61,LLaMA 7B           ,73.3%,LLaMA-7B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recgQDXaNeJ8VWV3x,LLaMA 13B          ,75.2%,LLaMA-13B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recSSkgQnDpJ9KDZ5,LLaMA 33B          ,77.2%,LLaMA-33B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recKARMKetCVFKMuN,LLaMA 65B          ,77.7%,LLaMA-65B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recrz4vp9TeQDMijg,LLAMA 2 7B         ,73.3%,Llama-2-7b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recVe2Wo6SWeUqwfw,LLAMA 2 13B        ,76.5%,Llama-2-13b,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec0O43m1j1P0bghs,LLAMA 2 70B        ,78.9%,Llama-2-70b-hf ,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec4vNMLVeNultNFD,StableBeluga2 70B  ,71.3%,StableBeluga2,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recgB9JIq6NldBDrd,QWEN 1.8B          ,58.4%,Qwen-1_8B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec4HYw9Hi7O5f0C3,QWEN 7B            ,67.9%,Qwen-7B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recn5IxI2fxa7AR5f,QWEN 14B           ,71.1%,Qwen-14B,,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recO5w0E8iGLF03SH,GPT-3,76.2%,text-davinci-001,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
reci74pJGLLVqHXJt,Gopher,74.5%,Gopher (280B),,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recs8hCOMtPRefESy,Chinchilla,77.4%,Chinchilla (70B),,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recdQcFMIIooSeC1P,MT-NLG,76.6%,Megatron-Turing NLG 530B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recbH5Bhjf8PSH14k,PaLM,77.9%,PaLM 540B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recvjENyQBR54sD6E,Inflection-1,78.5%,Inflection-1,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recnSUBDS2uY8d8BI,Falcon 7B,74.9%,falcon-7b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec7WtVRmVKLEnZmN,Falcon 40B,77.3%,falcon-40b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recr1kUAaOEhwPRGb,Falcon 180B,79.8%,falcon-180B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,