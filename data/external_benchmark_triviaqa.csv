id,Name,Model version,EM,Shots,Notes,Source,Source link
recTmyjydTD3zEmrN,Claude 2,claude-2.0,87.5%,5,,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf
rec2o8utHtyekSr2n,Claude instant,,78.9%,5,,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf
recLawKW7ayTST6hX,GPT-4-0613,gpt-4-0613,84.8%,,,PapersWithCode,https://paperswithcode.com/sota/question-answering-on-triviaqa
recqOhSu3mm8qtu8u,Claude 1.3,claude-1.3,86.7%,5,,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf
rec2eB4hK2TZtrSFg,PaLM 2-L,PaLM 2-L,86.1%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recSXmx6Jt3xz96O1,LLaMA 2 70B,Llama-2-70b-hf ,85.0%,1,,Llama 2: Open Foundation and Fine-Tuned Chat Models,https://arxiv.org/pdf/2307.09288v2
reczS1Jlz0DzFtnq1,PaLM 2-M,PaLM 2-M,81.7%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recjb3O3PZmDHTwPt,PaLM-540B,PaLM 540B,76.9%,0,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recrfwOWCS1C7TstM,PaLM-540B,PaLM 540B,81.4%,1,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recQGR1k0JWfvm4YO,PaLM-540B,PaLM 540B,81.4%,few,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
rec1H51pEEne0IdGs,PaLM 2-S,PaLM 2-S,75.2%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
rech95qTb390zRaA1,GPT-3,text-davinci-001,71.2%,few,,Language Models are Few-Shot Learners,https://arxiv.org/pdf/2005.14165v4
rec80FIbMEIFtngNq,GPT-3,text-davinci-001,68.0%,1,,Language Models are Few-Shot Learners,https://arxiv.org/pdf/2005.14165v4
recjbZ5iX41fMNKTS,GPT-3,text-davinci-001,64.3%,0,,Language Models are Few-Shot Learners,https://arxiv.org/pdf/2005.14165v4
recLZAYwaYmir4fkG,Gemma 2B,gemma-2b,53.2%,5,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recUuVZqNjnF8nZ5p,Gemma 7B,gemma-7b,63.4%,5,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
rec3x5YjE2OF68lfJ,LLaMA-2 7B,Llama-2-7b,72.1%,5,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recFncBMpsvN2mFPe,LLaMA-2 13B,Llama-2-13b,79.6%,5,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recvyWDwCUBlqF0P5,DeepSeek-V2 Base,DeepSeek-V2,80.0%,5,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
rec53JZuaKLqzEwju,Qwen2.5 72B Base,Qwen2.5-72B,71.9%,5,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
rec9b6hIEDSUXwyOk,DeepSeek-V3 Base,DeepSeek-V3,82.9%,5,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recXplA8GrPMhfKW1,Phi-3-mini,Phi-3-mini-4k-instruct,64.0%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recXHoTSIBIjrb2hr,Phi-3-small,Phi-3-small-8k-instruct,58.1%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recLjsOhngDpVoxi4,Phi-3-medium,,73.9%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recJZ6lcpMRWGlYfC,Phi-2,phi-2,45.2%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
rec9QQGCXZgygnfBz,GPT-3 (175B),text-davinci-001,64.3%,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recpfngtEfxU1D6zc,GLaM,GLaM (MoE),71.3%,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recEreiw1NzbvJYEK,GPT-3 (175B),text-davinci-001,68.0%,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rec5nxJnKiiqNIm4u,GLaM,GLaM (MoE),75.8%,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recPrYWrsvsjKbhpk,GPT-3,text-davinci-001,71.2%,64,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recTPeBzo4moFIytC,Gopher,Gopher (280B),57.1%,64,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recMagk4MHW8hCgKS,GLaM,GLaM (MoE),75.8%,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recTEzt7DQe7MTvSz,GPT-3,text-davinci-001,64.3%,0,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recbxIZpvmvJV4tOs,GPT-3,text-davinci-001,68.0%,1,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recNaxMPrhoe9tttx,GPT-3,text-davinci-001,71.2%,few,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165