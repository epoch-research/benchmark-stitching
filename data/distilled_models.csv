model,distilled,confidence,notes
amazon.nova-pro-v1:0,FALSE,low,"They also have some ""micro"" models. Maybe the difference is that this isn't distilled but those are. "
Baichuan-2-13B-Base,FALSE,high,"""trained from scratch"" in paper for both sizes, no distillation mentioned"
Baichuan-2-7B-Base,FALSE,high,"""trained from scratch"" in paper for both sizes, no distillation mentioned"
Baichuan-7B,FALSE,high,
Cerebras-GPT-13B,FALSE,high,"Pre-trained from scratch, no distllation mentioned"
chatglm2-6b,FALSE,low,
claude-2.0,FALSE,low,
claude-2.1,FALSE,low,
claude-3-5-haiku-20241022,TRUE,low,
claude-3-5-sonnet-20240620,FALSE,high,https://www.darioamodei.com/post/on-deepseek-and-export-controls
claude-3-5-sonnet-20241022,FALSE,medium,
claude-3-haiku-20240307,TRUE,low,
claude-3-opus-20240229,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-3-sonnet-20240229,TRUE,low,
claude-instant-1.2,TRUE,low,"Their old ""instant"" models were meant for low latency high throughput cases, so probably smaller, distilled versions"
claude-opus-4-1-20250805,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-opus-4-1-20250805_16K,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-opus-4-1-20250805_27K,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-opus-4-20250514,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-opus-4-20250514_16K,FALSE,medium,"Assumed, given its size relative to rest of series"
claude-sonnet-4-20250514,TRUE,low,
claude-sonnet-4-20250514_16K,TRUE,low,
claude-sonnet-4-20250514_32K,TRUE,low,
claude-sonnet-4-5-20250929,TRUE,low,
CodeQwen1.5-7B,FALSE,medium,No mention of distillation. some synthetic data but probably not that much
deepseek-coder-1.3b-base,FALSE,medium,Described as trained from scratch https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base 
deepseek-coder-33b-base,FALSE,medium,Described as trained from scratch https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base 
deepseek-coder-6.7b-base,FALSE,medium,Described as trained from scratch https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base 
DeepSeek-Coder-V2-Lite-Base,FALSE,medium,
DeepSeek-R1,FALSE,high,"""Specifically, we use DeepSeek-V3-Base as the base model"", see DeepSeek V3 entry, no distillation mentioned"
DeepSeek-R1-0528,FALSE,high,No distillation mentioned since original version
DeepSeek-R1-Distill-Llama-70B,TRUE,high,
DeepSeek-V2,FALSE,high,
DeepSeek-V3,FALSE,high,
DeepSeek-V3-0324,FALSE,high,
DeepSeek-V3.1,FALSE,high,
dolly-v2-12b,FALSE,low,
falcon-11b,FALSE,high,Training described
falcon-180B,FALSE,high,Training described
falcon-40b,FALSE,high,Training described
falcon-7b,FALSE,high,Training described
gemini-1.5-flash-001,TRUE,medium,https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/
gemini-1.5-flash-002,TRUE,medium,
gemini-1.5-pro-001,FALSE,medium,"Assumed, given its size relative to rest of series"
gemini-1.5-pro-002,FALSE,medium,"Assumed, given its size relative to rest of series"
gemini-2.0-flash-001,TRUE,medium,
gemini-2.0-flash-exp,TRUE,medium,
gemini-2.0-flash-thinking-exp-01-21,TRUE,medium,
gemini-2.0-pro-exp-02-05,FALSE,medium,"Assumed, given its size relative to rest of series"
gemini-2.5-flash-preview-04-17,TRUE,medium,
gemini-2.5-flash-preview-05-20,TRUE,medium,
gemini-2.5-pro-exp-03-25,FALSE,medium,Assumed
gemini-2.5-pro-preview-03-25,FALSE,medium,"Assumed, given its size relative to rest of series"
gemini-2.5-pro-preview-05-06,FALSE,medium,"Assumed, given its size relative to rest of series"
gemini-2.5-pro-preview-06-05,FALSE,medium,"Assumed, given its size relative to rest of series"
gemma-2-27b-it,FALSE,high,We also train the 2B and 9B models with knowledge distillation
gemma-2-9b-it,TRUE,high,We also train the 2B and 9B models with knowledge distillation
gemma-2b,TRUE,high,Training described
gemma-3-27b-it,TRUE,high,"""The Gemma 3 models are trained with distillation"""
gemma-7b,FALSE,high,Training described
gpt-3.5-turbo-0613,FALSE,low,
gpt-3.5-turbo-1106,FALSE,low,
gpt-4-0314,FALSE,medium,
gpt-4-0613,FALSE,medium,
gpt-4-turbo-2024-04-09,FALSE,low,
gpt-4.1-2025-04-14,FALSE,medium,
gpt-4.1-mini-2025-04-14,TRUE,medium,
gpt-4.1-nano-2025-04-14,TRUE,medium,
gpt-4.5-preview-2025-02-27,FALSE,high,"Assumed, given its size relative to rest of series"
gpt-4o-2024-05-13,FALSE,low,
gpt-4o-2024-08-06,FALSE,low,
gpt-4o-2024-11-20,FALSE,low,
gpt-4o-mini-2024-07-18,TRUE,medium,
gpt-5-2025-08-07_high,FALSE,low,
gpt-5-2025-08-07_medium,FALSE,low,
gpt-5-mini-2025-08-07_high,TRUE,medium,Speculated
gpt-5-mini-2025-08-07_medium,TRUE,medium,Speculated
gpt-5-nano-2025-08-07_high,TRUE,medium,Speculated
gpt-5-nano-2025-08-07_medium,TRUE,medium,Speculated
gpt-neo-2.7B,FALSE,high,Traning described
grok-2-1212,FALSE,medium,Speculated
grok-3-beta,FALSE,medium,Speculated
grok-3-mini-beta_high,TRUE,medium,
grok-3-mini-beta_low,TRUE,medium,
grok-4-0709,FALSE,medium,"Assumed, given its size relative to rest of series"
Inflection-1,FALSE,low,
internlm-20b,FALSE,high,No mention of distillation in report
internlm-7b,FALSE,high,No mention of distillation in report
Kimi-K2-Instruct,FALSE,high,
LLaMA-13B,FALSE,high,Training described
Llama-2-13b,FALSE,high,
Llama-2-34b,FALSE,high,
Llama-2-70b-hf,FALSE,high,
Llama-2-7b,FALSE,high,
Llama-3.1-405B,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
Llama-3.1-405B-Instruct,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
Llama-3.1-70B-Instruct,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
Llama-3.1-8B-Instruct,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
Llama-3.2-90B-Vision-Instruct,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
Llama-3.3-70B-Instruct,FALSE,medium,"Assumed to be similar to Llama 3, the paper for which does not mention distillation"
LLaMA-33B,FALSE,high,Training described
Llama-4-Maverick-17B-128E-Instruct,TRUE,high,codistillation
Llama-4-Maverick-17B-128E-Instruct-FP8,TRUE,high,codistillation
Llama-4-Scout-17B-16E-Instruct,TRUE,medium,
LLaMA-65B,FALSE,high,Training described
LLaMA-7B,FALSE,high,Training described
Meta-Llama-3-70B-Instruct,FALSE,high,No mention of distillation in report
Meta-Llama-3-8B-Instruct,FALSE,high,No mention of distillation in report
Mistral-7B-v0.1,FALSE,high,
mistral-large-2407,FALSE,low,
mistral-large-2411,FALSE,low,
mistral-medium-2505,FALSE,low,
mistral-small-2501,FALSE,low,
mistral-small-2503,FALSE,low,
Mixtral-8x7B-v0.1,FALSE,high,No mention of distillation in report
mpt-30b,FALSE,medium,No obvious indication
mpt-7b,FALSE,medium,No obvious indication
Nemotron-4 15B,FALSE,high,No mention of distillation in report
o1-2024-12-17_high,FALSE,low,
o1-2024-12-17_medium,FALSE,low,
o1-mini-2024-09-12_high,TRUE,medium,Speculated
o1-mini-2024-09-12_medium,TRUE,medium,Speculated
o1-preview-2024-09-12,FALSE,low,
o3-2025-04-16_high,FALSE,low,
o3-2025-04-16_medium,FALSE,low,
o3-mini-2025-01-31_high,TRUE,medium,Speculated
o3-mini-2025-01-31_medium,TRUE,medium,Speculated
o4-mini-2025-04-16_high,TRUE,medium,Speculated
o4-mini-2025-04-16_medium,TRUE,medium,Speculated
open_llama_7b,FALSE,medium,No obvious indication
PaLM 2-L,FALSE,high,No mention of distillation in report
PaLM 2-M,FALSE,high,No mention of distillation in report
PaLM 2-S,FALSE,high,No mention of distillation in report
phi-1_5,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
phi-2,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
Phi-3-medium-128k-instruct,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
Phi-3-mini-4k-instruct,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
Phi-3-small-8k-instruct,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
phi-4,TRUE,high,"Training on synthetic data, but not knowledge distillation? Still messes up the compute relationship though."
Qwen-1_8B,FALSE,high,No mention of distillation in report
Qwen-14B,FALSE,high,No mention of distillation in report
Qwen-7B,FALSE,high,No mention of distillation in report
qwen-max-2025-01-25,FALSE,low,
qwen2-72b-instruct,FALSE,low,
Qwen2.5-72B,FALSE,high,No mention of distillation in report
qwen2.5-72b-instruct,FALSE,high,Training described
Qwen2.5-Coder-0.5B,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
Qwen2.5-Coder-1.5B,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
Qwen2.5-Coder-14B,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
Qwen2.5-Coder-32B,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
qwen2.5-coder-3b,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
Qwen2.5-Coder-7B,TRUE,low,"There's this one step they call ""distillation method"" but it doesn't seem like distillation in the traditional sense"
Qwen2.5-VL-72B-Instruct,TRUE,low,"""human preference distillation"" in post-training"
qwen3-235b-a22b,FALSE,medium,Distillation of smaller models from this one mentioned in paper
RedPajama-INCITE-7B-Base,FALSE,low,
StableBeluga2,FALSE,low,"uses ""synthetically-generated dataset"" for SFT, but probably kinda minimal?"
stablelm-tuned-alpha-7b,FALSE,medium,No obvious indication
starcoder2-15b,FALSE,medium,No mention of distillation in report
starcoder2-3b,FALSE,medium,No mention of distillation in report
starcoder2-7b,FALSE,medium,No mention of distillation in report
text-davinci-003,FALSE,medium,
vicuna-13b-v1.1,FALSE,low,Paper describes training on ChatGPT conversations but just as fine-tuning
xgen-7b-8k-base,FALSE,low,Paper describes training on ChatGPT conversations but just as fine-tuning
Yi-34B-Chat,FALSE,high,No mention of distillation in report
Yi-6B,FALSE,high,No mention of distillation in report
Yi-9B,FALSE,high,No mention of distillation in report