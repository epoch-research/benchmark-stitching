id,Name,Score,Model version,Shots,Source,Source link,Notes
rec1xnVLr7TTueuek,,78.8%,Llama-2-7b,0,Gemma: Open Models Based on Gemini Research and Technology,http://arxiv.org/abs/2403.08295,
recIN14fg5Z6cb0Dy,,80.5%,Llama-2-13b,0,Gemma: Open Models Based on Gemini Research and Technology,http://arxiv.org/abs/2403.08295,
recwGYCRlmPQfK60l,Mistral 7B,82.2%,Mistral-7B-Instruct-v0.2,0,Gemma: Open Models Based on Gemini Research and Technology,http://arxiv.org/abs/2403.08295,
recJxRvElA0WHGRBp,,77.3%,gemma-2b,0,Gemma: Open Models Based on Gemini Research and Technology,http://arxiv.org/abs/2403.08295,
recjJz03ZH5MPEkVX,,81.2%,gemma-7b,0,Gemma: Open Models Based on Gemini Research and Technology,http://arxiv.org/abs/2403.08295,
reczbCPMBK72FaE5G,,80.6%,mpt-7b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recOqznsYbiFQfhfo,,76.7%,falcon-7b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recMuGIrZwsS5fFvR,ChatGLM2,69.6%,chatglm2-6b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rec8tnNTcy4oGb1kc,,77.9%,internlm-7b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
reccHVW9au1hhKQdJ,,80.3%,internlm-20b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recZ5llwYoXPLlK4z,,76.2%,Baichuan-7B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recPOD8pH0jFEUrRY,Baichuan2 13B,78.1%,Baichuan-2-13B-Base,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recHMsC136QYPMTA3,,79.8%,LLaMA-7B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recXKa35srTYRSqea,,80.1%,LLaMA-13B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recXN4J1Jsj6CbzV2,,82.3%,LLaMA-33B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recwOATQe7kYtlwDp,,82.8%,LLaMA-65B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
rect8NgxFMui2UBT9,,78.8%,Llama-2-7b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recK6uOl6du0fRDlX,,80.5%,Llama-2-13b,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recoepOMA4acXl4mG,,82.8%,Llama-2-70b-hf ,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recmTF7ULiFtmEP58,,83.3%,StableBeluga2,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recRKZzksugdKpDPC,,73.3%,Qwen-1_8B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recGbe9gweaWdaQek,,77.9%,Qwen-7B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recrb6Z8mgHAvK4WU,,79.9%,Qwen-14B,0,Qwen Technical Report,https://arxiv.org/abs/2309.16609,
recwRkBtx8A7fYhDl,GLaM (MoE) 0.1B/64E,70.0%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recB75GmJ7xdPOy9x,GLaM (MoE) 1.7B/64E,76.9%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recHfAFhoIkf5dq5z,GLaM (MoE) 8B/64E,78.6%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recEnLnhRPbek1CJ6,GLaM (MoE) 64B/64E,80.4%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recjTYlinXrCfTjzk,GLaM (Dense) 0.1B,64.4%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recPDJnWWDFAqjjlk,GLaM (Dense) 1.7B,73.6%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
reczZpxKfckhYoJ1T,GLaM (Dense) 8B,78.2%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rec1n5dW7M0s4nyct,GLaM (Dense) 137B,78.5%,,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recsYI4pESAvZyS6M,GPT3 175B,80.4%,text-davinci-001,0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recQ90kggeQFiypxU,GLaM (MoE) 0.1B/64E,69.0%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recMT7hL3uLEj0xbm,GLaM (MoE) 1.7B/64E,76.0%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rec35NzWH0GYGi92u,GLaM (MoE) 8B/64E,78.1%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recwTAOCHSLss5cUe,GLaM (MoE) 64B/64E,81.4%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recoi5YMDIgywEDBw,GLaM (Dense) 0.1B,63.7%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recuv70Khd0OGV5tx,GLaM (Dense) 1.7B,73.1%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rec9hBy4dBNfS6YB8,GLaM (Dense) 8B,76.3%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rechqgvwEoMCpU7OM,GLaM (Dense) 137B,79.5%,,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recH9xsP3Rbt6iIYp,GPT-3 (175B),80.5%,text-davinci-001,1,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recqAppx1NcjhhX4T,GLaM (MoE) 0.1B/64E,69.0%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recwfAK4nLJrYathu,GLaM (MoE) 1.7B/64E,76.1%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rects737oDQVD1Pkf,GLaM (MoE) 8B/64E,78.1%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
receHLAsg9DZiWVhL,GLaM (MoE) 64B/64E,81.8%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rect2I1dAOPCLstud,GLaM (Dense) 0.1B,64.2%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recFtdznxZpvC1Pl1,GLaM (Dense) 1.7B,73.1%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
rec2NrSw87YxnU8rl,GLaM (Dense) 8B,77.0%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recz3hchBvxIiEuZM,GLaM (Dense) 137B,80.8%,,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recmMgB5NbBUgeVIA,GPT-3 (175B),82.3%,text-davinci-001,few,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905,
recPqTKLkju5qZgJl,Phi-3.5-mini,81.0%,Phi-3.5-mini-instruct,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
rec6rmWvma2wXHWfl,Phi-3.5-MoE,88.6%,Phi-3.5-MoE-instruct,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
reclUJMIrRD6JaeFJ,Mistral 7B,73.4%,Mistral-7B-v0.1,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
recg6Csx6Pm68gfKA,Mistral-Nemo 12B,83.5%,Mistral-Nemo-Base-2407,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
recMRkMxBSeChUEeH,Llama-3.1-In 8B,81.2%,Llama-3.1-8B-Instruct,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
recwF8TNC8B3AT1J3,Gemma-2 9B,83.7%,gemma-2-9b,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
recgEUyXX8JGSCqol,Gemini-1.5 Flash,87.5%,gemini-1.5-flash-002,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
recGz1DuA5ylHICeB,GPT-4o-mini,88.7%,gpt-4o-mini-2024-07-18,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,
rechyypf7vUAiL6zd,GPT-3,81.0%,text-davinci-001,0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recNGhvAQW5IgDxpV,GPT-3,80.5%,text-davinci-001,1,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recjUybr3sNM10801,GPT-3,82.3%,text-davinci-001,few,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recHKitk77SUFNClZ,Gopher,81.8%,Gopher (280B),0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recC47h8xcapcI419,MT-NLG,82.0%,Megatron-Turing NLG 530B,0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
reccY8xsB8E7wx6VM,MT-NLG,81.0%,Megatron-Turing NLG 530B,1,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recQ4y5CR1XQQvNwM,MT-NLG,83.2%,Megatron-Turing NLG 530B,few,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990,
recCDyTiNQ3ihLhEw,DeepSeek-V2 Base ,83.9%,DeepSeek-V2,0,DeepSeek-V3 Technical Report,http://arxiv.org/abs/2412.19437,
recjoWxRpfZ7AQ7yl,Qwen2.5 72B Base,82.6%,Qwen2.5-72B,0,DeepSeek-V3 Technical Report,http://arxiv.org/abs/2412.19437,
recuGgadfFBqsMZOZ,LLaMA-3.1 405B Base,85.9%,Llama-3.1-405B,0,DeepSeek-V3 Technical Report,http://arxiv.org/abs/2412.19437,
recrLJI80qifq9QOn,DeepSeek-V3 Base,84.7%,DeepSeek-V3-Base,0,DeepSeek-V3 Technical Report,http://arxiv.org/abs/2412.19437,
recMGhvVkVNFWIxCR,Gopher,81.8%,Gopher (280B),0,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/abs/2112.11446,
recSmuagktvqatZ8Z,MPT 7B,80.6%,mpt-7b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recr2NM4gHz5DrPHK,MPT 30B,81.9%,mpt-30b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recy3uYhorC17XHuA,Falcon 7B,76.7%,falcon-7b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recRvbbjcqu4tHZUj,Falcon 40B,82.4%,falcon-40b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recqCeWBr36yASU0m,LLAMA 1 7B,79.8%,LLaMA-7B,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
rec55TMC1fSIbgbpL,LLAMA 1 13B,80.1%,LLaMA-13B,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recN2M5oOIzl0ODKF,LLAMA 1 33B,82.3%,LLaMA-33B,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recHgNAhYhVSOtBCE,LLAMA 1 65B,82.8%,LLaMA-65B,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recneSbTW5OeVjC1z,LLAMA 2 7B,78.8%,Llama-2-7b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recgjPq9v01M3wvTo,LLAMA 2 13B,80.5%,Llama-2-13b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recwvOiQUvfWl8wKI,LLAMA 2 34B,81.9%,Llama-2-34b,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
rechTvbEmR1HjwNVJ,LLAMA 2 70B,82.8%,Llama-2-70b-hf ,,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,
recsVbigtLpMbL6Qh,GPT-3 175B,81.0%,text-davinci-001,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
rec9o0a54dAKBr38y,Gopher 280B,81.8%,Gopher (280B),,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
recpuR9za4WmoSvTQ,Chinchilla 70B,81.8%,Chinchilla (70B),,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
recFicEkz7OwkVWpx,PaLM 62B,80.5%,PaLM 62B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
rectzHjogpUoYAQeW,PaLM-cont 62B,81.4%,,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
reciBD1ZNJdnqq65W,PaLM 540B,82.3%,PaLM 540B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
rec9VBfbQaSSc1u22,LLaMA 7B,79.8%,LLaMA-7B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
recl6sqNICNc9xkt9,LLaMA 13B,80.1%,LLaMA-13B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
recJX0keNAJ7aeMtF,LLaMA 33B,82.3%,LLaMA-33B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
reczKzFKydRHC3Y6G,LLaMA 65B,82.8%,LLaMA-65B,,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,
rec9pWZJyyC5cl7TQ,LLaMA 2 7B     ,77.9%,Llama-2-7b,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
recCgq5kWGQ8RBlfe,LLaMA 2 13B    ,80.8%,Llama-2-13b,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
recysRWV09WKs2EW7,LLaMA 1 33B    ,82.2%,LLaMA-33B,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
recF6goVanGsXhTuS,LLaMA 2 70B    ,82.6%,Llama-2-70b-hf ,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
recUMmAunynmFGxLQ,Mistral 7B     ,82.2%,Mistral-7B-Instruct-v0.1,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
rec2IWUndAuaG0h2n,Mixtral 8x7B   ,83.6%,Mixtral-8x7B-v0.1,,Mixtral of Experts,http://arxiv.org/abs/2401.04088,
recahE13EYTo3Wo1s,LLaMA-2 13B,79.8%,Llama-2-13b,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
rectIlFXJFhTHN5i7,LLaMA-2 34B,81.9%,Llama-2-34b,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
recmlOtDLYkLVNRjp,Baichuan-2 13B,78.1%,Baichuan-2-13B-Base,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
recBuu3qDHZsvO1PL,QWEN 14B,79.9%,Qwen-14B,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
recuKX9gOnTp5HRYM,Mistral 7B,83.0%,Mistral-7B-v0.1,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
recQvIlDdbnoPnv0d,Gemma 7B,81.2%,gemma-7b,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
rectxE1O7JQfuz9EJ,Nemotron-4 15B,82.4%,Nemotron-4 15B,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,
recHgkx12MmzkekLN,Vicuna-13B,77.4%,vicuna-13b-v1.1,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recaUCCpOTwgqm0Qm,Llama2-7B,78.1%,Llama-2-7b,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recsdhbPxzYLRRtjg,Llama-7B,77.9%,LLaMA-7B,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recwVckykBEQi1xoV,MPT-7B,78.9%,mpt-7b,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recb2fIZ2HfaPbGiN,Falcon-7B,79.4%,falcon-7b,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
rec3JaqZd42oD33Pb,Falcon-rw-1.3B,74.7%,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recj4wPV6UBqzmQPj,OPT-1.3B,69.0%,opt-1.3b,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
rec1GQU0OQPWk0ZHw,GPT-Neo-2.7B,72.9%,gpt-neo-2.7B,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
recUOcFFT7hhkGKpZ,GPT2-XL-1.5B,70.5%,gpt2-xl,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,
rec25UCR5WZroWYCB,GPT-3,81.0%,text-davinci-001,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec5VmQW9uyUN1SdY,Gopher,81.8%,Gopher (280B),,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recqTziBWCcXeSuRY,Chinchilla,81.8%,Chinchilla (70B),,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recqMqP0JxMAF9zPY,MT-NLG,82.0%,Megatron-Turing NLG 530B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recyNe5nwU6obKot5,PaLM,82.3%,PaLM 540B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recGIEisHrwAWU2xV,LLaMA-2 7B,78.8%,Llama-2-7b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recnLKoW7dq4Du1kv,LLaMA-2 13B,80.5%,Llama-2-13b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recyvBe2DxXox2gja,LLaMA-2 34B,81.9%,Llama-2-34b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec4hpg6dNmNaM2ao,LLaMA-2 70B,82.8%,Llama-2-70b-hf ,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec82aLV2vuRBCF2f,Inflection-1,84.2%,Inflection-1,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec079w7XJrd01GOg,Falcon 7B,80.3%,falcon-7b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recQxJmvpV3eZ20Lc,Falcon 40B,83.0%,falcon-40b,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
recksRN6FGoGt2lU1,Falcon 180B,84.9%,falcon-180B,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,
rec5iHfuuirY3jGxV,UNICORN,90.1%,,,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,https://ojs.aaai.org/index.php/AAAI/article/view/17590,Explicitly fine-tuned on it
rectQ5UXB5ekumh3q,XGen-7B,75.5%,xgen-7b-8k-base,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recp7DP6kLNhpXBvf,LLaMA-7B,78.7%,LLaMA-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recA3hoQGJKtLCEcS,Falcon-7B,79.4%,falcon-7b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recZRYbGGqawjK4fq,MPT-7B,79.1%,mpt-7b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
rec0RNZG538uttbyz,OpenLLaMA-7B,76.0%,open_llama_7b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recfbMlV6oYreReFy,Redpajama-7B,76.9%,RedPajama-INCITE-7B-Base,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
rec8Sh8crarIde2aF,GPT-neox-20B,76.7%,gpt-neox-20b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
rec34MBtTjoOBqvsy,OPT-13B,75.7%,opt-13b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recieMBRnl774Kp8o,GPT-J-6B,75.4%,gpt-j-6b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recmpn9dENCquDkxR,Dolly-v2-12B,75.4%,dolly-v2-12b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recNaQ21yNcdLdcSw,Cerebras-GPT-13B,73.5%,Cerebras-GPT-13B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,
recSBbD5ut1zFgBlR,StableLM-alpha-7B,65.8%,stablelm-tuned-alpha-7b,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,