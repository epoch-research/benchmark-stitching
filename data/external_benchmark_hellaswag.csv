id,Name,Overall accuracy,Model version,Shots,Notes,Source,Source link
recPk9HXP3iqyzA10,GPT-4,95.3%,gpt-4-0314,,,HellaSwag official leaderboard,https://rowanzellers.com/hellaswag/
recDP6AFPqvGUU7ko,PaLM-540B,83.4%,PaLM 540B,0,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recZj2LOfHy2dxU3Y,PaLM-540B,83.6%,PaLM 540B,1,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recMqMvo2ZR5xB7Td,PaLM-540B,83.8%,PaLM 540B,few,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
reclJ23a6dlbZmII4,PaLM 2-S,82.0%,PaLM 2-S,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
rec1xIDMlT4yARh9N,PaLM 2-M,84.0%,PaLM 2-M,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recPWo0FVBOyy6UQL,PaLM 2-L,86.8%,PaLM 2-L,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recjYC87LW5FsDXJe,LLaMA 7B,76.1%,LLaMA-7B,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recltSGk7UqgYJQGD,LLaMA 13B,79.2%,LLaMA-13B,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recSk7gas28ZeRmQV,LLaMA 33B,82.8%,LLaMA-33B,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recbhMjETLxddNfTo,LLaMA 65B,84.2%,LLaMA-65B,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recKJQ9e6G5eR4LY9,Qwen2.5-Coder-0.5B,48.4%,Qwen2.5-Coder-0.5B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recJ6JRT9b7jqkffL,Qwen2.5-Coder-1.5B,61.8%,Qwen2.5-Coder-1.5B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recIljwSiqroHuB4D,Qwen2.5-Coder-3B,70.9%,Qwen2.5-Coder-3B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
rec7vcmFtc4HEyNvG,Qwen2.5-Coder-7B,76.8%,Qwen2.5-Coder-7B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recJnaCHTOREy7sKU,Qwen2.5-Coder-14B,80.2%,Qwen2.5-Coder-14B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recxMCtl8GTjOanHW,Qwen2.5-Coder-32B,83.0%,Qwen2.5-Coder-32B,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
rec5L8vbrflxgqG9Y,Mixtral 8x7B,86.7%,Mixtral-8x7B-v0.1,10,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
recvCEZxPk8kXUffy,Mistral 7B,81.0%,Mistral-7B-v0.1,,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
receQE7wX2SUXU5kj,Mixtral 8x7B,84.4%,Mixtral-8x7B-v0.1,,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
rec1YZRNwjqtYEQgE,Gemma 7B,82.2%,gemma-7b,,Run with HuggingFace H6 suite,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
reczjwjnz8igKpnEi,Gemma 7B,81.2%,gemma-7b,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recBqQ9CYxvNxJKHn,Gemma 2B,71.4%,gemma-2b,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recLoCMBOHteyqSFV,LLaMA-2 7B,77.2%,Llama-2-7b,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
rec3jliHk20I9fwvl,LLaMA-2 13B,80.7%,Llama-2-13b,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recIDwVCV7xiAPtWR,DeepSeek-V2 Base,87.1%,DeepSeek-V2,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recwpbI0o0B1hbdpF,Qwen2.5 72B Base,84.8%,Qwen2.5-72B,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recOisJQ9AhmUw58Z,LLaMA-3.1 405B Base,89.2%,Llama-3.1-405B,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recnADYV42QsRB24p,DeepSeek-V3 Base,88.9%,DeepSeek-V3,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recC009XHETCkqHuq,phi-1.5,47.6%,phi-1_5,0,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recrDBkh4IHJOxHX3,Yi-6B,74.4%,Yi-6B,10,,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652
recrHT1sajTQ2TT4C,Yi-9B,76.4%,Yi-9B,,,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652
recNf0HzKB7GCWezm,Falcon 7B,76.3%,falcon-7b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
reciBfJ5AKzP8yAl6,Falcon 40B,82.8%,falcon-40b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
reclyVPS1rMuSndsk,Falcon 7B,78.1%,falcon-7b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recuz33sIq3QXEcxm,Falcon 40B,85.3%,falcon-40b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recIC1QoBZPGnA5tT,Falcon2-11B stage 4,82.1%,falcon-11b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recag3Lz0AvhGRmS4,Falcon2-11B stage 4,82.9%,falcon-11b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recGmteOirEpRk74U,Falcon-180B,89.0%,falcon-180B,10,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867
recDZLIX5tZdFHfZV,GPT-3.5,85.5%,text-davinci-003,10,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867
recOyxFNOYBvgCzru,GPT-4,95.3%,gpt-4-32k-0314,10,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867
recxJ90xKTkaaA1JI,Nemotron-4 15B,82.4%,Nemotron-4 15B,0,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819
recMJYP7H2VFfBaU5,Phi-3-mini,76.7%,Phi-3-mini-4k-instruct,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
rechnntOSjcnmoV5r,Phi-3-small,77.0%,Phi-3-small-8k-instruct,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recY158Aob3my3MXx,Phi-3-medium,82.4%,Phi-3-medium-128k-instruct,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
rec6Wjhlpz51HwZAb,Phi-2,53.6%,phi-2,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recdvddfEJCDhen1x,GPT-3 (175B),78.9%,text-davinci-001,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rec2jRUCp1AFzsN31,GLaM,76.6%,GLaM (MoE),0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recr3mOusR0K1TilG,GPT-3 (175B) ,78.1%,text-davinci-001,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recSe1QPYCHRdaNts,GLaM,76.8%,GLaM (MoE),1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recc3BHdP90Z1fI20,GPT-3 (175B),79.3%,text-davinci-001,20,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rec1fTXYPgL0mRzBe,Gopher (280B),79.2%,Gopher (280B),0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recb8yDVZF6TqwUZI,Megatron-NLG (530B),82.4%,Megatron-Turing NLG 530B,,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rece76lEt406di379,GLaM,77.2%,GLaM (MoE),8,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recw7hL34oyte0hO4,GPT-3 Zero-Shot,78.9%,text-davinci-001,0,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recLlYcFFQ2fkEaY4,GPT-3 One-Shot,78.1%,text-davinci-001,1,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
rec9hsfppA15xBC1Z,GPT-3 Few-Shot,79.3%,text-davinci-001,few,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recJnSsj1yJW0DpOz,MPT 7B,76.4%,mpt-7b,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recUsvn4nbRtm8w4V,Falcon 7B,74.1%,falcon-7b,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recGt5AcRHqUhevHc,ChatGLM2,57.0%,chatglm2-6b,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recystu4Gc11FUp5X,InternLM 7B,70.6%,internlm-7b,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recKfpodrVZGjl5CL,InternLM 20B,78.1%,internlm-20b,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
rec6q2wSPdc9L6x40,Baichuan2 7B,68.0%,Baichuan-2-7B-Base,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recKMHyLo7iHhIm9m,Baichuan2 13B,70.8%,Baichuan-2-13B-Base,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recX2iDmTM7fmBQcr,LLAMA 2 70B,85.3%,Llama-2-70b-hf ,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recRGvz0NvjpFvSSH,StableBeluga2,84.1%,StableBeluga2,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recaAsccv7Ypq42Vf,text-davinci-003,82.2%,text-davinci-003,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rechG6XtFFAfGaxk8,text-davinci-002,81.5%,text-davinci-002,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recjgw0ZOlh0jtyMY,Cohere xlarge v20220609 (52.4B),81.1%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recwaK0tFnDh9Tqzy,Cohere Command beta (52.4B),81.1%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recVxIk0J15SihM0z,Cohere xlarge v20221108 (52.4B),81.0%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recREHiAS7YYjRck5,Anthropic-LM v4-s3 (52B),80.7%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recxjdjjCdbqbpobG,TNLG v2 (530B),79.9%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recedbGBvF8R6FFVE,OPT (175B),79.1%,opt-175b,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recKfVK9KIvexAGQe,Jurassic-2 Jumbo (178B),78.8%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recysGdQck0k4NcMH,Jurassic-2 Grande (17B),78.1%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recbOlkritQTwwoUh,davinci (175B),77.5%,davinci,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recypK2SzPuf4pjl2,J1-Jumbo v1 (178B),76.5%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recuz2tek0qkudllk,J1-Grande v2 beta (17B),76.4%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rechxO8XYnPKqrhSQ,Cohere Command beta (6.1B),75.2%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recevvZp5GvZqnhTY,OPT (66B),74.5%,opt-66b,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recQSVqDaut5Qbuj2,BLOOM (176B),74.4%,bloom,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recSQwyBphOwlyxbx,J1-Grande v1 (17B),73.9%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
reccslyJuclpjphLN,Cohere large v20220720 (13.1B),73.6%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec91lq7ePZtc52ZO,Jurassic-2 Large (7.5B),72.9%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec0XBvVEvE3wM4iL,Cohere medium v20221108 (6.1B),72.6%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recwi6H4QDJ5RnKhl,GPT-NeoX (20B),71.8%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recDDdululYn98qLK,Cohere medium v20220720 (6.1B),70.6%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
reccuWvUpVMjCYrR8,TNLG v2 (6.7B),70.4%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recOLH1fId6Xyd8sA,J1-Large v1 (7.5B),70.0%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recKp5z9jwr9Zh0Ts,curie (6.7B),68.2%,curie,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recYhmf2CsNqDxs5a,text-curie-001,67.6%,text-curie-001,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recgEHai1fuGSnQ6U,GPT-J (6B),66.3%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recxrPuCbT16sDZrG,text-babbage-001,56.1%,text-babbage-001,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec9GDlYjK3e9FkXR,babbage (1.3B),55.5%,babbage,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recpkpM7Bs6dFms3C,Cohere small v20220720 (410M),48.3%,,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recfeLKZvBng82A20,ada (350M),43.5%,ada,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recNkQMe0Nqd3deYK,text-ada-001,42.9%,text-ada-001,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recnKx5JEOw8tZlbl,GPT-3,78.9%,text-davinci-001,0,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recCvPkcxVrRq6YCL,GPT-3,78.1%,text-davinci-001,1,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recsTSf13JcXIzyWp,GPT-3,79.3%,text-davinci-001,few,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recwxUQUprmMr9I8j,Gopher,79.2%,Gopher (280B),0,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
reclD9XvCzARYAaJD,MT-NLG,80.2%,Megatron-Turing NLG 530B,0,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recOS1vYKc0O63Nm9,MT-NLG,80.2%,Megatron-Turing NLG 530B,1,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recChavgsXvRqIfA4,MT-NLG,82.4%,Megatron-Turing NLG 530B,few,,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/pdf/2201.11990
recci6ceGcEr7HPnk,Vicuna-13B,57.8%,vicuna-13b-v1.1,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
rec18ziKAqmhepn9d,Llama2-7B,57.1%,Llama-2-7b,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recIgJOQCzWeRSNU8,Llama-7B,56.2%,LLaMA-7B,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recTjndtio3wpkQtJ,MPT-7B,57.1%,mpt-7b,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recAFpIJDF6mgmPuv,Falcon-7B,54.2%,falcon-7b,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recjNOzd1oubTWRwz,Falcon-rw-1.3B,46.6%,,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recX8gW8qBbzDWMF3,OPT-1.3B,41.5%,opt-1.3b,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recdUHL8y6j09NCyI,GPT-Neo-2.7B,42.7%,gpt-neo-2.7B,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
rechzRZkyb0rIGBL4,GPT2-XL-1.5B,40.0%,gpt2-xl,,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
reczFRmY0w4zEXGr0,Falcon 7B,76.3%,falcon-7b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recaRUn4YSr3AcqiP,Falcon 40B,82.8%,falcon-40b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recsqpRPEKmC1nrEh,Falcon2-11B stage 4,82.1%,falcon-11b,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recls5hl8NYpOc7i3,Falcon 7B,78.1%,falcon-7b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recya8LRIunACDqxv,Falcon 40B,85.3%,falcon-40b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recQ4BBxuAOaopnCC,Falcon2-11B stage 4,82.9%,falcon-11b,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recGg30WxShdoma6K,GPT-4,95.3%,gpt-4-0314,10,,GPT-4 Technical Report,http://arxiv.org/abs/2303.08774
recSoaSkn6hHNrdDG,GPT-3.5,85.5%,text-davinci-003,10,,GPT-4 Technical Report,http://arxiv.org/abs/2303.08774
recuFfA1xvh9vebqC,UNICORN,86.6%,,,,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,https://ojs.aaai.org/index.php/AAAI/article/view/17590
recy1P9FZOSqOxfEa,XGen-7B,74.2%,xgen-7b-8k-base,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recLOfEzuA2K0aFdy,LLaMA-7B,76.2%,LLaMA-7B,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recmVOgkewjlqKh8h,Falcon-7B,76.4%,falcon-7b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recjgraDh1VdRw31V,MPT-7B,76.1%,mpt-7b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recqC3Si74hlV1FmN,OpenLLaMA-7B,71.8%,open_llama_7b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recd9h5DECjR1gnE7,Redpajama-7B,70.3%,RedPajama-INCITE-7B-Base,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recKBrB9BpH26k9lE,GPT-neox-20B,70.5%,gpt-neox-20b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
reczB7H8nIbkL9Jpr,OPT-13B,69.9%,opt-13b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
reckIFCQgW9j4dxIA,GPT-J-6B,66.2%,gpt-j-6b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
recvtwJTHfsxN0xBD,Dolly-v2-12B,70.8%,dolly-v2-12b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
rec2aapgpGAXpxWQ6,Cerebras-GPT-13B,59.4%,Cerebras-GPT-13B,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450
reczrfJheZMsfglky,StableLM-alpha-7B,40.7%,stablelm-tuned-alpha-7b,0,,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450