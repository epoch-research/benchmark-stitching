id,Name,Model version,Overall accuracy,Shots,Notes,Source,Source link
recPk9HXP3iqyzA10,GPT-4,gpt-4-0314,95.3%,,,HellaSwag official leaderboard,https://rowanzellers.com/hellaswag/
recDP6AFPqvGUU7ko,PaLM-540B,PaLM 540B,83.4%,0,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recZj2LOfHy2dxU3Y,PaLM-540B,PaLM 540B,83.6%,1,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
recMqMvo2ZR5xB7Td,PaLM-540B,PaLM 540B,83.8%,few,,PaLM: Scaling Language Modeling with Pathways,https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf
reclJ23a6dlbZmII4,PaLM 2-S,PaLM 2-S,82.0%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
rec1xIDMlT4yARh9N,PaLM 2-M,PaLM 2-M,84.0%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recPWo0FVBOyy6UQL,PaLM 2-L,PaLM 2-L,86.8%,1,,PaLM 2 Technical Report,https://arxiv.org/pdf/2305.10403
recjYC87LW5FsDXJe,LLaMA 7B,LLaMA-7B,76.1%,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recltSGk7UqgYJQGD,LLaMA 13B,LLaMA-13B,79.2%,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recSk7gas28ZeRmQV,LLaMA 33B,LLaMA-33B,82.8%,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recbhMjETLxddNfTo,LLaMA 65B,LLaMA-65B,84.2%,0,,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/pdf/2302.13971
recKJQ9e6G5eR4LY9,Qwen2.5-Coder-0.5B,Qwen2.5-Coder-0.5B,48.4%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recJ6JRT9b7jqkffL,Qwen2.5-Coder-1.5B,Qwen2.5-Coder-1.5B,61.8%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recIljwSiqroHuB4D,Qwen2.5-Coder-3B,Qwen2.5-Coder-3B,70.9%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
rec7vcmFtc4HEyNvG,Qwen2.5-Coder-7B,Qwen2.5-Coder-7B,76.8%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recJnaCHTOREy7sKU,Qwen2.5-Coder-14B,Qwen2.5-Coder-14B,80.2%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
recxMCtl8GTjOanHW,Qwen2.5-Coder-32B,Qwen2.5-Coder-32B,83.0%,,,Qwen2.5-Coder Technical Report,https://arxiv.org/pdf/2409.12186
rec5L8vbrflxgqG9Y,Mixtral 8x7B,Mixtral-8x7B-v0.1,86.7%,10,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
recvCEZxPk8kXUffy,Mistral 7B,Mistral-7B-v0.1,81.0%,,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
receQE7wX2SUXU5kj,Mixtral 8x7B,Mixtral-8x7B-v0.1,84.4%,,,Mixtral of Experts,https://arxiv.org/pdf/2401.04088
rec1YZRNwjqtYEQgE,Gemma 7B,gemma-7b,82.2%,,Run with HuggingFace H6 suite,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
reczjwjnz8igKpnEi,Gemma 7B,gemma-7b,81.2%,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recBqQ9CYxvNxJKHn,Gemma 2B,gemma-2b,71.4%,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recLoCMBOHteyqSFV,LLaMA-2 7B,Llama-2-7b,77.2%,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
rec3jliHk20I9fwvl,LLaMA-2 13B,Llama-2-13b,80.7%,0,,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295
recIDwVCV7xiAPtWR,DeepSeek-V2 Base,DeepSeek-V2,87.1%,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recwpbI0o0B1hbdpF,Qwen2.5 72B Base,Qwen2.5-72B,84.8%,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recOisJQ9AhmUw58Z,LLaMA-3.1 405B Base,Llama-3.1-405B,89.2%,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recnADYV42QsRB24p,DeepSeek-V3 Base,DeepSeek-V3,88.9%,10,,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437
recC009XHETCkqHuq,phi-1.5,phi-1_5,47.6%,0,,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463
recrDBkh4IHJOxHX3,Yi-6B,Yi-6B,74.4%,10,,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652
recrHT1sajTQ2TT4C,Yi-9B,,76.4%,,,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652
recNf0HzKB7GCWezm,Falcon 7B,falcon-7b,76.3%,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
reciBfJ5AKzP8yAl6,Falcon 40B,falcon-40b,82.8%,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
reclyVPS1rMuSndsk,Falcon 7B,falcon-7b,78.1%,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recuz33sIq3QXEcxm,Falcon 40B,falcon-40b,85.3%,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recIC1QoBZPGnA5tT,Falcon2-11B stage 4,,82.1%,0,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recag3Lz0AvhGRmS4,Falcon2-11B stage 4,,82.9%,10,,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885
recGmteOirEpRk74U,Falcon-180B,falcon-180B,89.0%,10,,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867
recxJ90xKTkaaA1JI,Nemotron-4 15B,,82.4%,0,,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819
recMJYP7H2VFfBaU5,Phi-3-mini,Phi-3-mini-4k-instruct,76.7%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
rechnntOSjcnmoV5r,Phi-3-small,Phi-3-small-8k-instruct,77.0%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recY158Aob3my3MXx,Phi-3-medium,,82.4%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
rec6Wjhlpz51HwZAb,Phi-2,,53.6%,5,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219
recdvddfEJCDhen1x,GPT-3 (175B),text-davinci-001,78.9%,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rec2jRUCp1AFzsN31,GLaM,GLaM (MoE),76.6%,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recr3mOusR0K1TilG,GPT-3 (175B) ,text-davinci-001,78.1%,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recSe1QPYCHRdaNts,GLaM,GLaM (MoE),76.8%,1,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recc3BHdP90Z1fI20,GPT-3 (175B),text-davinci-001,79.3%,20,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rec1fTXYPgL0mRzBe,Gopher (280B),Gopher (280B),79.2%,0,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recb8yDVZF6TqwUZI,Megatron-NLG (530B),,82.4%,,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
rece76lEt406di379,GLaM,GLaM (MoE),77.2%,8,,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,http://arxiv.org/abs/2112.06905
recw7hL34oyte0hO4,GPT-3 Zero-Shot,text-davinci-001,78.9%,0,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recLlYcFFQ2fkEaY4,GPT-3 One-Shot,text-davinci-001,78.1%,1,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
rec9hsfppA15xBC1Z,GPT-3 Few-Shot,text-davinci-001,79.3%,few,,Language Models are Few-Shot Learners,http://arxiv.org/abs/2005.14165
recJnSsj1yJW0DpOz,MPT 7B,mpt-7b,76.4%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recUsvn4nbRtm8w4V,Falcon 7B,falcon-7b,74.1%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recGt5AcRHqUhevHc,ChatGLM2,,57.0%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recystu4Gc11FUp5X,InternLM 7B,internlm-7b,70.6%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recKfpodrVZGjl5CL,InternLM 20B,internlm-20b,78.1%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
rec6q2wSPdc9L6x40,Baichuan2 7B,Baichuan-2-7B-Base,68.0%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recKMHyLo7iHhIm9m,Baichuan2 13B,Baichuan-2-13B-Base,70.8%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recX2iDmTM7fmBQcr,LLAMA 2 70B,Llama-2-70b-hf ,85.3%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recRGvz0NvjpFvSSH,StableBeluga2,StableBeluga2,84.1%,0,,Qwen Technical Report,http://arxiv.org/abs/2309.16609
recaAsccv7Ypq42Vf,text-davinci-003,text-davinci-003,82.2%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rechG6XtFFAfGaxk8,text-davinci-002,text-davinci-002,81.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recjgw0ZOlh0jtyMY,Cohere xlarge v20220609 (52.4B),,81.1%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recwaK0tFnDh9Tqzy,Cohere Command beta (52.4B),,81.1%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recVxIk0J15SihM0z,Cohere xlarge v20221108 (52.4B),,81.0%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recREHiAS7YYjRck5,Anthropic-LM v4-s3 (52B),,80.7%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recxjdjjCdbqbpobG,TNLG v2 (530B),,79.9%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recedbGBvF8R6FFVE,OPT (175B),opt-175b,79.1%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recKfVK9KIvexAGQe,Jurassic-2 Jumbo (178B),,78.8%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recysGdQck0k4NcMH,Jurassic-2 Grande (17B),,78.1%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recbOlkritQTwwoUh,davinci (175B),davinci,77.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recypK2SzPuf4pjl2,J1-Jumbo v1 (178B),,76.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recuz2tek0qkudllk,J1-Grande v2 beta (17B),,76.4%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rechxO8XYnPKqrhSQ,Cohere Command beta (6.1B),,75.2%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recevvZp5GvZqnhTY,OPT (66B),opt-66b,74.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recQSVqDaut5Qbuj2,BLOOM (176B),bloom,74.4%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recSQwyBphOwlyxbx,J1-Grande v1 (17B),,73.9%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
reccslyJuclpjphLN,Cohere large v20220720 (13.1B),,73.6%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec91lq7ePZtc52ZO,Jurassic-2 Large (7.5B),,72.9%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec0XBvVEvE3wM4iL,Cohere medium v20221108 (6.1B),,72.6%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recwi6H4QDJ5RnKhl,GPT-NeoX (20B),,71.8%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recDDdululYn98qLK,Cohere medium v20220720 (6.1B),,70.6%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
reccuWvUpVMjCYrR8,TNLG v2 (6.7B),,70.4%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recOLH1fId6Xyd8sA,J1-Large v1 (7.5B),,70.0%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recKp5z9jwr9Zh0Ts,curie (6.7B),curie,68.2%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recYhmf2CsNqDxs5a,text-curie-001,text-curie-001,67.6%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recgEHai1fuGSnQ6U,GPT-J (6B),,66.3%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recxrPuCbT16sDZrG,text-babbage-001,text-babbage-001,56.1%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
rec9GDlYjK3e9FkXR,babbage (1.3B),babbage,55.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recpkpM7Bs6dFms3C,Cohere small v20220720 (410M),,48.3%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recfeLKZvBng82A20,ada (350M),ada,43.5%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag
recNkQMe0Nqd3deYK,text-ada-001,text-ada-001,42.9%,0,,Stanford HELM,https://crfm.stanford.edu/helm/classic/latest/#/groups/hellaswag