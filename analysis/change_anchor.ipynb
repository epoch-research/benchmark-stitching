{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a733e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path.cwd().parent)\n",
    "# print(\"cwd is now:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59f7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null performances after coercion: 0\n",
      "after saturation filter 2452\n",
      "after filter num benchmarks 2021\n",
      "after merge with model versions 2017\n",
      "after date filter (>= 2022-11-01) 1783\n",
      "after merge with benchmark dates 1783\n",
      "Original number of rows: 1783\n",
      "Number of rows after aggregation: 1306\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "from data_loader import scores_df\n",
    "from fit import fit_statistical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4df6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Custom color palette\n",
    "custom_colors = [\n",
    "    '#00A5A6',  # teal\n",
    "    '#E03D90',  # pink\n",
    "    '#FC6538',  # orange\n",
    "    '#6A3ECB',  # purple\n",
    "    '#0058DC',  # blue\n",
    "    '#EA8D00',  # yellow\n",
    "    '#B087F4',  # lightPurple\n",
    "    '#279E27',  # green\n",
    "    '#009AF1',  # lightBlue\n",
    "    '#015D90',  # darkBlue\n",
    "    '#EA4831',  # red\n",
    "    '#E1C700',  # yellow2\n",
    "    '#46FFFF',  # turquoise\n",
    "    '#63F039',  # lightGreen\n",
    "]\n",
    "\n",
    "sns.set_palette(custom_colors)\n",
    "colors = sns.color_palette()\n",
    "\n",
    "# === Seaborn global settings ===\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\",        # or \"darkgrid\", \"ticks\", \"white\"\n",
    "    palette=custom_colors,    # your custom color palette\n",
    "    context=\"notebook\"        # scaling for labels/titles (\"paper\", \"notebook\", \"talk\", \"poster\")\n",
    ")\n",
    "\n",
    "# === Matplotlib global settings (rcParams) ===\n",
    "plt.rcParams.update({\n",
    "    # Figure\n",
    "    \"figure.figsize\": (8, 5),\n",
    "    \"figure.dpi\": 120,\n",
    "    \n",
    "    # Axes\n",
    "    \"axes.titley\": 1.02,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.titlelocation\": 'center',\n",
    "    \"axes.titlepad\": 0,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.labelpad\": 10,           # spacing between axis and label\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \n",
    "    # Ticks\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    # tick marks size\n",
    "    \"xtick.major.size\": 5,\n",
    "    \"ytick.major.size\": 5,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \n",
    "    # tick visibility\n",
    "    \"xtick.top\": False,\n",
    "    \"xtick.bottom\": True,\n",
    "    \"ytick.left\": True,\n",
    "    \"ytick.right\": False,\n",
    "    \n",
    "    # Legend\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"legend.loc\": \"upper left\",\n",
    "    \"legend.frameon\": True,\n",
    "    \"legend.borderaxespad\": 0,\n",
    "\n",
    "    \n",
    "    # Lines and markers\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markersize\": 8,\n",
    "    \"lines.markeredgecolor\": 'auto',   # white outline (stroke)\n",
    "    \"lines.markeredgewidth\": 0.5,   \n",
    "    # title alignment left\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Error bars\n",
    "    \"errorbar.capsize\": 3,\n",
    "    \n",
    "    # Font\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.sans-serif\": [\"DejaVu Sans\"],\n",
    "    \n",
    "    # Grid\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"-\",\n",
    "    \"grid.color\": \"lightgray\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1fbd2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 53, initial cost 4.8749e+01, final cost 3.5499e+00, first-order optimality 9.69e-05.\n"
     ]
    }
   ],
   "source": [
    "anchor_mode = \"model\"  # \"model\", \"benchmark\"\n",
    "anchor_benchmark = \"Winogrande\"\n",
    "anchor_difficulty = 0\n",
    "anchor_slope = 1\n",
    "anchor_model1 = \"claude-2.0\"\n",
    "anchor_model1_capability = 1.177630\n",
    "anchor_model2 = \"claude-3-opus-20240229\"\n",
    "anchor_model2_capability = 1.311554\n",
    "\n",
    "df_anchor, df_cm_anchor, df_db_anchor = fit_statistical_model(\n",
    "    scores_df,\n",
    "    anchor_mode=anchor_mode,\n",
    "    anchor_benchmark=anchor_benchmark,\n",
    "    anchor_difficulty=anchor_difficulty,\n",
    "    anchor_slope=anchor_slope,\n",
    "    anchor_model1=anchor_model1,\n",
    "    anchor_model1_capability=anchor_model1_capability,\n",
    "    anchor_model2=anchor_model2,\n",
    "    anchor_model2_capability=anchor_model2_capability,\n",
    ")\n",
    "\n",
    "df_cm_anchor[\"date_obj\"] = pd.to_datetime(df_cm_anchor[\"date\"])\n",
    "\n",
    "# anchor_benchmark = \"Winogrande\"\n",
    "# anchor_difficulty = 0\n",
    "# anchor_slope = 1\n",
    "# df_anchor, df_cm_anchor, df_db_anchor = fit_statistical_model(scores_df, anchor_benchmark, anchor_difficulty, anchor_slope)\n",
    "\n",
    "# # Convert date strings to datetime objects\n",
    "# df_cm_anchor['date_obj'] = pd.to_datetime(df_cm_anchor['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193f1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5298e+01, final cost 3.4903e+00, first-order optimality 5.84e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5428e+01, final cost 3.4901e+00, first-order optimality 6.05e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5379e+01, final cost 3.4899e+00, first-order optimality 4.88e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5470e+01, final cost 3.4908e+00, first-order optimality 2.31e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.5228e+01, final cost 3.4910e+00, first-order optimality 7.92e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5456e+01, final cost 3.4910e+00, first-order optimality 3.37e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5414e+01, final cost 3.4897e+00, first-order optimality 4.77e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5436e+01, final cost 3.4912e+00, first-order optimality 2.66e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5502e+01, final cost 3.4912e+00, first-order optimality 2.95e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5434e+01, final cost 3.4914e+00, first-order optimality 3.18e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5504e+01, final cost 3.4896e+00, first-order optimality 4.17e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 33, initial cost 4.5432e+01, final cost 3.4903e+00, first-order optimality 2.67e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 33, initial cost 4.5448e+01, final cost 3.4910e+00, first-order optimality 3.59e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5523e+01, final cost 3.4906e+00, first-order optimality 5.12e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 33, initial cost 4.5433e+01, final cost 3.4898e+00, first-order optimality 7.67e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5419e+01, final cost 3.4897e+00, first-order optimality 7.30e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.5436e+01, final cost 3.4897e+00, first-order optimality 4.93e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5362e+01, final cost 3.4860e+00, first-order optimality 4.58e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 33, initial cost 4.5392e+01, final cost 3.4893e+00, first-order optimality 3.73e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.5462e+01, final cost 3.4888e+00, first-order optimality 5.00e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 27, initial cost 4.5466e+01, final cost 3.4881e+00, first-order optimality 6.27e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5675e+01, final cost 3.4871e+00, first-order optimality 4.80e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 23, initial cost 4.5334e+01, final cost 3.4824e+00, first-order optimality 1.01e-03.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5556e+01, final cost 3.4884e+00, first-order optimality 3.21e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5302e+01, final cost 3.4863e+00, first-order optimality 4.06e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 34, initial cost 4.5417e+01, final cost 3.4831e+00, first-order optimality 1.60e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 27, initial cost 4.5460e+01, final cost 3.4894e+00, first-order optimality 3.20e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.5555e+01, final cost 3.4893e+00, first-order optimality 3.97e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5457e+01, final cost 3.4893e+00, first-order optimality 4.45e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5469e+01, final cost 3.4896e+00, first-order optimality 3.88e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5392e+01, final cost 3.4878e+00, first-order optimality 4.56e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5460e+01, final cost 3.4895e+00, first-order optimality 7.15e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.5452e+01, final cost 3.4895e+00, first-order optimality 2.33e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5459e+01, final cost 3.4891e+00, first-order optimality 5.54e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 33, initial cost 4.5228e+01, final cost 3.4877e+00, first-order optimality 5.12e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.5434e+01, final cost 3.4895e+00, first-order optimality 4.92e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.4740e+01, final cost 3.4883e+00, first-order optimality 5.27e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 30, initial cost 4.5437e+01, final cost 3.4891e+00, first-order optimality 8.64e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 28, initial cost 4.5435e+01, final cost 3.4896e+00, first-order optimality 8.77e-04.\n"
     ]
    }
   ],
   "source": [
    "all_runs = {}  # will map benchmark_name -> dict of outputs\n",
    "failed = []  # keep track of anything that errors out\n",
    "\n",
    "# --- loop --------------------------------------------------------------------\n",
    "for _, row in df_db_anchor.iterrows():\n",
    "    anchor_benchmark = row[\"benchmark_name\"]  # e.g. \"HellaSwag\"\n",
    "    anchor_difficulty = float(row[\"estimated_difficulty\"])\n",
    "    anchor_slope = float(row[\"estimated_slope\"])\n",
    "\n",
    "    try:\n",
    "        df, df_cm, df_db = fit_statistical_model(\n",
    "            scores_df,\n",
    "            anchor_mode=\"benchmark\",\n",
    "            anchor_benchmark=anchor_benchmark,\n",
    "            anchor_difficulty=anchor_difficulty,\n",
    "            anchor_slope=anchor_slope,\n",
    "        )\n",
    "        all_runs[anchor_benchmark] = {\n",
    "            \"df1\": df,\n",
    "            \"df_cm1\": df_cm,\n",
    "            \"df_db\": df_db,\n",
    "            # cache the anchor values for reference\n",
    "            \"anchor_difficulty\": anchor_difficulty,\n",
    "            \"anchor_slope\": anchor_slope,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        failed.append((anchor_benchmark, str(e)))\n",
    "\n",
    "# --- post-processing (optional) ----------------------------------------------\n",
    "# 1) quick glance at what failed\n",
    "if failed:\n",
    "    print(\"Benchmarks that raised errors:\", failed)\n",
    "\n",
    "# 2) pull out the difficulty/slope re-estimates across all runs\n",
    "summary = pd.concat(\n",
    "    {\n",
    "        k: v[\"df_db\"][[\"benchmark_name\", \"estimated_difficulty\", \"estimated_slope\"]]\n",
    "        for k, v in all_runs.items()\n",
    "    },\n",
    "    names=[\"anchor_benchmark\"],\n",
    ").reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31753a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== variation in benchmark difficulties ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.725043</td>\n",
       "      <td>0.049622</td>\n",
       "      <td>0.622954</td>\n",
       "      <td>0.797447</td>\n",
       "      <td>0.067533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>0.072113</td>\n",
       "      <td>0.078011</td>\n",
       "      <td>-0.055982</td>\n",
       "      <td>0.178222</td>\n",
       "      <td>1.067455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>2.040985</td>\n",
       "      <td>0.035474</td>\n",
       "      <td>1.942866</td>\n",
       "      <td>2.116894</td>\n",
       "      <td>0.017150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>1.766796</td>\n",
       "      <td>0.034349</td>\n",
       "      <td>1.666368</td>\n",
       "      <td>1.829311</td>\n",
       "      <td>0.019184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.378204</td>\n",
       "      <td>0.063937</td>\n",
       "      <td>0.267554</td>\n",
       "      <td>0.468070</td>\n",
       "      <td>0.166814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>2.407985</td>\n",
       "      <td>0.040116</td>\n",
       "      <td>2.312863</td>\n",
       "      <td>2.504557</td>\n",
       "      <td>0.016439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>-1.196570</td>\n",
       "      <td>0.145638</td>\n",
       "      <td>-1.478440</td>\n",
       "      <td>-1.022539</td>\n",
       "      <td>-0.120101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>1.799740</td>\n",
       "      <td>0.034374</td>\n",
       "      <td>1.699628</td>\n",
       "      <td>1.863753</td>\n",
       "      <td>0.018846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>2.723564</td>\n",
       "      <td>0.041442</td>\n",
       "      <td>2.630750</td>\n",
       "      <td>2.824448</td>\n",
       "      <td>0.015014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>1.717978</td>\n",
       "      <td>0.029986</td>\n",
       "      <td>1.616011</td>\n",
       "      <td>1.765042</td>\n",
       "      <td>0.017223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>3.079526</td>\n",
       "      <td>0.046464</td>\n",
       "      <td>2.990651</td>\n",
       "      <td>3.192919</td>\n",
       "      <td>0.014888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction.LiveBench</th>\n",
       "      <td>1.644634</td>\n",
       "      <td>0.034419</td>\n",
       "      <td>1.543194</td>\n",
       "      <td>1.700727</td>\n",
       "      <td>0.020651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>2.721738</td>\n",
       "      <td>0.044308</td>\n",
       "      <td>2.630210</td>\n",
       "      <td>2.829740</td>\n",
       "      <td>0.016064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-Tier-4-2025-07-01-Private</th>\n",
       "      <td>3.500682</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>3.409730</td>\n",
       "      <td>3.601535</td>\n",
       "      <td>0.011817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>1.398172</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>1.295453</td>\n",
       "      <td>1.440303</td>\n",
       "      <td>0.025098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM8K</th>\n",
       "      <td>0.436605</td>\n",
       "      <td>0.061220</td>\n",
       "      <td>0.327260</td>\n",
       "      <td>0.523385</td>\n",
       "      <td>0.138360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>3.792956</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>3.702000</td>\n",
       "      <td>3.898227</td>\n",
       "      <td>0.011442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.797762</td>\n",
       "      <td>0.045162</td>\n",
       "      <td>0.689127</td>\n",
       "      <td>0.864325</td>\n",
       "      <td>0.055860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>-0.942513</td>\n",
       "      <td>0.131155</td>\n",
       "      <td>-1.192351</td>\n",
       "      <td>-0.781631</td>\n",
       "      <td>-0.137311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMBADA</th>\n",
       "      <td>-1.911212</td>\n",
       "      <td>0.162316</td>\n",
       "      <td>-2.218461</td>\n",
       "      <td>-1.719111</td>\n",
       "      <td>-0.083803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lech Mazur Writing</th>\n",
       "      <td>0.061084</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>-0.055217</td>\n",
       "      <td>0.166632</td>\n",
       "      <td>0.981175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LiveBench</th>\n",
       "      <td>1.409769</td>\n",
       "      <td>0.035559</td>\n",
       "      <td>1.307006</td>\n",
       "      <td>1.452378</td>\n",
       "      <td>0.024889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>1.356988</td>\n",
       "      <td>0.035896</td>\n",
       "      <td>1.253917</td>\n",
       "      <td>1.397291</td>\n",
       "      <td>0.026102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.217109</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>0.100114</td>\n",
       "      <td>0.315667</td>\n",
       "      <td>0.324097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>4.248049</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>4.160809</td>\n",
       "      <td>4.351838</td>\n",
       "      <td>0.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>1.959361</td>\n",
       "      <td>0.034539</td>\n",
       "      <td>1.859685</td>\n",
       "      <td>2.027806</td>\n",
       "      <td>0.017394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>1.788660</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>1.688274</td>\n",
       "      <td>1.852401</td>\n",
       "      <td>0.018974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>-0.020456</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>-0.158229</td>\n",
       "      <td>0.090671</td>\n",
       "      <td>-3.993479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIQA</th>\n",
       "      <td>-2.993599</td>\n",
       "      <td>0.225977</td>\n",
       "      <td>-3.439897</td>\n",
       "      <td>-2.739720</td>\n",
       "      <td>-0.074487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>2.049275</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>1.952001</td>\n",
       "      <td>2.125391</td>\n",
       "      <td>0.017083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceQA</th>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.070378</td>\n",
       "      <td>0.122290</td>\n",
       "      <td>0.335222</td>\n",
       "      <td>0.292034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>2.087334</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>1.989823</td>\n",
       "      <td>2.166280</td>\n",
       "      <td>0.017003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>2.507891</td>\n",
       "      <td>0.040773</td>\n",
       "      <td>2.416160</td>\n",
       "      <td>2.605707</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Agent Company</th>\n",
       "      <td>2.260128</td>\n",
       "      <td>0.034024</td>\n",
       "      <td>2.161019</td>\n",
       "      <td>2.324420</td>\n",
       "      <td>0.014854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>-1.979122</td>\n",
       "      <td>0.172792</td>\n",
       "      <td>-2.312362</td>\n",
       "      <td>-1.772403</td>\n",
       "      <td>-0.086151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>2.209806</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>2.113228</td>\n",
       "      <td>2.295318</td>\n",
       "      <td>0.016671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>-0.595924</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>-0.703038</td>\n",
       "      <td>-0.501537</td>\n",
       "      <td>-0.086445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>2.061171</td>\n",
       "      <td>0.035804</td>\n",
       "      <td>1.963054</td>\n",
       "      <td>2.138997</td>\n",
       "      <td>0.017141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>-1.113076</td>\n",
       "      <td>0.140630</td>\n",
       "      <td>-1.385011</td>\n",
       "      <td>-0.943224</td>\n",
       "      <td>-0.124670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mean       std       min  \\\n",
       "benchmark_name                                                         \n",
       "ANLI                                    0.725043  0.049622  0.622954   \n",
       "ARC AI2                                 0.072113  0.078011 -0.055982   \n",
       "ARC-AGI                                 2.040985  0.035474  1.942866   \n",
       "Aider polyglot                          1.766796  0.034349  1.666368   \n",
       "BBH                                     0.378204  0.063937  0.267554   \n",
       "Balrog                                  2.407985  0.040116  2.312863   \n",
       "BoolQ                                  -1.196570  0.145638 -1.478440   \n",
       "CadEval                                 1.799740  0.034374  1.699628   \n",
       "Cybench                                 2.723564  0.041442  2.630750   \n",
       "DeepResearch Bench                      1.717978  0.029986  1.616011   \n",
       "Factorio learning environment           3.079526  0.046464  2.990651   \n",
       "Fiction.LiveBench                       1.644634  0.034419  1.543194   \n",
       "FrontierMath-2025-02-28-Private         2.721738  0.044308  2.630210   \n",
       "FrontierMath-Tier-4-2025-07-01-Private  3.500682  0.041922  3.409730   \n",
       "GPQA diamond                            1.398172  0.035562  1.295453   \n",
       "GSM8K                                   0.436605  0.061220  0.327260   \n",
       "GSO-Bench                               3.792956  0.043982  3.702000   \n",
       "GeoBench                                0.797762  0.045162  0.689127   \n",
       "HellaSwag                              -0.942513  0.131155 -1.192351   \n",
       "LAMBADA                                -1.911212  0.162316 -2.218461   \n",
       "Lech Mazur Writing                      0.061084  0.060738 -0.055217   \n",
       "LiveBench                               1.409769  0.035559  1.307006   \n",
       "MATH level 5                            1.356988  0.035896  1.253917   \n",
       "MMLU                                    0.217109  0.071309  0.100114   \n",
       "OSUniverse                              4.248049  0.043030  4.160809   \n",
       "OSWorld                                 1.959361  0.034539  1.859685   \n",
       "OTIS Mock AIME 2024-2025                1.788660  0.034393  1.688274   \n",
       "OpenBookQA                             -0.020456  0.082788 -0.158229   \n",
       "PIQA                                   -2.993599  0.225977 -3.439897   \n",
       "SWE-Bench verified                      2.049275  0.035477  1.952001   \n",
       "ScienceQA                               0.237800  0.070378  0.122290   \n",
       "SimpleBench                             2.087334  0.035967  1.989823   \n",
       "Terminal Bench                          2.507891  0.040773  2.416160   \n",
       "The Agent Company                       2.260128  0.034024  2.161019   \n",
       "TriviaQA                               -1.979122  0.172792 -2.312362   \n",
       "VPCT                                    2.209806  0.037334  2.113228   \n",
       "VideoMME                               -0.595924  0.052206 -0.703038   \n",
       "WeirdML                                 2.061171  0.035804  1.963054   \n",
       "Winogrande                             -1.113076  0.140630 -1.385011   \n",
       "\n",
       "                                             max        cv  \n",
       "benchmark_name                                              \n",
       "ANLI                                    0.797447  0.067533  \n",
       "ARC AI2                                 0.178222  1.067455  \n",
       "ARC-AGI                                 2.116894  0.017150  \n",
       "Aider polyglot                          1.829311  0.019184  \n",
       "BBH                                     0.468070  0.166814  \n",
       "Balrog                                  2.504557  0.016439  \n",
       "BoolQ                                  -1.022539 -0.120101  \n",
       "CadEval                                 1.863753  0.018846  \n",
       "Cybench                                 2.824448  0.015014  \n",
       "DeepResearch Bench                      1.765042  0.017223  \n",
       "Factorio learning environment           3.192919  0.014888  \n",
       "Fiction.LiveBench                       1.700727  0.020651  \n",
       "FrontierMath-2025-02-28-Private         2.829740  0.016064  \n",
       "FrontierMath-Tier-4-2025-07-01-Private  3.601535  0.011817  \n",
       "GPQA diamond                            1.440303  0.025098  \n",
       "GSM8K                                   0.523385  0.138360  \n",
       "GSO-Bench                               3.898227  0.011442  \n",
       "GeoBench                                0.864325  0.055860  \n",
       "HellaSwag                              -0.781631 -0.137311  \n",
       "LAMBADA                                -1.719111 -0.083803  \n",
       "Lech Mazur Writing                      0.166632  0.981175  \n",
       "LiveBench                               1.452378  0.024889  \n",
       "MATH level 5                            1.397291  0.026102  \n",
       "MMLU                                    0.315667  0.324097  \n",
       "OSUniverse                              4.351838  0.009995  \n",
       "OSWorld                                 2.027806  0.017394  \n",
       "OTIS Mock AIME 2024-2025                1.852401  0.018974  \n",
       "OpenBookQA                              0.090671 -3.993479  \n",
       "PIQA                                   -2.739720 -0.074487  \n",
       "SWE-Bench verified                      2.125391  0.017083  \n",
       "ScienceQA                               0.335222  0.292034  \n",
       "SimpleBench                             2.166280  0.017003  \n",
       "Terminal Bench                          2.605707  0.016043  \n",
       "The Agent Company                       2.324420  0.014854  \n",
       "TriviaQA                               -1.772403 -0.086151  \n",
       "VPCT                                    2.295318  0.016671  \n",
       "VideoMME                               -0.501537 -0.086445  \n",
       "WeirdML                                 2.138997  0.017141  \n",
       "Winogrande                             -0.943224 -0.124670  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== variation in model capabilities ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-13B-Base</th>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.067859</td>\n",
       "      <td>0.186250</td>\n",
       "      <td>0.392318</td>\n",
       "      <td>0.226247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-7B-Base</th>\n",
       "      <td>-0.050632</td>\n",
       "      <td>0.085312</td>\n",
       "      <td>-0.183750</td>\n",
       "      <td>0.065290</td>\n",
       "      <td>-1.663202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-7B</th>\n",
       "      <td>-0.220527</td>\n",
       "      <td>0.094363</td>\n",
       "      <td>-0.376607</td>\n",
       "      <td>-0.094871</td>\n",
       "      <td>-0.422376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebras-GPT-13B</th>\n",
       "      <td>-0.546602</td>\n",
       "      <td>0.112655</td>\n",
       "      <td>-0.744687</td>\n",
       "      <td>-0.402383</td>\n",
       "      <td>-0.203442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeQwen1.5-7B</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.082948</td>\n",
       "      <td>-0.131183</td>\n",
       "      <td>0.113001</td>\n",
       "      <td>29056.801948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>-0.256101</td>\n",
       "      <td>0.096407</td>\n",
       "      <td>-0.417102</td>\n",
       "      <td>-0.128495</td>\n",
       "      <td>-0.371585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>-0.054446</td>\n",
       "      <td>0.085793</td>\n",
       "      <td>-0.191930</td>\n",
       "      <td>0.061664</td>\n",
       "      <td>-1.555397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003</th>\n",
       "      <td>0.829792</td>\n",
       "      <td>0.045638</td>\n",
       "      <td>0.728565</td>\n",
       "      <td>0.895812</td>\n",
       "      <td>0.054290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1</th>\n",
       "      <td>-0.047638</td>\n",
       "      <td>0.085492</td>\n",
       "      <td>-0.181946</td>\n",
       "      <td>0.068184</td>\n",
       "      <td>-1.771450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base</th>\n",
       "      <td>-0.181765</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>-0.333167</td>\n",
       "      <td>-0.058251</td>\n",
       "      <td>-0.503522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mean       std       min       max            cv\n",
       "model                                                                    \n",
       "Baichuan-2-13B-Base  0.296063  0.067859  0.186250  0.392318      0.226247\n",
       "Baichuan-2-7B-Base  -0.050632  0.085312 -0.183750  0.065290     -1.663202\n",
       "Baichuan-7B         -0.220527  0.094363 -0.376607 -0.094871     -0.422376\n",
       "Cerebras-GPT-13B    -0.546602  0.112655 -0.744687 -0.402383     -0.203442\n",
       "CodeQwen1.5-7B       0.000003  0.082948 -0.131183  0.113001  29056.801948\n",
       "...                       ...       ...       ...       ...           ...\n",
       "starcoder2-3b       -0.256101  0.096407 -0.417102 -0.128495     -0.371585\n",
       "starcoder2-7b       -0.054446  0.085793 -0.191930  0.061664     -1.555397\n",
       "text-davinci-003     0.829792  0.045638  0.728565  0.895812      0.054290\n",
       "vicuna-13b-v1.1     -0.047638  0.085492 -0.181946  0.068184     -1.771450\n",
       "xgen-7b-8k-base     -0.181765  0.092719 -0.333167 -0.058251     -0.503522\n",
       "\n",
       "[170 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1)  DIFFICULTY  ––  variation of each benchmark’s difficulty estimate\n",
    "# ---------------------------------------------------------------------------\n",
    "difficulty_rows = []\n",
    "\n",
    "for anchor, run in all_runs.items():\n",
    "    df_db = run[\"df_db\"]  # difficulty table from that fit\n",
    "    out = df_db[[\"benchmark_name\", \"estimated_difficulty\"]].copy()\n",
    "    out[\"anchor_benchmark\"] = anchor  # remember which fit this came from\n",
    "    difficulty_rows.append(out)\n",
    "\n",
    "difficulty_long = pd.concat(difficulty_rows, ignore_index=True)\n",
    "\n",
    "# drop the trivial row where the benchmark was forced to be the anchor (always fixed):\n",
    "difficulty_long = difficulty_long[\n",
    "    difficulty_long[\"benchmark_name\"] != difficulty_long[\"anchor_benchmark\"]\n",
    "]\n",
    "\n",
    "difficulty_stats = (\n",
    "    difficulty_long.groupby(\"benchmark_name\")[\"estimated_difficulty\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean(),  # coefficient of variation\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  CAPABILITY  ––  variation of each model’s capability estimate\n",
    "# ---------------------------------------------------------------------------\n",
    "capability_rows = []\n",
    "\n",
    "for anchor, run in all_runs.items():\n",
    "    df_cm = run[\"df_cm1\"]  # capability table from that fit\n",
    "    out = df_cm[[\"model\", \"estimated_capability\"]].copy()\n",
    "    out[\"anchor_benchmark\"] = anchor\n",
    "    capability_rows.append(out)\n",
    "\n",
    "capability_long = pd.concat(capability_rows, ignore_index=True)\n",
    "\n",
    "capability_stats = (\n",
    "    capability_long.groupby(\"model\")[\"estimated_capability\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean(),\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3)  quick look\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=== variation in benchmark difficulties ===\")\n",
    "display(difficulty_stats)\n",
    "\n",
    "print(\"\\n=== variation in model capabilities ===\")\n",
    "display(capability_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85924acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spearman rank correlation across fits (benchmark difficulties) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th>ANLI</th>\n",
       "      <th>ARC AI2</th>\n",
       "      <th>ARC-AGI</th>\n",
       "      <th>Aider polyglot</th>\n",
       "      <th>BBH</th>\n",
       "      <th>Balrog</th>\n",
       "      <th>BoolQ</th>\n",
       "      <th>CadEval</th>\n",
       "      <th>Cybench</th>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <th>...</th>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <th>ScienceQA</th>\n",
       "      <th>SimpleBench</th>\n",
       "      <th>Terminal Bench</th>\n",
       "      <th>The Agent Company</th>\n",
       "      <th>TriviaQA</th>\n",
       "      <th>VPCT</th>\n",
       "      <th>VideoMME</th>\n",
       "      <th>WeirdML</th>\n",
       "      <th>Winogrande</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.999198</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999021</td>\n",
       "      <td>0.999312</td>\n",
       "      <td>0.999202</td>\n",
       "      <td>0.999528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.999096</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>0.999406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0.998974</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>0.998744</td>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0.999728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>0.999198</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>0.998820</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998631</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.999523</td>\n",
       "      <td>0.999633</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.998974</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998881</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.998828</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.999186</td>\n",
       "      <td>0.999539</td>\n",
       "      <td>0.998820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.998881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998575</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998762</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.998604</td>\n",
       "      <td>0.998562</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.998562</td>\n",
       "      <td>0.999133</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.999519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.998575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999260</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.998707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.999021</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.998820</td>\n",
       "      <td>0.998894</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998856</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998804</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.998781</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.998779</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.999347</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>0.999312</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998762</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.998856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.998719</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.999553</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999704</td>\n",
       "      <td>0.998774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>0.999202</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>0.999367</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999187</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.998997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>0.999528</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.999294</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.999360</td>\n",
       "      <td>0.999357</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.998844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.998804</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.998956</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.998891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction.LiveBench</th>\n",
       "      <td>0.999611</td>\n",
       "      <td>0.999078</td>\n",
       "      <td>0.999531</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999019</td>\n",
       "      <td>0.999255</td>\n",
       "      <td>0.998931</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.999317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999459</td>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.999342</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0.999366</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.998868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>0.999138</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>0.999340</td>\n",
       "      <td>0.998941</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.999050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-Tier-4-2025-07-01-Private</th>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.999531</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.999041</td>\n",
       "      <td>0.999511</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.998905</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.999373</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.998983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.999196</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.999698</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>0.999186</td>\n",
       "      <td>0.999177</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.999195</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.998918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM8K</th>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.999633</td>\n",
       "      <td>0.998708</td>\n",
       "      <td>0.998942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>0.998813</td>\n",
       "      <td>0.998893</td>\n",
       "      <td>0.999015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998665</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998605</td>\n",
       "      <td>0.998624</td>\n",
       "      <td>0.998587</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.999075</td>\n",
       "      <td>0.998631</td>\n",
       "      <td>0.999483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.999061</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.999047</td>\n",
       "      <td>0.999605</td>\n",
       "      <td>0.999696</td>\n",
       "      <td>0.999633</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>0.999503</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.999099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999331</td>\n",
       "      <td>0.999253</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.999354</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.999377</td>\n",
       "      <td>0.999223</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999202</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>0.999371</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.999160</td>\n",
       "      <td>0.998971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>0.998668</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998715</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.998799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.998626</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.998628</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>0.998623</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMBADA</th>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.999377</td>\n",
       "      <td>0.999386</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.999355</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.999460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.999104</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.999373</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999359</td>\n",
       "      <td>0.999430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lech Mazur Writing</th>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.999032</td>\n",
       "      <td>0.999177</td>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999007</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.998977</td>\n",
       "      <td>0.999033</td>\n",
       "      <td>0.998981</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>0.998974</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LiveBench</th>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999135</td>\n",
       "      <td>0.999454</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>0.999095</td>\n",
       "      <td>0.999213</td>\n",
       "      <td>0.998949</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999387</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>0.999294</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.998893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999261</td>\n",
       "      <td>0.999315</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.999262</td>\n",
       "      <td>0.999136</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.999245</td>\n",
       "      <td>0.999698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999259</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>0.999135</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>0.999376</td>\n",
       "      <td>0.999212</td>\n",
       "      <td>0.998944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998605</td>\n",
       "      <td>0.998785</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>0.998851</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998533</td>\n",
       "      <td>0.998578</td>\n",
       "      <td>0.998530</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.998526</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>0.998549</td>\n",
       "      <td>0.999608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999430</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.999284</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.999242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.998846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.998711</td>\n",
       "      <td>0.999508</td>\n",
       "      <td>0.998837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998672</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.999096</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998819</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.998875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.998771</td>\n",
       "      <td>0.999545</td>\n",
       "      <td>0.999376</td>\n",
       "      <td>0.999433</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.998797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>0.999062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.998729</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.998567</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.998870</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998575</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>0.998544</td>\n",
       "      <td>0.999546</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.999804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIQA</th>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>0.999578</td>\n",
       "      <td>0.999501</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999431</td>\n",
       "      <td>0.999424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999430</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.998627</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.998804</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceQA</th>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.998631</td>\n",
       "      <td>0.998828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.998719</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998549</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.998542</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.998540</td>\n",
       "      <td>0.999197</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>0.999560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>0.999085</td>\n",
       "      <td>0.998749</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.998781</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>0.999096</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.999523</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.998604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.998588</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.999081</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.998731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Agent Company</th>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.999633</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.998562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998779</td>\n",
       "      <td>0.999494</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.998542</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999033</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.998693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999260</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.999360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>0.999577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.998744</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.998562</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.999553</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.998540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.998689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999186</td>\n",
       "      <td>0.999133</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.999347</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999187</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.999197</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.999081</td>\n",
       "      <td>0.999033</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999539</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.998791</td>\n",
       "      <td>0.999704</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998568</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>0.998733</td>\n",
       "      <td>0.998820</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998774</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>0.998844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998715</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>0.998731</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.998689</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "anchor_benchmark                            ANLI   ARC AI2   ARC-AGI  \\\n",
       "anchor_benchmark                                                       \n",
       "ANLI                                    1.000000  0.999406  0.999198   \n",
       "ARC AI2                                 0.999406  1.000000  0.998812   \n",
       "ARC-AGI                                 0.999198  0.998812  1.000000   \n",
       "Aider polyglot                          0.999451  0.998974  0.999701   \n",
       "BBH                                     0.999452  0.999712  0.998665   \n",
       "Balrog                                  0.999071  0.998768  0.999571   \n",
       "BoolQ                                   0.999021  0.999673  0.998820   \n",
       "CadEval                                 0.999312  0.998884  0.999894   \n",
       "Cybench                                 0.999202  0.999074  0.999483   \n",
       "DeepResearch Bench                      0.999528  0.999024  0.999613   \n",
       "Factorio learning environment           0.999288  0.999004  0.999494   \n",
       "Fiction.LiveBench                       0.999611  0.999078  0.999531   \n",
       "FrontierMath-2025-02-28-Private         0.999138  0.999142  0.999493   \n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999401  0.999117  0.999531   \n",
       "GPQA diamond                            0.999794  0.999196  0.999382   \n",
       "GSM8K                                   0.999559  0.999633  0.998708   \n",
       "GSO-Bench                               0.999543  0.999258  0.999596   \n",
       "GeoBench                                1.000000  0.999331  0.999253   \n",
       "HellaSwag                               0.999002  0.999802  0.998668   \n",
       "LAMBADA                                 0.999521  0.999377  0.999386   \n",
       "Lech Mazur Writing                      0.999561  0.999776  0.999032   \n",
       "LiveBench                               0.999700  0.999135  0.999454   \n",
       "MATH level 5                            0.999894  0.999261  0.999315   \n",
       "MMLU                                    0.999263  0.999894  0.998605   \n",
       "OSUniverse                              0.999719  0.999430  0.999694   \n",
       "OSWorld                                 0.999251  0.998846  1.000000   \n",
       "OTIS Mock AIME 2024-2025                0.999378  0.998927  0.999794   \n",
       "OpenBookQA                              0.999062  1.000000  0.998598   \n",
       "PIQA                                    0.999478  0.999654  0.999478   \n",
       "SWE-Bench verified                      0.999151  0.998784  1.000000   \n",
       "ScienceQA                               0.999353  0.999798  0.998631   \n",
       "SimpleBench                             0.999085  0.998749  0.999796   \n",
       "Terminal Bench                          0.999096  0.998800  0.999523   \n",
       "The Agent Company                       0.999062  0.998750  0.999633   \n",
       "TriviaQA                                0.999370  0.999615  0.999302   \n",
       "VPCT                                    0.999067  0.998744  0.999709   \n",
       "VideoMME                                0.999470  0.999575  0.999071   \n",
       "WeirdML                                 0.999113  0.998763  0.999894   \n",
       "Winogrande                              0.999000  0.999728  0.998733   \n",
       "\n",
       "anchor_benchmark                        Aider polyglot       BBH    Balrog  \\\n",
       "anchor_benchmark                                                             \n",
       "ANLI                                          0.999451  0.999452  0.999071   \n",
       "ARC AI2                                       0.998974  0.999712  0.998768   \n",
       "ARC-AGI                                       0.999701  0.998665  0.999571   \n",
       "Aider polyglot                                1.000000  0.998881  0.999346   \n",
       "BBH                                           0.998881  1.000000  0.998575   \n",
       "Balrog                                        0.999346  0.998575  1.000000   \n",
       "BoolQ                                         0.998894  0.999519  0.998790   \n",
       "CadEval                                       0.999894  0.998762  0.999450   \n",
       "Cybench                                       0.999367  0.998874  0.999804   \n",
       "DeepResearch Bench                            1.000000  0.998947  0.999299   \n",
       "Factorio learning environment                 0.999417  0.998804  0.999733   \n",
       "Fiction.LiveBench                             0.999894  0.999019  0.999255   \n",
       "FrontierMath-2025-02-28-Private               0.999340  0.998941  0.999894   \n",
       "FrontierMath-Tier-4-2025-07-01-Private        0.999493  0.998918  0.999684   \n",
       "GPQA diamond                                  0.999698  0.999176  0.999173   \n",
       "GSM8K                                         0.998942  1.000000  0.998598   \n",
       "GSO-Bench                                     0.999599  0.999061  0.999659   \n",
       "GeoBench                                      0.999527  0.999354  0.999102   \n",
       "HellaSwag                                     0.998770  0.999541  0.998644   \n",
       "LAMBADA                                       0.999446  0.999089  0.999355   \n",
       "Lech Mazur Writing                            0.999177  0.999575  0.999000   \n",
       "LiveBench                                     0.999793  0.999095  0.999213   \n",
       "MATH level 5                                  0.999609  0.999262  0.999136   \n",
       "MMLU                                          0.998785  0.999894  0.998547   \n",
       "OSUniverse                                    0.999738  0.999236  0.999662   \n",
       "OSWorld                                       0.999794  0.998711  0.999508   \n",
       "OTIS Mock AIME 2024-2025                      1.000000  0.998819  0.999396   \n",
       "OpenBookQA                                    0.998729  0.999639  0.998567   \n",
       "PIQA                                          0.999516  0.999451  0.999423   \n",
       "SWE-Bench verified                            0.999616  0.998627  0.999639   \n",
       "ScienceQA                                     0.998828  1.000000  0.998558   \n",
       "SimpleBench                                   0.999473  0.998574  0.999799   \n",
       "Terminal Bench                                0.999334  0.998604  1.000000   \n",
       "The Agent Company                             0.999375  0.998562  1.000000   \n",
       "TriviaQA                                      0.999352  0.999368  0.999260   \n",
       "VPCT                                          0.999418  0.998562  0.999894   \n",
       "VideoMME                                      0.999186  0.999133  0.999050   \n",
       "WeirdML                                       0.999539  0.998596  0.999715   \n",
       "Winogrande                                    0.998820  0.999519  0.998707   \n",
       "\n",
       "anchor_benchmark                           BoolQ   CadEval   Cybench  \\\n",
       "anchor_benchmark                                                       \n",
       "ANLI                                    0.999021  0.999312  0.999202   \n",
       "ARC AI2                                 0.999673  0.998884  0.999074   \n",
       "ARC-AGI                                 0.998820  0.999894  0.999483   \n",
       "Aider polyglot                          0.998894  0.999894  0.999367   \n",
       "BBH                                     0.999519  0.998762  0.998874   \n",
       "Balrog                                  0.998790  0.999450  0.999804   \n",
       "BoolQ                                   1.000000  0.998856  0.999073   \n",
       "CadEval                                 0.998856  1.000000  0.999420   \n",
       "Cybench                                 0.999073  0.999420  1.000000   \n",
       "DeepResearch Bench                      0.998913  0.999793  0.999341   \n",
       "Factorio learning environment           0.998956  0.999453  1.000000   \n",
       "Fiction.LiveBench                       0.998931  0.999699  0.999317   \n",
       "FrontierMath-2025-02-28-Private         0.999122  0.999411  1.000000   \n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999041  0.999511  0.999894   \n",
       "GPQA diamond                            0.998966  0.999527  0.999269   \n",
       "GSM8K                                   0.999494  0.998813  0.998893   \n",
       "GSO-Bench                               0.999149  0.999599  0.999811   \n",
       "GeoBench                                0.999002  0.999377  0.999223   \n",
       "HellaSwag                               0.999894  0.998715  0.998939   \n",
       "LAMBADA                                 0.999534  0.999417  0.999446   \n",
       "Lech Mazur Writing                      0.999395  0.999097  0.999148   \n",
       "LiveBench                               0.998949  0.999610  0.999292   \n",
       "MATH level 5                            0.998984  0.999449  0.999245   \n",
       "MMLU                                    0.999582  0.998686  0.998851   \n",
       "OSUniverse                              0.999284  0.999719  0.999752   \n",
       "OSWorld                                 0.998837  1.000000  0.999450   \n",
       "OTIS Mock AIME 2024-2025                0.998875  1.000000  0.999393   \n",
       "OpenBookQA                              0.999733  0.998657  0.998870   \n",
       "PIQA                                    0.999578  0.999501  0.999680   \n",
       "SWE-Bench verified                      0.998804  0.999795  0.999519   \n",
       "ScienceQA                               0.999548  0.998719  0.998860   \n",
       "SimpleBench                             0.998781  0.999623  0.999608   \n",
       "Terminal Bench                          0.998811  0.999422  0.999894   \n",
       "The Agent Company                       0.998779  0.999494  0.999728   \n",
       "TriviaQA                                0.999660  0.999329  0.999527   \n",
       "VPCT                                    0.998777  0.999553  0.999663   \n",
       "VideoMME                                0.999347  0.999123  0.999187   \n",
       "WeirdML                                 0.998791  0.999704  0.999560   \n",
       "Winogrande                              1.000000  0.998774  0.998997   \n",
       "\n",
       "anchor_benchmark                        DeepResearch Bench  ...  \\\n",
       "anchor_benchmark                                            ...   \n",
       "ANLI                                              0.999528  ...   \n",
       "ARC AI2                                           0.999024  ...   \n",
       "ARC-AGI                                           0.999613  ...   \n",
       "Aider polyglot                                    1.000000  ...   \n",
       "BBH                                               0.998947  ...   \n",
       "Balrog                                            0.999299  ...   \n",
       "BoolQ                                             0.998913  ...   \n",
       "CadEval                                           0.999793  ...   \n",
       "Cybench                                           0.999341  ...   \n",
       "DeepResearch Bench                                1.000000  ...   \n",
       "Factorio learning environment                     0.999399  ...   \n",
       "Fiction.LiveBench                                 1.000000  ...   \n",
       "FrontierMath-2025-02-28-Private                   0.999307  ...   \n",
       "FrontierMath-Tier-4-2025-07-01-Private            0.999483  ...   \n",
       "GPQA diamond                                      0.999793  ...   \n",
       "GSM8K                                             0.999015  ...   \n",
       "GSO-Bench                                         0.999597  ...   \n",
       "GeoBench                                          0.999610  ...   \n",
       "HellaSwag                                         0.998799  ...   \n",
       "LAMBADA                                           0.999460  ...   \n",
       "Lech Mazur Writing                                0.999222  ...   \n",
       "LiveBench                                         0.999894  ...   \n",
       "MATH level 5                                      0.999698  ...   \n",
       "MMLU                                              0.998841  ...   \n",
       "OSUniverse                                        0.999744  ...   \n",
       "OSWorld                                           0.999700  ...   \n",
       "OTIS Mock AIME 2024-2025                          0.999894  ...   \n",
       "OpenBookQA                                        0.998768  ...   \n",
       "PIQA                                              0.999519  ...   \n",
       "SWE-Bench verified                                0.999535  ...   \n",
       "ScienceQA                                         0.998889  ...   \n",
       "SimpleBench                                       0.999405  ...   \n",
       "Terminal Bench                                    0.999294  ...   \n",
       "The Agent Company                                 0.999321  ...   \n",
       "TriviaQA                                          0.999360  ...   \n",
       "VPCT                                              0.999357  ...   \n",
       "VideoMME                                          0.999220  ...   \n",
       "WeirdML                                           0.999465  ...   \n",
       "Winogrande                                        0.998844  ...   \n",
       "\n",
       "anchor_benchmark                        SWE-Bench verified  ScienceQA  \\\n",
       "anchor_benchmark                                                        \n",
       "ANLI                                              0.999151   0.999353   \n",
       "ARC AI2                                           0.998784   0.999798   \n",
       "ARC-AGI                                           1.000000   0.998631   \n",
       "Aider polyglot                                    0.999616   0.998828   \n",
       "BBH                                               0.998627   1.000000   \n",
       "Balrog                                            0.999639   0.998558   \n",
       "BoolQ                                             0.998804   0.999548   \n",
       "CadEval                                           0.999795   0.998719   \n",
       "Cybench                                           0.999519   0.998860   \n",
       "DeepResearch Bench                                0.999535   0.998889   \n",
       "Factorio learning environment                     0.999519   0.998791   \n",
       "Fiction.LiveBench                                 0.999459   0.998955   \n",
       "FrontierMath-2025-02-28-Private                   0.999541   0.998927   \n",
       "FrontierMath-Tier-4-2025-07-01-Private            0.999544   0.998905   \n",
       "GPQA diamond                                      0.999321   0.999099   \n",
       "GSM8K                                             0.998665   0.999894   \n",
       "GSO-Bench                                         0.999596   0.999047   \n",
       "GeoBench                                          0.999202   0.999263   \n",
       "HellaSwag                                         0.998650   0.999594   \n",
       "LAMBADA                                           0.999372   0.999104   \n",
       "Lech Mazur Writing                                0.999007   0.999671   \n",
       "LiveBench                                         0.999387   0.999024   \n",
       "MATH level 5                                      0.999259   0.999178   \n",
       "MMLU                                              0.998574   1.000000   \n",
       "OSUniverse                                        0.999681   0.999222   \n",
       "OSWorld                                           0.999894   0.998672   \n",
       "OTIS Mock AIME 2024-2025                          0.999702   0.998771   \n",
       "OpenBookQA                                        0.998575   0.999715   \n",
       "PIQA                                              0.999465   0.999439   \n",
       "SWE-Bench verified                                1.000000   0.998596   \n",
       "ScienceQA                                         0.998596   1.000000   \n",
       "SimpleBench                                       0.999894   0.998549   \n",
       "Terminal Bench                                    0.999581   0.998588   \n",
       "The Agent Company                                 0.999712   0.998542   \n",
       "TriviaQA                                          0.999288   0.999370   \n",
       "VPCT                                              0.999797   0.998540   \n",
       "VideoMME                                          0.999051   0.999197   \n",
       "WeirdML                                           1.000000   0.998568   \n",
       "Winogrande                                        0.998715   0.999560   \n",
       "\n",
       "anchor_benchmark                        SimpleBench  Terminal Bench  \\\n",
       "anchor_benchmark                                                      \n",
       "ANLI                                       0.999085        0.999096   \n",
       "ARC AI2                                    0.998749        0.998800   \n",
       "ARC-AGI                                    0.999796        0.999523   \n",
       "Aider polyglot                             0.999473        0.999334   \n",
       "BBH                                        0.998574        0.998604   \n",
       "Balrog                                     0.999799        1.000000   \n",
       "BoolQ                                      0.998781        0.998811   \n",
       "CadEval                                    0.999623        0.999422   \n",
       "Cybench                                    0.999608        0.999894   \n",
       "DeepResearch Bench                         0.999405        0.999294   \n",
       "Factorio learning environment              0.999582        0.999806   \n",
       "Fiction.LiveBench                          0.999342        0.999256   \n",
       "FrontierMath-2025-02-28-Private            0.999654        1.000000   \n",
       "FrontierMath-Tier-4-2025-07-01-Private     0.999580        0.999739   \n",
       "GPQA diamond                               0.999226        0.999186   \n",
       "GSM8K                                      0.998605        0.998624   \n",
       "GSO-Bench                                  0.999605        0.999696   \n",
       "GeoBench                                   0.999127        0.999123   \n",
       "HellaSwag                                  0.998626        0.998670   \n",
       "LAMBADA                                    0.999350        0.999373   \n",
       "Lech Mazur Writing                         0.998977        0.999033   \n",
       "LiveBench                                  0.999282        0.999220   \n",
       "MATH level 5                               0.999175        0.999153   \n",
       "MMLU                                       0.998533        0.998578   \n",
       "OSUniverse                                 0.999660        0.999680   \n",
       "OSWorld                                    0.999706        0.999470   \n",
       "OTIS Mock AIME 2024-2025                   0.999545        0.999376   \n",
       "OpenBookQA                                 0.998547        0.998598   \n",
       "PIQA                                       0.999439        0.999431   \n",
       "SWE-Bench verified                         0.999894        0.999581   \n",
       "ScienceQA                                  0.998549        0.998588   \n",
       "SimpleBench                                1.000000        0.999719   \n",
       "Terminal Bench                             0.999719        1.000000   \n",
       "The Agent Company                          0.999894        0.999894   \n",
       "TriviaQA                                   0.999265        0.999272   \n",
       "VPCT                                       1.000000        0.999801   \n",
       "VideoMME                                   0.999027        0.999081   \n",
       "WeirdML                                    1.000000        0.999646   \n",
       "Winogrande                                 0.998693        0.998731   \n",
       "\n",
       "anchor_benchmark                        The Agent Company  TriviaQA      VPCT  \\\n",
       "anchor_benchmark                                                                \n",
       "ANLI                                             0.999062  0.999370  0.999067   \n",
       "ARC AI2                                          0.998750  0.999615  0.998744   \n",
       "ARC-AGI                                          0.999633  0.999302  0.999709   \n",
       "Aider polyglot                                   0.999375  0.999352  0.999418   \n",
       "BBH                                              0.998562  0.999368  0.998562   \n",
       "Balrog                                           1.000000  0.999260  0.999894   \n",
       "BoolQ                                            0.998779  0.999660  0.998777   \n",
       "CadEval                                          0.999494  0.999329  0.999553   \n",
       "Cybench                                          0.999728  0.999527  0.999663   \n",
       "DeepResearch Bench                               0.999321  0.999360  0.999357   \n",
       "Factorio learning environment                    0.999673  0.999382  0.999623   \n",
       "Fiction.LiveBench                                0.999270  0.999366  0.999299   \n",
       "FrontierMath-2025-02-28-Private                  0.999802  0.999563  0.999723   \n",
       "FrontierMath-Tier-4-2025-07-01-Private           0.999640  0.999452  0.999606   \n",
       "GPQA diamond                                     0.999177  0.999372  0.999195   \n",
       "GSM8K                                            0.998587  0.999368  0.998590   \n",
       "GSO-Bench                                        0.999633  0.999543  0.999615   \n",
       "GeoBench                                         0.999097  0.999371  0.999106   \n",
       "HellaSwag                                        0.998628  0.999513  0.998623   \n",
       "LAMBADA                                          0.999346  0.999765  0.999345   \n",
       "Lech Mazur Writing                               0.998981  0.999403  0.998974   \n",
       "LiveBench                                        0.999222  0.999370  0.999245   \n",
       "MATH level 5                                     0.999135  0.999372  0.999149   \n",
       "MMLU                                             0.998530  0.999375  0.998526   \n",
       "OSUniverse                                       0.999655  0.999659  0.999654   \n",
       "OSWorld                                          0.999561  0.999316  0.999628   \n",
       "OTIS Mock AIME 2024-2025                         0.999433  0.999341  0.999483   \n",
       "OpenBookQA                                       0.998550  0.999428  0.998544   \n",
       "PIQA                                             0.999424  1.000000  0.999430   \n",
       "SWE-Bench verified                               0.999712  0.999288  0.999797   \n",
       "ScienceQA                                        0.998542  0.999370  0.998540   \n",
       "SimpleBench                                      0.999894  0.999265  1.000000   \n",
       "Terminal Bench                                   0.999894  0.999272  0.999801   \n",
       "The Agent Company                                1.000000  0.999255  1.000000   \n",
       "TriviaQA                                         0.999255  1.000000  0.999258   \n",
       "VPCT                                             1.000000  0.999258  1.000000   \n",
       "VideoMME                                         0.999033  0.999464  0.999025   \n",
       "WeirdML                                          0.999798  0.999276  0.999894   \n",
       "Winogrande                                       0.998693  0.999577  0.998689   \n",
       "\n",
       "anchor_benchmark                        VideoMME   WeirdML  Winogrande  \n",
       "anchor_benchmark                                                        \n",
       "ANLI                                    0.999470  0.999113    0.999000  \n",
       "ARC AI2                                 0.999575  0.998763    0.999728  \n",
       "ARC-AGI                                 0.999071  0.999894    0.998733  \n",
       "Aider polyglot                          0.999186  0.999539    0.998820  \n",
       "BBH                                     0.999133  0.998596    0.999519  \n",
       "Balrog                                  0.999050  0.999715    0.998707  \n",
       "BoolQ                                   0.999347  0.998791    1.000000  \n",
       "CadEval                                 0.999123  0.999704    0.998774  \n",
       "Cybench                                 0.999187  0.999560    0.998997  \n",
       "DeepResearch Bench                      0.999220  0.999465    0.998844  \n",
       "Factorio learning environment           0.999269  0.999548    0.998891  \n",
       "Fiction.LiveBench                       0.999256  0.999395    0.998868  \n",
       "FrontierMath-2025-02-28-Private         0.999125  0.999594    0.999050  \n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999373  0.999560    0.998983  \n",
       "GPQA diamond                            0.999334  0.999268    0.998918  \n",
       "GSM8K                                   0.999075  0.998631    0.999483  \n",
       "GSO-Bench                               0.999503  0.999599    0.999099  \n",
       "GeoBench                                0.999422  0.999160    0.998971  \n",
       "HellaSwag                               0.999544  0.998635    1.000000  \n",
       "LAMBADA                                 0.999739  0.999359    0.999430  \n",
       "Lech Mazur Writing                      0.999894  0.998988    0.999436  \n",
       "LiveBench                               0.999294  0.999330    0.998893  \n",
       "MATH level 5                            0.999376  0.999212    0.998944  \n",
       "MMLU                                    0.999269  0.998549    0.999608  \n",
       "OSUniverse                              0.999662  0.999669    0.999242  \n",
       "OSWorld                                 0.999096  0.999795    0.998752  \n",
       "OTIS Mock AIME 2024-2025                0.999153  0.999619    0.998797  \n",
       "OpenBookQA                              0.999546  0.998558    0.999804  \n",
       "PIQA                                    0.999447  0.999451    0.999518  \n",
       "SWE-Bench verified                      0.999051  1.000000    0.998715  \n",
       "ScienceQA                               0.999197  0.998568    0.999560  \n",
       "SimpleBench                             0.999027  1.000000    0.998693  \n",
       "Terminal Bench                          0.999081  0.999646    0.998731  \n",
       "The Agent Company                       0.999033  0.999798    0.998693  \n",
       "TriviaQA                                0.999464  0.999276    0.999577  \n",
       "VPCT                                    0.999025  0.999894    0.998689  \n",
       "VideoMME                                1.000000  0.999036    0.999436  \n",
       "WeirdML                                 0.999036  1.000000    0.998702  \n",
       "Winogrande                              0.999436  0.998702    1.000000  \n",
       "\n",
       "[39 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean off-diagonal Spearman per fit (difficulties):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>0.999616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIQA</th>\n",
       "      <td>0.999526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>0.999515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-Tier-4-2025-07-01-Private</th>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMBADA</th>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>0.999424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction.LiveBench</th>\n",
       "      <td>0.999393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>0.999392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LiveBench</th>\n",
       "      <td>0.999392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>0.999387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>0.999387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>0.999386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>0.999380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>0.999380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>0.999378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>0.999369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.999369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>0.999357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.999357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>0.999344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lech Mazur Writing</th>\n",
       "      <td>0.999335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>0.999329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>0.999315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>0.999302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>0.999292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>0.999292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>0.999292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Agent Company</th>\n",
       "      <td>0.999286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>0.999285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>0.999250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.999162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM8K</th>\n",
       "      <td>0.999133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.999121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.999119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceQA</th>\n",
       "      <td>0.999104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.999094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.999091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>0.999073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        mean_rho\n",
       "anchor_benchmark                                \n",
       "OSUniverse                              0.999616\n",
       "PIQA                                    0.999526\n",
       "GSO-Bench                               0.999515\n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999436\n",
       "LAMBADA                                 0.999436\n",
       "TriviaQA                                0.999424\n",
       "Fiction.LiveBench                       0.999393\n",
       "DeepResearch Bench                      0.999392\n",
       "LiveBench                               0.999392\n",
       "FrontierMath-2025-02-28-Private         0.999390\n",
       "Aider polyglot                          0.999387\n",
       "GPQA diamond                            0.999387\n",
       "Cybench                                 0.999386\n",
       "OTIS Mock AIME 2024-2025                0.999380\n",
       "MATH level 5                            0.999380\n",
       "Factorio learning environment           0.999378\n",
       "CadEval                                 0.999369\n",
       "GeoBench                                0.999369\n",
       "OSWorld                                 0.999357\n",
       "ANLI                                    0.999357\n",
       "ARC-AGI                                 0.999344\n",
       "Lech Mazur Writing                      0.999335\n",
       "SWE-Bench verified                      0.999329\n",
       "WeirdML                                 0.999315\n",
       "SimpleBench                             0.999302\n",
       "VPCT                                    0.999292\n",
       "Terminal Bench                          0.999292\n",
       "VideoMME                                0.999292\n",
       "The Agent Company                       0.999286\n",
       "Balrog                                  0.999285\n",
       "ARC AI2                                 0.999250\n",
       "BoolQ                                   0.999162\n",
       "GSM8K                                   0.999133\n",
       "Winogrande                              0.999121\n",
       "BBH                                     0.999119\n",
       "ScienceQA                               0.999104\n",
       "HellaSwag                               0.999094\n",
       "MMLU                                    0.999091\n",
       "OpenBookQA                              0.999073"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Spearman rank correlation across fits (model capabilities) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th>ANLI</th>\n",
       "      <th>ARC AI2</th>\n",
       "      <th>ARC-AGI</th>\n",
       "      <th>Aider polyglot</th>\n",
       "      <th>BBH</th>\n",
       "      <th>Balrog</th>\n",
       "      <th>BoolQ</th>\n",
       "      <th>CadEval</th>\n",
       "      <th>Cybench</th>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <th>...</th>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <th>ScienceQA</th>\n",
       "      <th>SimpleBench</th>\n",
       "      <th>Terminal Bench</th>\n",
       "      <th>The Agent Company</th>\n",
       "      <th>TriviaQA</th>\n",
       "      <th>VPCT</th>\n",
       "      <th>VideoMME</th>\n",
       "      <th>WeirdML</th>\n",
       "      <th>Winogrande</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>0.999741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.999888</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999888</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0.999927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction.LiveBench</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999704</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-Tier-4-2025-07-01-Private</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999792</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM8K</th>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999736</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999770</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMBADA</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lech Mazur Writing</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LiveBench</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.999770</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIQA</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceQA</th>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Agent Company</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.999770</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.999770</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "anchor_benchmark                            ANLI   ARC AI2   ARC-AGI  \\\n",
       "anchor_benchmark                                                       \n",
       "ANLI                                    1.000000  0.999741  0.999985   \n",
       "ARC AI2                                 0.999741  1.000000  0.999724   \n",
       "ARC-AGI                                 0.999985  0.999724  1.000000   \n",
       "Aider polyglot                          0.999968  0.999753  0.999958   \n",
       "BBH                                     0.999714  0.999976  0.999697   \n",
       "Balrog                                  0.999995  0.999734  0.999985   \n",
       "BoolQ                                   0.999932  0.999905  0.999917   \n",
       "CadEval                                 0.999998  0.999739  0.999988   \n",
       "Cybench                                 0.999998  0.999739  0.999983   \n",
       "DeepResearch Bench                      0.999995  0.999719  0.999985   \n",
       "Factorio learning environment           0.999998  0.999736  0.999988   \n",
       "Fiction.LiveBench                       0.999988  0.999731  0.999978   \n",
       "FrontierMath-2025-02-28-Private         1.000000  0.999741  0.999985   \n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999995  0.999719  0.999985   \n",
       "GPQA diamond                            0.999973  0.999792  0.999956   \n",
       "GSM8K                                   0.999751  0.999980  0.999734   \n",
       "GSO-Bench                               0.999995  0.999719  0.999985   \n",
       "GeoBench                                0.999995  0.999736  0.999980   \n",
       "HellaSwag                               0.999778  0.999983  0.999761   \n",
       "LAMBADA                                 0.999995  0.999734  0.999980   \n",
       "Lech Mazur Writing                      1.000000  0.999741  0.999985   \n",
       "LiveBench                               0.999988  0.999744  0.999971   \n",
       "MATH level 5                            0.999963  0.999827  0.999946   \n",
       "MMLU                                    0.999748  0.999985  0.999731   \n",
       "OSUniverse                              0.999988  0.999712  0.999978   \n",
       "OSWorld                                 1.000000  0.999741  0.999985   \n",
       "OTIS Mock AIME 2024-2025                0.999980  0.999773  0.999963   \n",
       "OpenBookQA                              0.999785  0.999971  0.999768   \n",
       "PIQA                                    0.999995  0.999775  0.999980   \n",
       "SWE-Bench verified                      0.999980  0.999719  0.999980   \n",
       "ScienceQA                               0.999971  0.999812  0.999956   \n",
       "SimpleBench                             0.999985  0.999746  0.999968   \n",
       "Terminal Bench                          0.999988  0.999726  0.999978   \n",
       "The Agent Company                       0.999995  0.999719  0.999985   \n",
       "TriviaQA                                0.999980  0.999766  0.999966   \n",
       "VPCT                                    1.000000  0.999741  0.999985   \n",
       "VideoMME                                0.999995  0.999719  0.999985   \n",
       "WeirdML                                 0.999995  0.999758  0.999980   \n",
       "Winogrande                              0.999770  0.999983  0.999753   \n",
       "\n",
       "anchor_benchmark                        Aider polyglot       BBH    Balrog  \\\n",
       "anchor_benchmark                                                             \n",
       "ANLI                                          0.999968  0.999714  0.999995   \n",
       "ARC AI2                                       0.999753  0.999976  0.999734   \n",
       "ARC-AGI                                       0.999958  0.999697  0.999985   \n",
       "Aider polyglot                                1.000000  0.999726  0.999968   \n",
       "BBH                                           0.999726  1.000000  0.999707   \n",
       "Balrog                                        0.999968  0.999707  1.000000   \n",
       "BoolQ                                         0.999927  0.999888  0.999927   \n",
       "CadEval                                       0.999966  0.999712  0.999993   \n",
       "Cybench                                       0.999971  0.999712  0.999993   \n",
       "DeepResearch Bench                            0.999966  0.999692  0.999995   \n",
       "Factorio learning environment                 0.999971  0.999709  0.999998   \n",
       "Fiction.LiveBench                             0.999966  0.999704  0.999988   \n",
       "FrontierMath-2025-02-28-Private               0.999968  0.999714  0.999995   \n",
       "FrontierMath-Tier-4-2025-07-01-Private        0.999966  0.999692  0.999995   \n",
       "GPQA diamond                                  0.999956  0.999775  0.999966   \n",
       "GSM8K                                         0.999761  0.999971  0.999744   \n",
       "GSO-Bench                                     0.999966  0.999692  0.999995   \n",
       "GeoBench                                      0.999968  0.999709  0.999990   \n",
       "HellaSwag                                     0.999785  0.999973  0.999770   \n",
       "LAMBADA                                       0.999961  0.999714  0.999990   \n",
       "Lech Mazur Writing                            0.999968  0.999714  0.999995   \n",
       "LiveBench                                     0.999978  0.999717  0.999980   \n",
       "MATH level 5                                  0.999944  0.999812  0.999956   \n",
       "MMLU                                          0.999761  0.999973  0.999739   \n",
       "OSUniverse                                    0.999958  0.999685  0.999988   \n",
       "OSWorld                                       0.999968  0.999714  0.999995   \n",
       "OTIS Mock AIME 2024-2025                      0.999963  0.999751  0.999973   \n",
       "OpenBookQA                                    0.999795  0.999973  0.999778   \n",
       "PIQA                                          0.999968  0.999753  0.999990   \n",
       "SWE-Bench verified                            0.999958  0.999692  0.999980   \n",
       "ScienceQA                                     0.999956  0.999785  0.999966   \n",
       "SimpleBench                                   0.999980  0.999719  0.999978   \n",
       "Terminal Bench                                0.999966  0.999700  0.999988   \n",
       "The Agent Company                             0.999966  0.999692  0.999995   \n",
       "TriviaQA                                      0.999958  0.999758  0.999976   \n",
       "VPCT                                          0.999968  0.999714  0.999995   \n",
       "VideoMME                                      0.999966  0.999692  0.999995   \n",
       "WeirdML                                       0.999973  0.999731  0.999990   \n",
       "Winogrande                                    0.999780  0.999980  0.999763   \n",
       "\n",
       "anchor_benchmark                           BoolQ   CadEval   Cybench  \\\n",
       "anchor_benchmark                                                       \n",
       "ANLI                                    0.999932  0.999998  0.999998   \n",
       "ARC AI2                                 0.999905  0.999739  0.999739   \n",
       "ARC-AGI                                 0.999917  0.999988  0.999983   \n",
       "Aider polyglot                          0.999927  0.999966  0.999971   \n",
       "BBH                                     0.999888  0.999712  0.999712   \n",
       "Balrog                                  0.999927  0.999993  0.999993   \n",
       "BoolQ                                   1.000000  0.999929  0.999929   \n",
       "CadEval                                 0.999929  1.000000  0.999995   \n",
       "Cybench                                 0.999929  0.999995  1.000000   \n",
       "DeepResearch Bench                      0.999922  0.999993  0.999993   \n",
       "Factorio learning environment           0.999929  0.999995  0.999995   \n",
       "Fiction.LiveBench                       0.999924  0.999985  0.999985   \n",
       "FrontierMath-2025-02-28-Private         0.999932  0.999998  0.999998   \n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999922  0.999993  0.999993   \n",
       "GPQA diamond                            0.999951  0.999971  0.999971   \n",
       "GSM8K                                   0.999905  0.999748  0.999748   \n",
       "GSO-Bench                               0.999922  0.999993  0.999993   \n",
       "GeoBench                                0.999927  0.999993  0.999998   \n",
       "HellaSwag                               0.999927  0.999775  0.999775   \n",
       "LAMBADA                                 0.999924  0.999993  0.999993   \n",
       "Lech Mazur Writing                      0.999932  0.999998  0.999998   \n",
       "LiveBench                               0.999927  0.999985  0.999990   \n",
       "MATH level 5                            0.999966  0.999961  0.999961   \n",
       "MMLU                                    0.999912  0.999746  0.999746   \n",
       "OSUniverse                              0.999915  0.999985  0.999985   \n",
       "OSWorld                                 0.999932  0.999998  0.999998   \n",
       "OTIS Mock AIME 2024-2025                0.999941  0.999978  0.999978   \n",
       "OpenBookQA                              0.999932  0.999783  0.999783   \n",
       "PIQA                                    0.999946  0.999993  0.999993   \n",
       "SWE-Bench verified                      0.999912  0.999983  0.999983   \n",
       "ScienceQA                               0.999963  0.999968  0.999968   \n",
       "SimpleBench                             0.999929  0.999983  0.999983   \n",
       "Terminal Bench                          0.999919  0.999985  0.999990   \n",
       "The Agent Company                       0.999922  0.999993  0.999993   \n",
       "TriviaQA                                0.999936  0.999978  0.999978   \n",
       "VPCT                                    0.999932  0.999998  0.999998   \n",
       "VideoMME                                0.999922  0.999993  0.999993   \n",
       "WeirdML                                 0.999939  0.999993  0.999993   \n",
       "Winogrande                              0.999927  0.999768  0.999768   \n",
       "\n",
       "anchor_benchmark                        DeepResearch Bench  ...  \\\n",
       "anchor_benchmark                                            ...   \n",
       "ANLI                                              0.999995  ...   \n",
       "ARC AI2                                           0.999719  ...   \n",
       "ARC-AGI                                           0.999985  ...   \n",
       "Aider polyglot                                    0.999966  ...   \n",
       "BBH                                               0.999692  ...   \n",
       "Balrog                                            0.999995  ...   \n",
       "BoolQ                                             0.999922  ...   \n",
       "CadEval                                           0.999993  ...   \n",
       "Cybench                                           0.999993  ...   \n",
       "DeepResearch Bench                                1.000000  ...   \n",
       "Factorio learning environment                     0.999998  ...   \n",
       "Fiction.LiveBench                                 0.999988  ...   \n",
       "FrontierMath-2025-02-28-Private                   0.999995  ...   \n",
       "FrontierMath-Tier-4-2025-07-01-Private            1.000000  ...   \n",
       "GPQA diamond                                      0.999963  ...   \n",
       "GSM8K                                             0.999731  ...   \n",
       "GSO-Bench                                         1.000000  ...   \n",
       "GeoBench                                          0.999990  ...   \n",
       "HellaSwag                                         0.999761  ...   \n",
       "LAMBADA                                           0.999990  ...   \n",
       "Lech Mazur Writing                                0.999995  ...   \n",
       "LiveBench                                         0.999980  ...   \n",
       "MATH level 5                                      0.999954  ...   \n",
       "MMLU                                              0.999729  ...   \n",
       "OSUniverse                                        0.999993  ...   \n",
       "OSWorld                                           0.999995  ...   \n",
       "OTIS Mock AIME 2024-2025                          0.999971  ...   \n",
       "OpenBookQA                                        0.999768  ...   \n",
       "PIQA                                              0.999988  ...   \n",
       "SWE-Bench verified                                0.999980  ...   \n",
       "ScienceQA                                         0.999963  ...   \n",
       "SimpleBench                                       0.999978  ...   \n",
       "Terminal Bench                                    0.999988  ...   \n",
       "The Agent Company                                 1.000000  ...   \n",
       "TriviaQA                                          0.999973  ...   \n",
       "VPCT                                              0.999995  ...   \n",
       "VideoMME                                          1.000000  ...   \n",
       "WeirdML                                           0.999988  ...   \n",
       "Winogrande                                        0.999753  ...   \n",
       "\n",
       "anchor_benchmark                        SWE-Bench verified  ScienceQA  \\\n",
       "anchor_benchmark                                                        \n",
       "ANLI                                              0.999980   0.999971   \n",
       "ARC AI2                                           0.999719   0.999812   \n",
       "ARC-AGI                                           0.999980   0.999956   \n",
       "Aider polyglot                                    0.999958   0.999956   \n",
       "BBH                                               0.999692   0.999785   \n",
       "Balrog                                            0.999980   0.999966   \n",
       "BoolQ                                             0.999912   0.999963   \n",
       "CadEval                                           0.999983   0.999968   \n",
       "Cybench                                           0.999983   0.999968   \n",
       "DeepResearch Bench                                0.999980   0.999963   \n",
       "Factorio learning environment                     0.999983   0.999968   \n",
       "Fiction.LiveBench                                 0.999973   0.999963   \n",
       "FrontierMath-2025-02-28-Private                   0.999980   0.999971   \n",
       "FrontierMath-Tier-4-2025-07-01-Private            0.999980   0.999963   \n",
       "GPQA diamond                                      0.999946   0.999983   \n",
       "GSM8K                                             0.999729   0.999822   \n",
       "GSO-Bench                                         0.999980   0.999963   \n",
       "GeoBench                                          0.999980   0.999966   \n",
       "HellaSwag                                         0.999756   0.999839   \n",
       "LAMBADA                                           0.999976   0.999966   \n",
       "Lech Mazur Writing                                0.999980   0.999971   \n",
       "LiveBench                                         0.999971   0.999958   \n",
       "MATH level 5                                      0.999939   0.999983   \n",
       "MMLU                                              0.999726   0.999827   \n",
       "OSUniverse                                        0.999973   0.999956   \n",
       "OSWorld                                           0.999980   0.999971   \n",
       "OTIS Mock AIME 2024-2025                          0.999954   0.999985   \n",
       "OpenBookQA                                        0.999763   0.999851   \n",
       "PIQA                                              0.999976   0.999971   \n",
       "SWE-Bench verified                                1.000000   0.999951   \n",
       "ScienceQA                                         0.999951   1.000000   \n",
       "SimpleBench                                       0.999963   0.999961   \n",
       "Terminal Bench                                    0.999993   0.999958   \n",
       "The Agent Company                                 0.999980   0.999963   \n",
       "TriviaQA                                          0.999961   0.999971   \n",
       "VPCT                                              0.999980   0.999971   \n",
       "VideoMME                                          0.999980   0.999963   \n",
       "WeirdML                                           0.999976   0.999976   \n",
       "Winogrande                                        0.999748   0.999844   \n",
       "\n",
       "anchor_benchmark                        SimpleBench  Terminal Bench  \\\n",
       "anchor_benchmark                                                      \n",
       "ANLI                                       0.999985        0.999988   \n",
       "ARC AI2                                    0.999746        0.999726   \n",
       "ARC-AGI                                    0.999968        0.999978   \n",
       "Aider polyglot                             0.999980        0.999966   \n",
       "BBH                                        0.999719        0.999700   \n",
       "Balrog                                     0.999978        0.999988   \n",
       "BoolQ                                      0.999929        0.999919   \n",
       "CadEval                                    0.999983        0.999985   \n",
       "Cybench                                    0.999983        0.999990   \n",
       "DeepResearch Bench                         0.999978        0.999988   \n",
       "Factorio learning environment              0.999980        0.999990   \n",
       "Fiction.LiveBench                          0.999976        0.999980   \n",
       "FrontierMath-2025-02-28-Private            0.999985        0.999988   \n",
       "FrontierMath-Tier-4-2025-07-01-Private     0.999978        0.999988   \n",
       "GPQA diamond                               0.999963        0.999954   \n",
       "GSM8K                                      0.999756        0.999736   \n",
       "GSO-Bench                                  0.999978        0.999988   \n",
       "GeoBench                                   0.999980        0.999988   \n",
       "HellaSwag                                  0.999783        0.999763   \n",
       "LAMBADA                                    0.999980        0.999983   \n",
       "Lech Mazur Writing                         0.999985        0.999988   \n",
       "LiveBench                                  0.999993        0.999978   \n",
       "MATH level 5                               0.999949        0.999946   \n",
       "MMLU                                       0.999753        0.999734   \n",
       "OSUniverse                                 0.999971        0.999980   \n",
       "OSWorld                                    0.999985        0.999988   \n",
       "OTIS Mock AIME 2024-2025                   0.999971        0.999961   \n",
       "OpenBookQA                                 0.999790        0.999770   \n",
       "PIQA                                       0.999980        0.999983   \n",
       "SWE-Bench verified                         0.999963        0.999993   \n",
       "ScienceQA                                  0.999961        0.999958   \n",
       "SimpleBench                                1.000000        0.999971   \n",
       "Terminal Bench                             0.999971        1.000000   \n",
       "The Agent Company                          0.999978        0.999988   \n",
       "TriviaQA                                   0.999966        0.999968   \n",
       "VPCT                                       0.999985        0.999988   \n",
       "VideoMME                                   0.999978        0.999988   \n",
       "WeirdML                                    0.999985        0.999983   \n",
       "Winogrande                                 0.999775        0.999756   \n",
       "\n",
       "anchor_benchmark                        The Agent Company  TriviaQA      VPCT  \\\n",
       "anchor_benchmark                                                                \n",
       "ANLI                                             0.999995  0.999980  1.000000   \n",
       "ARC AI2                                          0.999719  0.999766  0.999741   \n",
       "ARC-AGI                                          0.999985  0.999966  0.999985   \n",
       "Aider polyglot                                   0.999966  0.999958  0.999968   \n",
       "BBH                                              0.999692  0.999758  0.999714   \n",
       "Balrog                                           0.999995  0.999976  0.999995   \n",
       "BoolQ                                            0.999922  0.999936  0.999932   \n",
       "CadEval                                          0.999993  0.999978  0.999998   \n",
       "Cybench                                          0.999993  0.999978  0.999998   \n",
       "DeepResearch Bench                               1.000000  0.999973  0.999995   \n",
       "Factorio learning environment                    0.999998  0.999978  0.999998   \n",
       "Fiction.LiveBench                                0.999988  0.999968  0.999988   \n",
       "FrontierMath-2025-02-28-Private                  0.999995  0.999980  1.000000   \n",
       "FrontierMath-Tier-4-2025-07-01-Private           1.000000  0.999973  0.999995   \n",
       "GPQA diamond                                     0.999963  0.999973  0.999973   \n",
       "GSM8K                                            0.999731  0.999787  0.999751   \n",
       "GSO-Bench                                        1.000000  0.999973  0.999995   \n",
       "GeoBench                                         0.999990  0.999976  0.999995   \n",
       "HellaSwag                                        0.999761  0.999797  0.999778   \n",
       "LAMBADA                                          0.999990  0.999980  0.999995   \n",
       "Lech Mazur Writing                               0.999995  0.999980  1.000000   \n",
       "LiveBench                                        0.999980  0.999968  0.999988   \n",
       "MATH level 5                                     0.999954  0.999968  0.999963   \n",
       "MMLU                                             0.999729  0.999780  0.999748   \n",
       "OSUniverse                                       0.999993  0.999966  0.999988   \n",
       "OSWorld                                          0.999995  0.999980  1.000000   \n",
       "OTIS Mock AIME 2024-2025                         0.999971  0.999978  0.999980   \n",
       "OpenBookQA                                       0.999768  0.999814  0.999785   \n",
       "PIQA                                             0.999988  0.999985  0.999995   \n",
       "SWE-Bench verified                               0.999980  0.999961  0.999980   \n",
       "ScienceQA                                        0.999963  0.999971  0.999971   \n",
       "SimpleBench                                      0.999978  0.999966  0.999985   \n",
       "Terminal Bench                                   0.999988  0.999968  0.999988   \n",
       "The Agent Company                                1.000000  0.999973  0.999995   \n",
       "TriviaQA                                         0.999973  1.000000  0.999980   \n",
       "VPCT                                             0.999995  0.999980  1.000000   \n",
       "VideoMME                                         1.000000  0.999973  0.999995   \n",
       "WeirdML                                          0.999988  0.999980  0.999995   \n",
       "Winogrande                                       0.999753  0.999800  0.999770   \n",
       "\n",
       "anchor_benchmark                        VideoMME   WeirdML  Winogrande  \n",
       "anchor_benchmark                                                        \n",
       "ANLI                                    0.999995  0.999995    0.999770  \n",
       "ARC AI2                                 0.999719  0.999758    0.999983  \n",
       "ARC-AGI                                 0.999985  0.999980    0.999753  \n",
       "Aider polyglot                          0.999966  0.999973    0.999780  \n",
       "BBH                                     0.999692  0.999731    0.999980  \n",
       "Balrog                                  0.999995  0.999990    0.999763  \n",
       "BoolQ                                   0.999922  0.999939    0.999927  \n",
       "CadEval                                 0.999993  0.999993    0.999768  \n",
       "Cybench                                 0.999993  0.999993    0.999768  \n",
       "DeepResearch Bench                      1.000000  0.999988    0.999753  \n",
       "Factorio learning environment           0.999998  0.999993    0.999766  \n",
       "Fiction.LiveBench                       0.999988  0.999988    0.999761  \n",
       "FrontierMath-2025-02-28-Private         0.999995  0.999995    0.999770  \n",
       "FrontierMath-Tier-4-2025-07-01-Private  1.000000  0.999988    0.999753  \n",
       "GPQA diamond                            0.999963  0.999978    0.999824  \n",
       "GSM8K                                   0.999731  0.999766    0.999978  \n",
       "GSO-Bench                               1.000000  0.999988    0.999753  \n",
       "GeoBench                                0.999990  0.999990    0.999766  \n",
       "HellaSwag                               0.999761  0.999790    0.999995  \n",
       "LAMBADA                                 0.999990  0.999990    0.999766  \n",
       "Lech Mazur Writing                      0.999995  0.999995    0.999770  \n",
       "LiveBench                               0.999980  0.999983    0.999773  \n",
       "MATH level 5                            0.999954  0.999968    0.999858  \n",
       "MMLU                                    0.999729  0.999763    0.999985  \n",
       "OSUniverse                              0.999993  0.999980    0.999746  \n",
       "OSWorld                                 0.999995  0.999995    0.999770  \n",
       "OTIS Mock AIME 2024-2025                0.999971  0.999985    0.999805  \n",
       "OpenBookQA                              0.999768  0.999797    0.999988  \n",
       "PIQA                                    0.999988  0.999995    0.999800  \n",
       "SWE-Bench verified                      0.999980  0.999976    0.999748  \n",
       "ScienceQA                               0.999963  0.999976    0.999844  \n",
       "SimpleBench                             0.999978  0.999985    0.999775  \n",
       "Terminal Bench                          0.999988  0.999983    0.999756  \n",
       "The Agent Company                       1.000000  0.999988    0.999753  \n",
       "TriviaQA                                0.999973  0.999980    0.999800  \n",
       "VPCT                                    0.999995  0.999995    0.999770  \n",
       "VideoMME                                1.000000  0.999988    0.999753  \n",
       "WeirdML                                 0.999988  1.000000    0.999783  \n",
       "Winogrande                              0.999753  0.999783    1.000000  \n",
       "\n",
       "[39 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean off-diagonal Spearman per fit (capabilities):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor_benchmark</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PIQA</th>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lech Mazur Writing</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>0.999941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScienceQA</th>\n",
       "      <td>0.999941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.999941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMBADA</th>\n",
       "      <td>0.999940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-Tier-4-2025-07-01-Private</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Agent Company</th>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>0.999938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>0.999938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>0.999937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LiveBench</th>\n",
       "      <td>0.999937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction.LiveBench</th>\n",
       "      <td>0.999936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>0.999936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>0.999934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>0.999932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>0.999931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>0.999928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>0.999928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.999927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>0.999823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.999812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSM8K</th>\n",
       "      <td>0.999794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.999793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>0.999786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.999764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        mean_rho\n",
       "anchor_benchmark                                \n",
       "PIQA                                    0.999948\n",
       "WeirdML                                 0.999945\n",
       "ANLI                                    0.999945\n",
       "OSWorld                                 0.999945\n",
       "Lech Mazur Writing                      0.999945\n",
       "FrontierMath-2025-02-28-Private         0.999945\n",
       "VPCT                                    0.999945\n",
       "Factorio learning environment           0.999943\n",
       "Cybench                                 0.999943\n",
       "CadEval                                 0.999943\n",
       "Balrog                                  0.999941\n",
       "ScienceQA                               0.999941\n",
       "GeoBench                                0.999941\n",
       "LAMBADA                                 0.999940\n",
       "OTIS Mock AIME 2024-2025                0.999939\n",
       "DeepResearch Bench                      0.999939\n",
       "GSO-Bench                               0.999939\n",
       "FrontierMath-Tier-4-2025-07-01-Private  0.999939\n",
       "VideoMME                                0.999939\n",
       "The Agent Company                       0.999939\n",
       "TriviaQA                                0.999938\n",
       "GPQA diamond                            0.999938\n",
       "MATH level 5                            0.999937\n",
       "LiveBench                               0.999937\n",
       "Fiction.LiveBench                       0.999936\n",
       "SimpleBench                             0.999936\n",
       "Terminal Bench                          0.999934\n",
       "ARC-AGI                                 0.999932\n",
       "OSUniverse                              0.999931\n",
       "SWE-Bench verified                      0.999928\n",
       "Aider polyglot                          0.999928\n",
       "BoolQ                                   0.999927\n",
       "OpenBookQA                              0.999823\n",
       "HellaSwag                               0.999816\n",
       "Winogrande                              0.999812\n",
       "GSM8K                                   0.999794\n",
       "MMLU                                    0.999793\n",
       "ARC AI2                                 0.999786\n",
       "BBH                                     0.999764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4)  Rank correlation across fits (Spearman) for difficulties and capabilities\n",
    "#     Robust to duplicates and missing values\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _spearman_corr_from_long(\n",
    "    df_long: pd.DataFrame, index_col: str, columns_col: str, values_col: str\n",
    ") -> pd.DataFrame:\n",
    "    # Allow duplicates by aggregating with mean; coerce to numeric in case of stray dtypes\n",
    "    wide = df_long.pivot_table(\n",
    "        index=index_col,\n",
    "        columns=columns_col,\n",
    "        values=values_col,\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "    wide = wide.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows that are entirely NaN\n",
    "    wide = wide.dropna(axis=0, how=\"all\")\n",
    "\n",
    "    # Need at least two fits (columns) to compute a correlation matrix\n",
    "    if wide.shape[1] < 2:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Compute Spearman by ranking then applying Pearson correlation\n",
    "    ranks = wide.rank(axis=0, method=\"average\", na_option=\"keep\")\n",
    "    corr = ranks.corr(method=\"pearson\", min_periods=2)\n",
    "    return corr\n",
    "\n",
    "\n",
    "print(\"=== Spearman rank correlation across fits (benchmark difficulties) ===\")\n",
    "spearman_difficulty = _spearman_corr_from_long(\n",
    "    difficulty_long,\n",
    "    index_col=\"benchmark_name\",\n",
    "    columns_col=\"anchor_benchmark\",\n",
    "    values_col=\"estimated_difficulty\",\n",
    ")\n",
    "if spearman_difficulty.empty:\n",
    "    print(\"Not enough comparable fits to compute correlations for difficulties.\")\n",
    "else:\n",
    "    display(spearman_difficulty)\n",
    "    mean_rho_difficulty = (\n",
    "        spearman_difficulty.apply(lambda s: s.drop(labels=s.name).mean(), axis=0)\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(\"mean_rho\")\n",
    "    )\n",
    "    print(\"\\nMean off-diagonal Spearman per fit (difficulties):\")\n",
    "    display(mean_rho_difficulty)\n",
    "\n",
    "print(\"\\n=== Spearman rank correlation across fits (model capabilities) ===\")\n",
    "spearman_capability = _spearman_corr_from_long(\n",
    "    capability_long,\n",
    "    index_col=\"model\",\n",
    "    columns_col=\"anchor_benchmark\",\n",
    "    values_col=\"estimated_capability\",\n",
    ")\n",
    "if spearman_capability.empty:\n",
    "    print(\"Not enough comparable fits to compute correlations for capabilities.\")\n",
    "else:\n",
    "    display(spearman_capability)\n",
    "    mean_rho_capability = (\n",
    "        spearman_capability.apply(lambda s: s.drop(labels=s.name).mean(), axis=0)\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(\"mean_rho\")\n",
    "    )\n",
    "    print(\"\\nMean off-diagonal Spearman per fit (capabilities):\")\n",
    "    display(mean_rho_capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bf280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 4.8400e+01, final cost 3.5160e+00, first-order optimality 1.46e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 29, initial cost 4.8698e+01, final cost 3.5189e+00, first-order optimality 2.71e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 32, initial cost 4.8166e+01, final cost 3.5215e+00, first-order optimality 1.30e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 36, initial cost 4.8335e+01, final cost 3.5233e+00, first-order optimality 1.65e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 36, initial cost 4.8702e+01, final cost 3.5239e+00, first-order optimality 5.62e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 38, initial cost 4.8765e+01, final cost 3.5246e+00, first-order optimality 2.31e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.8954e+01, final cost 3.5251e+00, first-order optimality 2.86e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8652e+01, final cost 3.5252e+00, first-order optimality 8.60e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.8416e+01, final cost 3.5253e+00, first-order optimality 1.10e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 35, initial cost 4.8972e+01, final cost 3.5265e+00, first-order optimality 2.28e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 36, initial cost 4.8552e+01, final cost 3.5269e+00, first-order optimality 1.22e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.7962e+01, final cost 3.5282e+00, first-order optimality 1.03e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.7901e+01, final cost 3.5285e+00, first-order optimality 2.73e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.8181e+01, final cost 3.5282e+00, first-order optimality 1.80e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 38, initial cost 4.9282e+01, final cost 3.5288e+00, first-order optimality 3.66e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.9170e+01, final cost 3.5289e+00, first-order optimality 1.48e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.8366e+01, final cost 3.5286e+00, first-order optimality 3.66e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.8460e+01, final cost 3.5287e+00, first-order optimality 2.29e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 4.8011e+01, final cost 3.5282e+00, first-order optimality 9.21e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.7712e+01, final cost 3.5287e+00, first-order optimality 1.01e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.8278e+01, final cost 3.5293e+00, first-order optimality 1.13e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 31, initial cost 4.8472e+01, final cost 3.5298e+00, first-order optimality 3.07e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 28, initial cost 4.8346e+01, final cost 3.5298e+00, first-order optimality 3.08e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8044e+01, final cost 3.5294e+00, first-order optimality 1.09e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.8422e+01, final cost 3.5297e+00, first-order optimality 2.09e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.8306e+01, final cost 3.5301e+00, first-order optimality 4.09e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.8034e+01, final cost 3.5303e+00, first-order optimality 8.72e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 38, initial cost 4.8401e+01, final cost 3.5304e+00, first-order optimality 2.78e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.8405e+01, final cost 3.5309e+00, first-order optimality 1.14e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 34, initial cost 4.8785e+01, final cost 3.5313e+00, first-order optimality 1.94e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 4.9029e+01, final cost 3.5313e+00, first-order optimality 8.58e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 46, initial cost 4.9361e+01, final cost 3.5311e+00, first-order optimality 1.34e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9399e+01, final cost 3.5315e+00, first-order optimality 1.45e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.8950e+01, final cost 3.5319e+00, first-order optimality 1.64e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 37, initial cost 4.9284e+01, final cost 3.5321e+00, first-order optimality 1.48e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.9268e+01, final cost 3.5318e+00, first-order optimality 3.46e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.8960e+01, final cost 3.5323e+00, first-order optimality 7.97e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8452e+01, final cost 3.5324e+00, first-order optimality 1.20e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.8323e+01, final cost 3.5322e+00, first-order optimality 9.78e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 47, initial cost 4.7396e+01, final cost 3.5324e+00, first-order optimality 8.65e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 47, initial cost 4.7139e+01, final cost 3.5326e+00, first-order optimality 1.14e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9451e+01, final cost 3.5333e+00, first-order optimality 1.03e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 5.0154e+01, final cost 3.5338e+00, first-order optimality 1.18e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 5.0076e+01, final cost 3.5340e+00, first-order optimality 1.64e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.9496e+01, final cost 3.5340e+00, first-order optimality 2.81e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9873e+01, final cost 3.5340e+00, first-order optimality 1.28e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 41, initial cost 4.9921e+01, final cost 3.5343e+00, first-order optimality 2.29e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 4.9248e+01, final cost 3.5344e+00, first-order optimality 2.22e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9593e+01, final cost 3.5345e+00, first-order optimality 8.18e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9634e+01, final cost 3.5346e+00, first-order optimality 1.32e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 35, initial cost 4.9319e+01, final cost 3.5346e+00, first-order optimality 7.98e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.8308e+01, final cost 3.5346e+00, first-order optimality 1.83e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8373e+01, final cost 3.5351e+00, first-order optimality 4.62e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 46, initial cost 4.9165e+01, final cost 3.5355e+00, first-order optimality 8.91e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 41, initial cost 5.0097e+01, final cost 3.5357e+00, first-order optimality 4.79e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 5.0552e+01, final cost 3.5360e+00, first-order optimality 6.64e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 4.9881e+01, final cost 3.5362e+00, first-order optimality 1.16e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8472e+01, final cost 3.5367e+00, first-order optimality 1.28e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 34, initial cost 4.8693e+01, final cost 3.5369e+00, first-order optimality 2.47e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 41, initial cost 4.9429e+01, final cost 3.5372e+00, first-order optimality 1.14e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.8093e+01, final cost 3.5369e+00, first-order optimality 1.36e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.7568e+01, final cost 3.5380e+00, first-order optimality 4.88e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 47, initial cost 4.8863e+01, final cost 3.5385e+00, first-order optimality 2.15e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.8904e+01, final cost 3.5385e+00, first-order optimality 9.95e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 46, initial cost 4.8570e+01, final cost 3.5386e+00, first-order optimality 6.34e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.7508e+01, final cost 3.5382e+00, first-order optimality 1.40e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.8299e+01, final cost 3.5383e+00, first-order optimality 1.62e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.9417e+01, final cost 3.5387e+00, first-order optimality 3.76e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.9508e+01, final cost 3.5385e+00, first-order optimality 1.63e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.9271e+01, final cost 3.5386e+00, first-order optimality 8.36e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.8526e+01, final cost 3.5378e+00, first-order optimality 1.02e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.7331e+01, final cost 3.5292e+00, first-order optimality 1.52e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 50, initial cost 4.9010e+01, final cost 3.5386e+00, first-order optimality 1.41e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 5.0469e+01, final cost 3.5393e+00, first-order optimality 3.03e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 46, initial cost 4.9269e+01, final cost 3.5394e+00, first-order optimality 2.40e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8547e+01, final cost 3.5395e+00, first-order optimality 3.31e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8750e+01, final cost 3.5396e+00, first-order optimality 1.01e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 47, initial cost 4.9633e+01, final cost 3.5399e+00, first-order optimality 1.86e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 4.9031e+01, final cost 3.5399e+00, first-order optimality 1.52e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 54, initial cost 4.7954e+01, final cost 3.5391e+00, first-order optimality 1.78e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.7986e+01, final cost 3.5392e+00, first-order optimality 1.57e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.8924e+01, final cost 3.5399e+00, first-order optimality 5.80e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 42, initial cost 4.9296e+01, final cost 3.5400e+00, first-order optimality 6.70e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8981e+01, final cost 3.5401e+00, first-order optimality 1.50e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.8620e+01, final cost 3.5401e+00, first-order optimality 1.11e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.9241e+01, final cost 3.5404e+00, first-order optimality 3.05e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.9673e+01, final cost 3.5399e+00, first-order optimality 3.62e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8930e+01, final cost 3.5396e+00, first-order optimality 4.57e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8436e+01, final cost 3.5403e+00, first-order optimality 2.57e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8689e+01, final cost 3.5404e+00, first-order optimality 3.42e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8541e+01, final cost 3.5396e+00, first-order optimality 6.19e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 50, initial cost 4.7619e+01, final cost 3.5339e+00, first-order optimality 3.31e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 44, initial cost 4.6977e+01, final cost 3.5397e+00, first-order optimality 2.53e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 50, initial cost 4.7631e+01, final cost 3.5492e+00, first-order optimality 7.20e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 48, initial cost 4.8121e+01, final cost 3.5491e+00, first-order optimality 6.36e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 51, initial cost 4.7549e+01, final cost 3.5394e+00, first-order optimality 2.31e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 52, initial cost 4.7361e+01, final cost 3.5395e+00, first-order optimality 1.54e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.7652e+01, final cost 3.5407e+00, first-order optimality 2.64e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.6666e+01, final cost 3.5399e+00, first-order optimality 9.57e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 54, initial cost 4.7219e+01, final cost 3.5395e+00, first-order optimality 2.07e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 43, initial cost 4.8649e+01, final cost 3.5402e+00, first-order optimality 8.27e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 52, initial cost 4.8023e+01, final cost 3.5389e+00, first-order optimality 1.56e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 49, initial cost 4.7414e+01, final cost 3.5388e+00, first-order optimality 1.31e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 47, initial cost 4.6498e+01, final cost 3.5391e+00, first-order optimality 5.76e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 50, initial cost 4.6914e+01, final cost 3.5390e+00, first-order optimality 2.64e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 45, initial cost 4.8340e+01, final cost 3.5401e+00, first-order optimality 1.42e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 50, initial cost 4.7952e+01, final cost 3.5400e+00, first-order optimality 9.55e-05.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 46, initial cost 4.7242e+01, final cost 3.5389e+00, first-order optimality 3.44e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 59, initial cost 4.6851e+01, final cost 3.5350e+00, first-order optimality 1.01e-04.\n",
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 56, initial cost 4.6998e+01, final cost 3.5370e+00, first-order optimality 2.48e-04.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m pair_key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchor_model1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchor_model2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     df, df_cm, df_db = \u001b[43mfit_statistical_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscores_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_benchmark\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_benchmark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep the same benchmark\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_difficulty\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_difficulty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep the same difficulty\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_slope\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_slope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep the same slope\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_model1\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_model1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_model1_capability\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_model1_capability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_model2\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_model2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43manchor_model2_capability\u001b[49m\u001b[43m=\u001b[49m\u001b[43manchor_model2_capability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     all_runs[pair_key] = {\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdf1\u001b[39m\u001b[33m\"\u001b[39m: df,\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdf_cm1\u001b[39m\u001b[33m\"\u001b[39m: df_cm,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manchor_model2_capability\u001b[39m\u001b[33m\"\u001b[39m: anchor_model2_capability,\n\u001b[32m     54\u001b[39m     }\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/fit.py:262\u001b[39m, in \u001b[36mfit_statistical_model\u001b[39m\u001b[34m(df, anchor_mode, anchor_benchmark, anchor_difficulty, anchor_slope, anchor_model1, anchor_model1_capability, anchor_model2, anchor_model2_capability, slope_init, regularization_strength, df_model, random_state)\u001b[39m\n\u001b[32m    253\u001b[39m     upper_bounds = np.concatenate([\n\u001b[32m    254\u001b[39m         np.full(num_models - \u001b[32m2\u001b[39m, \u001b[32m10\u001b[39m),\n\u001b[32m    255\u001b[39m         np.full(num_benchmarks, \u001b[32m10\u001b[39m),\n\u001b[32m    256\u001b[39m         np.full(num_benchmarks, \u001b[32m10\u001b[39m)\n\u001b[32m    257\u001b[39m     ])\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# 6)  Fit with bounds\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m result = \u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_idx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbench_idx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_bounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add bounds to prevent extreme values\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    269\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# 7)  Recover full parameter vectors\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m    274\u001b[39m theta_hat = result.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/_lib/_util.py:689\u001b[39m, in \u001b[36m_workers_wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MapWrapper(_workers) \u001b[38;5;28;01mas\u001b[39;00m mf:\n\u001b[32m    688\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mworkers\u001b[39m\u001b[33m'\u001b[39m] = mf\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/optimize/_lsq/least_squares.py:1019\u001b[39m, in \u001b[36mleast_squares\u001b[39m\u001b[34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs, callback, workers)\u001b[39m\n\u001b[32m   1015\u001b[39m     result = call_minpack(vector_fun.fun, x0, vector_fun.jac, ftol, xtol, gtol,\n\u001b[32m   1016\u001b[39m                           max_nfev, x_scale, jac_method=jac)\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mtrf\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     result = \u001b[43mtrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_fun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_fun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mdogbox\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tr_solver == \u001b[33m'\u001b[39m\u001b[33mlsmr\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mregularize\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tr_options:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/optimize/_lsq/trf.py:124\u001b[39m, in \u001b[36mtrf\u001b[39m\u001b[34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose, callback)\u001b[39m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trf_no_bounds(\n\u001b[32m    121\u001b[39m         fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[32m    122\u001b[39m         loss_function, tr_solver, tr_options, verbose, callback=callback)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrf_bounds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/optimize/_lsq/trf.py:303\u001b[39m, in \u001b[36mtrf_bounds\u001b[39m\u001b[34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose, callback)\u001b[39m\n\u001b[32m    301\u001b[39m J_h = J_augmented[:m]  \u001b[38;5;66;03m# Memory view.\u001b[39;00m\n\u001b[32m    302\u001b[39m J_augmented[m:] = np.diag(diag_h**\u001b[32m0.5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m U, s, V = \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ_augmented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m V = V.T\n\u001b[32m    305\u001b[39m uf = U.T.dot(f_augmented)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/_lib/_util.py:1226\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1229\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Epoch/benchmark-stitching/.venv/lib/python3.11/site-packages/scipy/linalg/_decomp_svd.py:166\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[39m\n\u001b[32m    162\u001b[39m lwork = _compute_lwork(gesXd_lwork, a1.shape[\u001b[32m0\u001b[39m], a1.shape[\u001b[32m1\u001b[39m],\n\u001b[32m    163\u001b[39m                        compute_uv=compute_uv, full_matrices=full_matrices)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m u, s, v, info = \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33mSVD did not converge\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_runs = {}  # will map model_pair -> dict of outputs\n",
    "failed = []  # keep track of anything that errors out\n",
    "\n",
    "# --- Create model pairs for anchoring -------------------------------------\n",
    "# Get unique models with their estimated capabilities\n",
    "models_with_capability = df_cm_anchor[\n",
    "    [\"model\", \"estimated_capability\"]\n",
    "].drop_duplicates()\n",
    "models_list = models_with_capability.to_dict(\"records\")\n",
    "\n",
    "# Create pairs of models to use as anchors\n",
    "# You can adjust this logic based on your needs\n",
    "model_pairs = []\n",
    "for i in range(len(models_list) - 1):\n",
    "    # Each model paired with the next one\n",
    "    model_pairs.append((models_list[i], models_list[i + 1]))\n",
    "\n",
    "# Alternatively, you could pair each model with a fixed reference model:\n",
    "# reference_model = models_list[0]  # or find a specific model\n",
    "# model_pairs = [(reference_model, model) for model in models_list[1:]]\n",
    "\n",
    "# --- loop over model pairs ------------------------------------------------\n",
    "for model1_info, model2_info in model_pairs:\n",
    "    anchor_model1 = model1_info[\"model\"]\n",
    "    anchor_model1_capability = float(model1_info[\"estimated_capability\"])\n",
    "    anchor_model2 = model2_info[\"model\"]\n",
    "    anchor_model2_capability = float(model2_info[\"estimated_capability\"])\n",
    "\n",
    "    # Create a key for storing results\n",
    "    pair_key = f\"{anchor_model1}_{anchor_model2}\"\n",
    "\n",
    "    try:\n",
    "        df, df_cm, df_db = fit_statistical_model(\n",
    "            scores_df,\n",
    "            anchor_mode=\"model\",\n",
    "            anchor_benchmark=anchor_benchmark,  # Keep the same benchmark\n",
    "            anchor_difficulty=anchor_difficulty,  # Keep the same difficulty\n",
    "            anchor_slope=anchor_slope,  # Keep the same slope\n",
    "            anchor_model1=anchor_model1,\n",
    "            anchor_model1_capability=anchor_model1_capability,\n",
    "            anchor_model2=anchor_model2,\n",
    "            anchor_model2_capability=anchor_model2_capability,\n",
    "        )\n",
    "\n",
    "        all_runs[pair_key] = {\n",
    "            \"df1\": df,\n",
    "            \"df_cm1\": df_cm,\n",
    "            \"df_db\": df_db,\n",
    "            # cache the anchor values for reference\n",
    "            \"anchor_model1\": anchor_model1,\n",
    "            \"anchor_model1_capability\": anchor_model1_capability,\n",
    "            \"anchor_model2\": anchor_model2,\n",
    "            \"anchor_model2_capability\": anchor_model2_capability,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        failed.append((pair_key, str(e)))\n",
    "\n",
    "# --- post-processing (optional) ----------------------------------------------\n",
    "# 1) quick glance at what failed\n",
    "if failed:\n",
    "    print(\"Model pairs that raised errors:\", failed)\n",
    "\n",
    "# 2) pull out the model capability estimates across all runs\n",
    "summary_models = pd.concat(\n",
    "    {k: v[\"df_cm1\"][[\"model\", \"estimated_capability\"]] for k, v in all_runs.items()},\n",
    "    names=[\"anchor_model_pair\"],\n",
    ").reset_index(level=0)\n",
    "\n",
    "# 3) pull out the benchmark difficulty/slope estimates across all runs\n",
    "summary_benchmarks = pd.concat(\n",
    "    {\n",
    "        k: v[\"df_db\"][[\"benchmark_name\", \"estimated_difficulty\", \"estimated_slope\"]]\n",
    "        for k, v in all_runs.items()\n",
    "    },\n",
    "    names=[\"anchor_model_pair\"],\n",
    ").reset_index(level=0)\n",
    "\n",
    "print(f\"Processed {len(all_runs)} model pair combinations\")\n",
    "print(f\"Failed: {len(failed)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce8f885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1680x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Capabilities and difficulties over time with std error bars (from variation tables)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Expect these to exist from earlier cells:\n",
    "# - capability_stats: index=model, columns include ['mean','std']\n",
    "# - difficulty_stats: index=benchmark_name, columns include ['mean','std']\n",
    "# - df_cm_anchor: per-model table with 'model' and 'date' (string) or 'date_obj'\n",
    "# - df_db_anchor: per-benchmark table with 'benchmark_name' and 'benchmark_release_date'\n",
    "\n",
    "# Merge stats with model release dates\n",
    "cap_stats_df = capability_stats.reset_index()  # 'model', 'mean', 'std', ...\n",
    "model_dates = df_cm_anchor[[\"model\", \"date\"]].drop_duplicates(subset=[\"model\"]).copy()\n",
    "model_dates[\"date_obj\"] = pd.to_datetime(model_dates[\"date\"], errors=\"coerce\")\n",
    "cap_plot_df = (\n",
    "    cap_stats_df.merge(model_dates, on=\"model\", how=\"left\")\n",
    "    .dropna(subset=[\"date_obj\", \"mean\"])  # require a date and mean\n",
    "    .sort_values(\"date_obj\")\n",
    ")\n",
    "cap_plot_df[\"std\"] = cap_plot_df[\"std\"].fillna(0.0)\n",
    "\n",
    "# Merge stats with benchmark release dates\n",
    "diff_stats_df = difficulty_stats.reset_index()  # 'benchmark_name', 'mean', 'std', ...\n",
    "bench_dates = (\n",
    "    df_db_anchor[[\"benchmark_name\", \"benchmark_release_date\"]]\n",
    "    .drop_duplicates(subset=[\"benchmark_name\"])\n",
    "    .copy()\n",
    ")\n",
    "bench_dates[\"benchmark_release_date\"] = pd.to_datetime(\n",
    "    bench_dates[\"benchmark_release_date\"], errors=\"coerce\"\n",
    ")\n",
    "diff_plot_df = (\n",
    "    diff_stats_df.merge(bench_dates, on=\"benchmark_name\", how=\"left\")\n",
    "    .dropna(subset=[\"benchmark_release_date\", \"mean\"])  # require a date and mean\n",
    "    .sort_values(\"benchmark_release_date\")\n",
    ")\n",
    "diff_plot_df[\"std\"] = diff_plot_df[\"std\"].fillna(0.0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.errorbar(\n",
    "    cap_plot_df[\"date_obj\"],\n",
    "    cap_plot_df[\"mean\"],\n",
    "    yerr=cap_plot_df[\"std\"],\n",
    "    fmt=\"o\",\n",
    "    color=colors[0],\n",
    "    ecolor=colors[0],\n",
    "    markeredgecolor='white',\n",
    "\n",
    "    elinewidth=1,\n",
    "    capsize=3,\n",
    "    alpha=0.9,\n",
    "    label=\"Model capability (mean ± std)\",\n",
    ")\n",
    "\n",
    "ax.errorbar(\n",
    "    diff_plot_df[\"benchmark_release_date\"],\n",
    "    diff_plot_df[\"mean\"],\n",
    "    yerr=diff_plot_df[\"std\"],\n",
    "    fmt=\"o\",\n",
    "    color=colors[1],\n",
    "    ecolor=colors[1],\n",
    "    markeredgecolor='white',\n",
    "    elinewidth=1,\n",
    "    capsize=3,\n",
    "    alpha=0.9,\n",
    "    label=\"Benchmark difficulty (mean ± std)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Release date\", fontsize=14)\n",
    "ax.set_ylabel(\"Estimated capability / difficulty\", fontsize=14)\n",
    "ax.set_title(\n",
    "    \"AI model capabilities & benchmark difficulties (with std error bars)\", fontsize=18\n",
    ")\n",
    "\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "# fig.autofmt_xdate()\n",
    "\n",
    "ax.grid(True, alpha=0.25, linestyle=\"--\")\n",
    "ax.legend(frameon=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"outputs/change_anchor/capabilities_and_benchmarks_over_time.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedf969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Variation in benchmark difficulties across model anchor pairs ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.576904</td>\n",
       "      <td>0.171304</td>\n",
       "      <td>0.148446</td>\n",
       "      <td>0.853594</td>\n",
       "      <td>149</td>\n",
       "      <td>0.295938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>-0.031989</td>\n",
       "      <td>0.221886</td>\n",
       "      <td>-0.572622</td>\n",
       "      <td>0.263367</td>\n",
       "      <td>149</td>\n",
       "      <td>-6.913003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>1.948494</td>\n",
       "      <td>0.273432</td>\n",
       "      <td>1.213769</td>\n",
       "      <td>2.212920</td>\n",
       "      <td>149</td>\n",
       "      <td>0.139858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>1.614525</td>\n",
       "      <td>0.237116</td>\n",
       "      <td>0.937043</td>\n",
       "      <td>1.854031</td>\n",
       "      <td>149</td>\n",
       "      <td>0.146371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>0.324886</td>\n",
       "      <td>0.180106</td>\n",
       "      <td>-0.148678</td>\n",
       "      <td>0.584454</td>\n",
       "      <td>149</td>\n",
       "      <td>0.552502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>2.368297</td>\n",
       "      <td>0.332024</td>\n",
       "      <td>1.558319</td>\n",
       "      <td>2.714604</td>\n",
       "      <td>149</td>\n",
       "      <td>0.139724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>-1.218257</td>\n",
       "      <td>0.447152</td>\n",
       "      <td>-2.080091</td>\n",
       "      <td>-0.602495</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.365809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA2</th>\n",
       "      <td>-0.329824</td>\n",
       "      <td>0.161517</td>\n",
       "      <td>-0.760793</td>\n",
       "      <td>-0.125501</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.488062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>1.653583</td>\n",
       "      <td>0.240238</td>\n",
       "      <td>0.970420</td>\n",
       "      <td>1.892235</td>\n",
       "      <td>149</td>\n",
       "      <td>0.144795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>2.624834</td>\n",
       "      <td>0.342001</td>\n",
       "      <td>1.813172</td>\n",
       "      <td>3.004368</td>\n",
       "      <td>149</td>\n",
       "      <td>0.129856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>2.026143</td>\n",
       "      <td>0.281046</td>\n",
       "      <td>1.281005</td>\n",
       "      <td>2.298657</td>\n",
       "      <td>149</td>\n",
       "      <td>0.138244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>3.089380</td>\n",
       "      <td>0.374788</td>\n",
       "      <td>2.255571</td>\n",
       "      <td>3.638008</td>\n",
       "      <td>149</td>\n",
       "      <td>0.120907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>2.717558</td>\n",
       "      <td>0.358346</td>\n",
       "      <td>1.879115</td>\n",
       "      <td>3.108806</td>\n",
       "      <td>149</td>\n",
       "      <td>0.131420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>1.187227</td>\n",
       "      <td>0.200232</td>\n",
       "      <td>0.596881</td>\n",
       "      <td>1.460580</td>\n",
       "      <td>149</td>\n",
       "      <td>0.168088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>3.344705</td>\n",
       "      <td>0.279304</td>\n",
       "      <td>2.650416</td>\n",
       "      <td>3.626853</td>\n",
       "      <td>149</td>\n",
       "      <td>0.083226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.200472</td>\n",
       "      <td>0.237256</td>\n",
       "      <td>-0.518416</td>\n",
       "      <td>0.582646</td>\n",
       "      <td>149</td>\n",
       "      <td>1.179508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>-0.822732</td>\n",
       "      <td>0.365867</td>\n",
       "      <td>-1.566462</td>\n",
       "      <td>-0.321051</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.443203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>1.148314</td>\n",
       "      <td>0.199474</td>\n",
       "      <td>0.564125</td>\n",
       "      <td>1.427669</td>\n",
       "      <td>149</td>\n",
       "      <td>0.173126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>0.133153</td>\n",
       "      <td>0.199575</td>\n",
       "      <td>-0.377287</td>\n",
       "      <td>0.385396</td>\n",
       "      <td>149</td>\n",
       "      <td>1.493795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>3.954557</td>\n",
       "      <td>0.356858</td>\n",
       "      <td>3.197051</td>\n",
       "      <td>4.504084</td>\n",
       "      <td>149</td>\n",
       "      <td>0.089936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>2.169621</td>\n",
       "      <td>0.263194</td>\n",
       "      <td>1.457812</td>\n",
       "      <td>2.463596</td>\n",
       "      <td>149</td>\n",
       "      <td>0.120901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>1.628528</td>\n",
       "      <td>0.239105</td>\n",
       "      <td>0.947641</td>\n",
       "      <td>1.867289</td>\n",
       "      <td>149</td>\n",
       "      <td>0.146329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.215308</td>\n",
       "      <td>-0.518672</td>\n",
       "      <td>0.298280</td>\n",
       "      <td>149</td>\n",
       "      <td>15.795612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>2.858395</td>\n",
       "      <td>0.273963</td>\n",
       "      <td>2.190634</td>\n",
       "      <td>3.240134</td>\n",
       "      <td>149</td>\n",
       "      <td>0.095523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>2.030469</td>\n",
       "      <td>0.285972</td>\n",
       "      <td>1.278678</td>\n",
       "      <td>2.311704</td>\n",
       "      <td>149</td>\n",
       "      <td>0.140367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>3.076576</td>\n",
       "      <td>0.373991</td>\n",
       "      <td>2.244720</td>\n",
       "      <td>3.608909</td>\n",
       "      <td>149</td>\n",
       "      <td>0.121152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>-0.739751</td>\n",
       "      <td>0.349003</td>\n",
       "      <td>-1.506936</td>\n",
       "      <td>-0.263312</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.470199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>2.319608</td>\n",
       "      <td>0.326949</td>\n",
       "      <td>1.516098</td>\n",
       "      <td>2.661901</td>\n",
       "      <td>149</td>\n",
       "      <td>0.140476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>-0.885066</td>\n",
       "      <td>0.161061</td>\n",
       "      <td>-1.302365</td>\n",
       "      <td>-0.573860</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.181365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>2.078987</td>\n",
       "      <td>0.288501</td>\n",
       "      <td>1.327180</td>\n",
       "      <td>2.362700</td>\n",
       "      <td>149</td>\n",
       "      <td>0.138304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>-1.455591</td>\n",
       "      <td>0.498654</td>\n",
       "      <td>-2.409633</td>\n",
       "      <td>-0.761618</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.341427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     mean       std       min       max  \\\n",
       "benchmark_name                                                            \n",
       "ANLI                             0.576904  0.171304  0.148446  0.853594   \n",
       "ARC AI2                         -0.031989  0.221886 -0.572622  0.263367   \n",
       "ARC-AGI                          1.948494  0.273432  1.213769  2.212920   \n",
       "Aider polyglot                   1.614525  0.237116  0.937043  1.854031   \n",
       "BBH                              0.324886  0.180106 -0.148678  0.584454   \n",
       "Balrog                           2.368297  0.332024  1.558319  2.714604   \n",
       "BoolQ                           -1.218257  0.447152 -2.080091 -0.602495   \n",
       "CSQA2                           -0.329824  0.161517 -0.760793 -0.125501   \n",
       "CadEval                          1.653583  0.240238  0.970420  1.892235   \n",
       "Cybench                          2.624834  0.342001  1.813172  3.004368   \n",
       "DeepResearch Bench               2.026143  0.281046  1.281005  2.298657   \n",
       "Factorio learning environment    3.089380  0.374788  2.255571  3.638008   \n",
       "FrontierMath-2025-02-28-Private  2.717558  0.358346  1.879115  3.108806   \n",
       "GPQA diamond                     1.187227  0.200232  0.596881  1.460580   \n",
       "GSO-Bench                        3.344705  0.279304  2.650416  3.626853   \n",
       "GeoBench                         0.200472  0.237256 -0.518416  0.582646   \n",
       "HellaSwag                       -0.822732  0.365867 -1.566462 -0.321051   \n",
       "MATH level 5                     1.148314  0.199474  0.564125  1.427669   \n",
       "MMLU                             0.133153  0.199575 -0.377287  0.385396   \n",
       "OSUniverse                       3.954557  0.356858  3.197051  4.504084   \n",
       "OSWorld                          2.169621  0.263194  1.457812  2.463596   \n",
       "OTIS Mock AIME 2024-2025         1.628528  0.239105  0.947641  1.867289   \n",
       "OpenBookQA                       0.013585  0.215308 -0.518672  0.298280   \n",
       "SWE-Bench verified               2.858395  0.273963  2.190634  3.240134   \n",
       "SimpleBench                      2.030469  0.285972  1.278678  2.311704   \n",
       "Terminal Bench                   3.076576  0.373991  2.244720  3.608909   \n",
       "TriviaQA                        -0.739751  0.349003 -1.506936 -0.263312   \n",
       "VPCT                             2.319608  0.326949  1.516098  2.661901   \n",
       "VideoMME                        -0.885066  0.161061 -1.302365 -0.573860   \n",
       "WeirdML                          2.078987  0.288501  1.327180  2.362700   \n",
       "Winogrande                      -1.455591  0.498654 -2.409633 -0.761618   \n",
       "\n",
       "                                 count         cv  \n",
       "benchmark_name                                     \n",
       "ANLI                               149   0.295938  \n",
       "ARC AI2                            149  -6.913003  \n",
       "ARC-AGI                            149   0.139858  \n",
       "Aider polyglot                     149   0.146371  \n",
       "BBH                                149   0.552502  \n",
       "Balrog                             149   0.139724  \n",
       "BoolQ                              149  -0.365809  \n",
       "CSQA2                              149  -0.488062  \n",
       "CadEval                            149   0.144795  \n",
       "Cybench                            149   0.129856  \n",
       "DeepResearch Bench                 149   0.138244  \n",
       "Factorio learning environment      149   0.120907  \n",
       "FrontierMath-2025-02-28-Private    149   0.131420  \n",
       "GPQA diamond                       149   0.168088  \n",
       "GSO-Bench                          149   0.083226  \n",
       "GeoBench                           149   1.179508  \n",
       "HellaSwag                          149  -0.443203  \n",
       "MATH level 5                       149   0.173126  \n",
       "MMLU                               149   1.493795  \n",
       "OSUniverse                         149   0.089936  \n",
       "OSWorld                            149   0.120901  \n",
       "OTIS Mock AIME 2024-2025           149   0.146329  \n",
       "OpenBookQA                         149  15.795612  \n",
       "SWE-Bench verified                 149   0.095523  \n",
       "SimpleBench                        149   0.140367  \n",
       "Terminal Bench                     149   0.121152  \n",
       "TriviaQA                           149  -0.470199  \n",
       "VPCT                               149   0.140476  \n",
       "VideoMME                           149  -0.181365  \n",
       "WeirdML                            149   0.138304  \n",
       "Winogrande                         149  -0.341427  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Variation in benchmark slopes across model anchor pairs ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.949958</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.676248</td>\n",
       "      <td>1.395438</td>\n",
       "      <td>149</td>\n",
       "      <td>0.259561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC AI2</th>\n",
       "      <td>1.511995</td>\n",
       "      <td>0.411552</td>\n",
       "      <td>1.070354</td>\n",
       "      <td>2.288111</td>\n",
       "      <td>149</td>\n",
       "      <td>0.271277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-AGI</th>\n",
       "      <td>4.478621</td>\n",
       "      <td>0.708559</td>\n",
       "      <td>2.984404</td>\n",
       "      <td>5.299933</td>\n",
       "      <td>149</td>\n",
       "      <td>0.157677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aider polyglot</th>\n",
       "      <td>3.244570</td>\n",
       "      <td>0.554589</td>\n",
       "      <td>2.250287</td>\n",
       "      <td>3.901585</td>\n",
       "      <td>149</td>\n",
       "      <td>0.170354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBH</th>\n",
       "      <td>1.433160</td>\n",
       "      <td>0.386742</td>\n",
       "      <td>0.993533</td>\n",
       "      <td>2.153350</td>\n",
       "      <td>149</td>\n",
       "      <td>0.268945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balrog</th>\n",
       "      <td>1.090131</td>\n",
       "      <td>0.187621</td>\n",
       "      <td>0.755459</td>\n",
       "      <td>1.317417</td>\n",
       "      <td>149</td>\n",
       "      <td>0.171531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>1.008272</td>\n",
       "      <td>0.274833</td>\n",
       "      <td>0.720922</td>\n",
       "      <td>1.546536</td>\n",
       "      <td>149</td>\n",
       "      <td>0.271662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA2</th>\n",
       "      <td>0.326550</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>0.255513</td>\n",
       "      <td>0.427432</td>\n",
       "      <td>149</td>\n",
       "      <td>0.148275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CadEval</th>\n",
       "      <td>2.272964</td>\n",
       "      <td>0.388145</td>\n",
       "      <td>1.568320</td>\n",
       "      <td>2.734986</td>\n",
       "      <td>149</td>\n",
       "      <td>0.170192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cybench</th>\n",
       "      <td>1.514883</td>\n",
       "      <td>0.220294</td>\n",
       "      <td>1.136790</td>\n",
       "      <td>1.768655</td>\n",
       "      <td>149</td>\n",
       "      <td>0.144931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepResearch Bench</th>\n",
       "      <td>1.786276</td>\n",
       "      <td>0.208198</td>\n",
       "      <td>1.077785</td>\n",
       "      <td>2.014284</td>\n",
       "      <td>149</td>\n",
       "      <td>0.116163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factorio learning environment</th>\n",
       "      <td>0.983832</td>\n",
       "      <td>0.129129</td>\n",
       "      <td>0.692443</td>\n",
       "      <td>1.126748</td>\n",
       "      <td>149</td>\n",
       "      <td>0.130809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontierMath-2025-02-28-Private</th>\n",
       "      <td>2.846115</td>\n",
       "      <td>0.392604</td>\n",
       "      <td>1.921386</td>\n",
       "      <td>3.292237</td>\n",
       "      <td>149</td>\n",
       "      <td>0.137480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPQA diamond</th>\n",
       "      <td>1.976358</td>\n",
       "      <td>0.369805</td>\n",
       "      <td>1.391562</td>\n",
       "      <td>2.448865</td>\n",
       "      <td>149</td>\n",
       "      <td>0.186486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSO-Bench</th>\n",
       "      <td>1.966086</td>\n",
       "      <td>0.058438</td>\n",
       "      <td>1.798012</td>\n",
       "      <td>2.044672</td>\n",
       "      <td>149</td>\n",
       "      <td>0.029623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeoBench</th>\n",
       "      <td>0.421385</td>\n",
       "      <td>0.073349</td>\n",
       "      <td>0.284468</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>149</td>\n",
       "      <td>0.173480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>1.142522</td>\n",
       "      <td>0.310237</td>\n",
       "      <td>0.811887</td>\n",
       "      <td>1.730546</td>\n",
       "      <td>149</td>\n",
       "      <td>0.270624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATH level 5</th>\n",
       "      <td>4.054979</td>\n",
       "      <td>0.764789</td>\n",
       "      <td>2.865316</td>\n",
       "      <td>5.034238</td>\n",
       "      <td>149</td>\n",
       "      <td>0.187971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMLU</th>\n",
       "      <td>1.287200</td>\n",
       "      <td>0.349209</td>\n",
       "      <td>0.933610</td>\n",
       "      <td>1.922992</td>\n",
       "      <td>149</td>\n",
       "      <td>0.270381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSUniverse</th>\n",
       "      <td>0.804524</td>\n",
       "      <td>0.065488</td>\n",
       "      <td>0.636564</td>\n",
       "      <td>0.874449</td>\n",
       "      <td>149</td>\n",
       "      <td>0.081126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSWorld</th>\n",
       "      <td>2.489533</td>\n",
       "      <td>0.291283</td>\n",
       "      <td>1.967193</td>\n",
       "      <td>2.802127</td>\n",
       "      <td>149</td>\n",
       "      <td>0.116610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTIS Mock AIME 2024-2025</th>\n",
       "      <td>5.269133</td>\n",
       "      <td>0.929806</td>\n",
       "      <td>3.686315</td>\n",
       "      <td>6.392673</td>\n",
       "      <td>149</td>\n",
       "      <td>0.175870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenBookQA</th>\n",
       "      <td>1.308167</td>\n",
       "      <td>0.366135</td>\n",
       "      <td>0.931499</td>\n",
       "      <td>2.002583</td>\n",
       "      <td>149</td>\n",
       "      <td>0.278943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWE-Bench verified</th>\n",
       "      <td>0.300955</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.327093</td>\n",
       "      <td>149</td>\n",
       "      <td>0.077658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleBench</th>\n",
       "      <td>1.520359</td>\n",
       "      <td>0.263484</td>\n",
       "      <td>1.054793</td>\n",
       "      <td>1.835006</td>\n",
       "      <td>149</td>\n",
       "      <td>0.172721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terminal Bench</th>\n",
       "      <td>0.955044</td>\n",
       "      <td>0.116421</td>\n",
       "      <td>0.691470</td>\n",
       "      <td>1.078302</td>\n",
       "      <td>149</td>\n",
       "      <td>0.121491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TriviaQA</th>\n",
       "      <td>0.753252</td>\n",
       "      <td>0.200825</td>\n",
       "      <td>0.516775</td>\n",
       "      <td>1.131481</td>\n",
       "      <td>149</td>\n",
       "      <td>0.265715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VPCT</th>\n",
       "      <td>0.645896</td>\n",
       "      <td>0.110994</td>\n",
       "      <td>0.449487</td>\n",
       "      <td>0.780313</td>\n",
       "      <td>149</td>\n",
       "      <td>0.171268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoMME</th>\n",
       "      <td>0.406961</td>\n",
       "      <td>0.040932</td>\n",
       "      <td>0.354980</td>\n",
       "      <td>0.501210</td>\n",
       "      <td>149</td>\n",
       "      <td>0.100241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeirdML</th>\n",
       "      <td>1.056381</td>\n",
       "      <td>0.173236</td>\n",
       "      <td>0.703163</td>\n",
       "      <td>1.237619</td>\n",
       "      <td>149</td>\n",
       "      <td>0.163439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.600502</td>\n",
       "      <td>0.163828</td>\n",
       "      <td>0.424570</td>\n",
       "      <td>0.909301</td>\n",
       "      <td>149</td>\n",
       "      <td>0.271901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     mean       std       min       max  \\\n",
       "benchmark_name                                                            \n",
       "ANLI                             0.949958  0.247404  0.676248  1.395438   \n",
       "ARC AI2                          1.511995  0.411552  1.070354  2.288111   \n",
       "ARC-AGI                          4.478621  0.708559  2.984404  5.299933   \n",
       "Aider polyglot                   3.244570  0.554589  2.250287  3.901585   \n",
       "BBH                              1.433160  0.386742  0.993533  2.153350   \n",
       "Balrog                           1.090131  0.187621  0.755459  1.317417   \n",
       "BoolQ                            1.008272  0.274833  0.720922  1.546536   \n",
       "CSQA2                            0.326550  0.048583  0.255513  0.427432   \n",
       "CadEval                          2.272964  0.388145  1.568320  2.734986   \n",
       "Cybench                          1.514883  0.220294  1.136790  1.768655   \n",
       "DeepResearch Bench               1.786276  0.208198  1.077785  2.014284   \n",
       "Factorio learning environment    0.983832  0.129129  0.692443  1.126748   \n",
       "FrontierMath-2025-02-28-Private  2.846115  0.392604  1.921386  3.292237   \n",
       "GPQA diamond                     1.976358  0.369805  1.391562  2.448865   \n",
       "GSO-Bench                        1.966086  0.058438  1.798012  2.044672   \n",
       "GeoBench                         0.421385  0.073349  0.284468  0.512605   \n",
       "HellaSwag                        1.142522  0.310237  0.811887  1.730546   \n",
       "MATH level 5                     4.054979  0.764789  2.865316  5.034238   \n",
       "MMLU                             1.287200  0.349209  0.933610  1.922992   \n",
       "OSUniverse                       0.804524  0.065488  0.636564  0.874449   \n",
       "OSWorld                          2.489533  0.291283  1.967193  2.802127   \n",
       "OTIS Mock AIME 2024-2025         5.269133  0.929806  3.686315  6.392673   \n",
       "OpenBookQA                       1.308167  0.366135  0.931499  2.002583   \n",
       "SWE-Bench verified               0.300955  0.023450  0.233501  0.327093   \n",
       "SimpleBench                      1.520359  0.263484  1.054793  1.835006   \n",
       "Terminal Bench                   0.955044  0.116421  0.691470  1.078302   \n",
       "TriviaQA                         0.753252  0.200825  0.516775  1.131481   \n",
       "VPCT                             0.645896  0.110994  0.449487  0.780313   \n",
       "VideoMME                         0.406961  0.040932  0.354980  0.501210   \n",
       "WeirdML                          1.056381  0.173236  0.703163  1.237619   \n",
       "Winogrande                       0.600502  0.163828  0.424570  0.909301   \n",
       "\n",
       "                                 count        cv  \n",
       "benchmark_name                                    \n",
       "ANLI                               149  0.259561  \n",
       "ARC AI2                            149  0.271277  \n",
       "ARC-AGI                            149  0.157677  \n",
       "Aider polyglot                     149  0.170354  \n",
       "BBH                                149  0.268945  \n",
       "Balrog                             149  0.171531  \n",
       "BoolQ                              149  0.271662  \n",
       "CSQA2                              149  0.148275  \n",
       "CadEval                            149  0.170192  \n",
       "Cybench                            149  0.144931  \n",
       "DeepResearch Bench                 149  0.116163  \n",
       "Factorio learning environment      149  0.130809  \n",
       "FrontierMath-2025-02-28-Private    149  0.137480  \n",
       "GPQA diamond                       149  0.186486  \n",
       "GSO-Bench                          149  0.029623  \n",
       "GeoBench                           149  0.173480  \n",
       "HellaSwag                          149  0.270624  \n",
       "MATH level 5                       149  0.187971  \n",
       "MMLU                               149  0.270381  \n",
       "OSUniverse                         149  0.081126  \n",
       "OSWorld                            149  0.116610  \n",
       "OTIS Mock AIME 2024-2025           149  0.175870  \n",
       "OpenBookQA                         149  0.278943  \n",
       "SWE-Bench verified                 149  0.077658  \n",
       "SimpleBench                        149  0.172721  \n",
       "Terminal Bench                     149  0.121491  \n",
       "TriviaQA                           149  0.265715  \n",
       "VPCT                               149  0.171268  \n",
       "VideoMME                           149  0.100241  \n",
       "WeirdML                            149  0.163439  \n",
       "Winogrande                         149  0.271901  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Variation in model capabilities (all models) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-13B-Base</th>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.215473</td>\n",
       "      <td>-0.510097</td>\n",
       "      <td>0.304139</td>\n",
       "      <td>149</td>\n",
       "      <td>9.450721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-7B-Base</th>\n",
       "      <td>-0.163597</td>\n",
       "      <td>0.243944</td>\n",
       "      <td>-0.731591</td>\n",
       "      <td>0.166295</td>\n",
       "      <td>149</td>\n",
       "      <td>-1.486114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebras-GPT-13B</th>\n",
       "      <td>-0.587071</td>\n",
       "      <td>0.319552</td>\n",
       "      <td>-1.252025</td>\n",
       "      <td>-0.146496</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.542487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <td>0.474530</td>\n",
       "      <td>0.172668</td>\n",
       "      <td>0.028658</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>149</td>\n",
       "      <td>0.362649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-R1</th>\n",
       "      <td>1.647710</td>\n",
       "      <td>0.240956</td>\n",
       "      <td>0.963706</td>\n",
       "      <td>1.887455</td>\n",
       "      <td>149</td>\n",
       "      <td>0.145745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <td>-0.880515</td>\n",
       "      <td>0.377027</td>\n",
       "      <td>-1.638438</td>\n",
       "      <td>-0.363670</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.426750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-001</th>\n",
       "      <td>0.011444</td>\n",
       "      <td>0.215855</td>\n",
       "      <td>-0.521015</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>149</td>\n",
       "      <td>18.798225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002</th>\n",
       "      <td>0.743699</td>\n",
       "      <td>0.174332</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>1.019313</td>\n",
       "      <td>149</td>\n",
       "      <td>0.233624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1</th>\n",
       "      <td>-0.197130</td>\n",
       "      <td>0.248311</td>\n",
       "      <td>-0.768657</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>149</td>\n",
       "      <td>-1.255393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base</th>\n",
       "      <td>-0.252937</td>\n",
       "      <td>0.257418</td>\n",
       "      <td>-0.833983</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>149</td>\n",
       "      <td>-1.014296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean       std       min       max  count  \\\n",
       "model                                                                    \n",
       "Baichuan-2-13B-Base      0.022723  0.215473 -0.510097  0.304139    149   \n",
       "Baichuan-2-7B-Base      -0.163597  0.243944 -0.731591  0.166295    149   \n",
       "Cerebras-GPT-13B        -0.587071  0.319552 -1.252025 -0.146496    149   \n",
       "Chinchilla (70B)         0.474530  0.172668  0.028658  0.738817    149   \n",
       "DeepSeek-R1              1.647710  0.240956  0.963706  1.887455    149   \n",
       "...                           ...       ...       ...       ...    ...   \n",
       "stablelm-tuned-alpha-7b -0.880515  0.377027 -1.638438 -0.363670    149   \n",
       "text-davinci-001         0.011444  0.215855 -0.521015  0.295527    149   \n",
       "text-davinci-002         0.743699  0.174332  0.279800  1.019313    149   \n",
       "vicuna-13b-v1.1         -0.197130  0.248311 -0.768657  0.140212    149   \n",
       "xgen-7b-8k-base         -0.252937  0.257418 -0.833983  0.099999    149   \n",
       "\n",
       "                                cv  \n",
       "model                               \n",
       "Baichuan-2-13B-Base       9.450721  \n",
       "Baichuan-2-7B-Base       -1.486114  \n",
       "Cerebras-GPT-13B         -0.542487  \n",
       "Chinchilla (70B)          0.362649  \n",
       "DeepSeek-R1               0.145745  \n",
       "...                            ...  \n",
       "stablelm-tuned-alpha-7b  -0.426750  \n",
       "text-davinci-001         18.798225  \n",
       "text-davinci-002          0.233624  \n",
       "vicuna-13b-v1.1          -1.255393  \n",
       "xgen-7b-8k-base          -1.014296  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Variation in model capabilities (excluding anchored models) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-13B-Base</th>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.215056</td>\n",
       "      <td>-0.510097</td>\n",
       "      <td>0.304139</td>\n",
       "      <td>147</td>\n",
       "      <td>11.033022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-7B-Base</th>\n",
       "      <td>-0.166798</td>\n",
       "      <td>0.244040</td>\n",
       "      <td>-0.731591</td>\n",
       "      <td>0.166295</td>\n",
       "      <td>147</td>\n",
       "      <td>-1.458106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebras-GPT-13B</th>\n",
       "      <td>-0.590008</td>\n",
       "      <td>0.320727</td>\n",
       "      <td>-1.252025</td>\n",
       "      <td>-0.146496</td>\n",
       "      <td>147</td>\n",
       "      <td>-0.541745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <td>0.471035</td>\n",
       "      <td>0.171192</td>\n",
       "      <td>0.028658</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>147</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-R1</th>\n",
       "      <td>1.645051</td>\n",
       "      <td>0.241504</td>\n",
       "      <td>0.963706</td>\n",
       "      <td>1.887455</td>\n",
       "      <td>147</td>\n",
       "      <td>0.146307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <td>-0.883271</td>\n",
       "      <td>0.378849</td>\n",
       "      <td>-1.638438</td>\n",
       "      <td>-0.363670</td>\n",
       "      <td>147</td>\n",
       "      <td>-0.427455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-001</th>\n",
       "      <td>0.008153</td>\n",
       "      <td>0.215451</td>\n",
       "      <td>-0.521015</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>147</td>\n",
       "      <td>26.337233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002</th>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.172714</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>1.019313</td>\n",
       "      <td>147</td>\n",
       "      <td>0.232574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1</th>\n",
       "      <td>-0.200307</td>\n",
       "      <td>0.248487</td>\n",
       "      <td>-0.768657</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>147</td>\n",
       "      <td>-1.236304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base</th>\n",
       "      <td>-0.256070</td>\n",
       "      <td>0.257751</td>\n",
       "      <td>-0.833983</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>147</td>\n",
       "      <td>-1.003134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean       std       min       max  count  \\\n",
       "model                                                                    \n",
       "Baichuan-2-13B-Base      0.019426  0.215056 -0.510097  0.304139    147   \n",
       "Baichuan-2-7B-Base      -0.166798  0.244040 -0.731591  0.166295    147   \n",
       "Cerebras-GPT-13B        -0.590008  0.320727 -1.252025 -0.146496    147   \n",
       "Chinchilla (70B)         0.471035  0.171192  0.028658  0.738817    147   \n",
       "DeepSeek-R1              1.645051  0.241504  0.963706  1.887455    147   \n",
       "...                           ...       ...       ...       ...    ...   \n",
       "stablelm-tuned-alpha-7b -0.883271  0.378849 -1.638438 -0.363670    147   \n",
       "text-davinci-001         0.008153  0.215451 -0.521015  0.295527    147   \n",
       "text-davinci-002         0.740088  0.172714  0.279800  1.019313    147   \n",
       "vicuna-13b-v1.1         -0.200307  0.248487 -0.768657  0.140212    147   \n",
       "xgen-7b-8k-base         -0.256070  0.257751 -0.833983  0.099999    147   \n",
       "\n",
       "                                cv  \n",
       "model                               \n",
       "Baichuan-2-13B-Base      11.033022  \n",
       "Baichuan-2-7B-Base       -1.458106  \n",
       "Cerebras-GPT-13B         -0.541745  \n",
       "Chinchilla (70B)          0.362200  \n",
       "DeepSeek-R1               0.146307  \n",
       "...                            ...  \n",
       "stablelm-tuned-alpha-7b  -0.427455  \n",
       "text-davinci-001         26.337233  \n",
       "text-davinci-002          0.232574  \n",
       "vicuna-13b-v1.1          -1.236304  \n",
       "xgen-7b-8k-base          -1.003134  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model pair stability (lower deviation = more stable) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_difficulty_deviation</th>\n",
       "      <th>mean_capability_deviation</th>\n",
       "      <th>combined_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B-v0.1_text-davinci-002</th>\n",
       "      <td>0.060466</td>\n",
       "      <td>0.028353</td>\n",
       "      <td>0.033914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-Coder-14B_falcon-180B</th>\n",
       "      <td>0.070134</td>\n",
       "      <td>0.035910</td>\n",
       "      <td>0.041837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613_internlm-20b</th>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>0.054030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-180B_gpt-3.5-turbo-0613</th>\n",
       "      <td>0.082704</td>\n",
       "      <td>0.053932</td>\n",
       "      <td>0.058914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internlm-20b_Qwen2.5-Coder-32B</th>\n",
       "      <td>0.097957</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.063789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <td>0.109445</td>\n",
       "      <td>0.072264</td>\n",
       "      <td>0.078703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-Coder-32B_Qwen-14B</th>\n",
       "      <td>0.119925</td>\n",
       "      <td>0.083617</td>\n",
       "      <td>0.089905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-4k-instruct_PaLM 2-L</th>\n",
       "      <td>0.125860</td>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.099695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla (70B)_Qwen2.5-Coder-7B</th>\n",
       "      <td>0.151842</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.100849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-Coder-7B_PaLM 540B</th>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.117983</td>\n",
       "      <td>0.128742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    mean_difficulty_deviation  \\\n",
       "Mixtral-8x7B-v0.1_text-davinci-002                   0.060466   \n",
       "Qwen2.5-Coder-14B_falcon-180B                        0.070134   \n",
       "gpt-3.5-turbo-0613_internlm-20b                      0.085505   \n",
       "falcon-180B_gpt-3.5-turbo-0613                       0.082704   \n",
       "internlm-20b_Qwen2.5-Coder-32B                       0.097957   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                   0.109445   \n",
       "Qwen2.5-Coder-32B_Qwen-14B                           0.119925   \n",
       "Phi-3-mini-4k-instruct_PaLM 2-L                      0.125860   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                    0.151842   \n",
       "Qwen2.5-Coder-7B_PaLM 540B                           0.180110   \n",
       "\n",
       "                                    mean_capability_deviation  \\\n",
       "Mixtral-8x7B-v0.1_text-davinci-002                   0.028353   \n",
       "Qwen2.5-Coder-14B_falcon-180B                        0.035910   \n",
       "gpt-3.5-turbo-0613_internlm-20b                      0.047437   \n",
       "falcon-180B_gpt-3.5-turbo-0613                       0.053932   \n",
       "internlm-20b_Qwen2.5-Coder-32B                       0.056632   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                   0.072264   \n",
       "Qwen2.5-Coder-32B_Qwen-14B                           0.083617   \n",
       "Phi-3-mini-4k-instruct_PaLM 2-L                      0.094214   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                    0.090167   \n",
       "Qwen2.5-Coder-7B_PaLM 540B                           0.117983   \n",
       "\n",
       "                                    combined_deviation  \n",
       "Mixtral-8x7B-v0.1_text-davinci-002            0.033914  \n",
       "Qwen2.5-Coder-14B_falcon-180B                 0.041837  \n",
       "gpt-3.5-turbo-0613_internlm-20b               0.054030  \n",
       "falcon-180B_gpt-3.5-turbo-0613                0.058914  \n",
       "internlm-20b_Qwen2.5-Coder-32B                0.063789  \n",
       "text-davinci-002_Qwen2.5-Coder-14B            0.078703  \n",
       "Qwen2.5-Coder-32B_Qwen-14B                    0.089905  \n",
       "Phi-3-mini-4k-instruct_PaLM 2-L               0.099695  \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B             0.100849  \n",
       "Qwen2.5-Coder-7B_PaLM 540B                    0.128742  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1)  DIFFICULTY  ––  variation of each benchmark's difficulty estimate\n",
    "#     across different model anchor pairs\n",
    "# ---------------------------------------------------------------------------\n",
    "difficulty_rows = []\n",
    "\n",
    "for anchor_pair, run in all_runs.items():\n",
    "    df_db = run[\"df_db\"]  # difficulty table from that fit\n",
    "    out = df_db[[\"benchmark_name\", \"estimated_difficulty\"]].copy()\n",
    "    out[\"anchor_model_pair\"] = anchor_pair  # remember which model pair this came from\n",
    "    # Also store the individual models for more detailed analysis if needed\n",
    "    out[\"anchor_model1\"] = run[\"anchor_model1\"]\n",
    "    out[\"anchor_model2\"] = run[\"anchor_model2\"]\n",
    "    difficulty_rows.append(out)\n",
    "\n",
    "difficulty_long = pd.concat(difficulty_rows, ignore_index=True)\n",
    "\n",
    "# No need to drop trivial rows since we're not anchoring on benchmarks\n",
    "# All benchmark estimates are free to vary\n",
    "\n",
    "difficulty_stats = (\n",
    "    difficulty_long.groupby(\"benchmark_name\")[\"estimated_difficulty\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        count=\"count\",  # how many model pairs estimated this\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean() if s.mean() != 0 else np.nan,\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  CAPABILITY  ––  variation of each model's capability estimate\n",
    "#     across different model anchor pairs\n",
    "# ---------------------------------------------------------------------------\n",
    "capability_rows = []\n",
    "\n",
    "for anchor_pair, run in all_runs.items():\n",
    "    df_cm = run[\"df_cm1\"]  # capability table from that fit\n",
    "    # Check the actual column name - might be 'model' or 'model_name'\n",
    "    model_col = \"model_name\" if \"model_name\" in df_cm.columns else \"model\"\n",
    "    out = df_cm[[model_col, \"estimated_capability\"]].copy()\n",
    "    out.rename(columns={model_col: \"model\"}, inplace=True)  # standardize column name\n",
    "    out[\"anchor_model_pair\"] = anchor_pair\n",
    "    # Also store the individual anchor models\n",
    "    out[\"anchor_model1\"] = run[\"anchor_model1\"]\n",
    "    out[\"anchor_model2\"] = run[\"anchor_model2\"]\n",
    "    capability_rows.append(out)\n",
    "\n",
    "capability_long = pd.concat(capability_rows, ignore_index=True)\n",
    "\n",
    "# For model capabilities, we might want to exclude rows where the model\n",
    "# was one of the anchors (since those were fixed)\n",
    "capability_long_free = capability_long[\n",
    "    (capability_long[\"model\"] != capability_long[\"anchor_model1\"])\n",
    "    & (capability_long[\"model\"] != capability_long[\"anchor_model2\"])\n",
    "]\n",
    "\n",
    "# Stats for all models (including anchored ones)\n",
    "capability_stats_all = (\n",
    "    capability_long.groupby(\"model\")[\"estimated_capability\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        count=\"count\",\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean() if s.mean() != 0 else np.nan,\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Stats for non-anchored models only (more meaningful variation)\n",
    "capability_stats_free = (\n",
    "    capability_long_free.groupby(\"model\")[\"estimated_capability\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        count=\"count\",\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean() if s.mean() != 0 else np.nan,\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3)  SLOPE  ––  variation of each benchmark's slope estimate\n",
    "# ---------------------------------------------------------------------------\n",
    "slope_rows = []\n",
    "\n",
    "for anchor_pair, run in all_runs.items():\n",
    "    df_db = run[\"df_db\"]\n",
    "    out = df_db[[\"benchmark_name\", \"estimated_slope\"]].copy()\n",
    "    out[\"anchor_model_pair\"] = anchor_pair\n",
    "    slope_rows.append(out)\n",
    "\n",
    "slope_long = pd.concat(slope_rows, ignore_index=True)\n",
    "\n",
    "slope_stats = (\n",
    "    slope_long.groupby(\"benchmark_name\")[\"estimated_slope\"]\n",
    "    .agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        count=\"count\",\n",
    "        cv=lambda s: s.std(ddof=0) / s.mean() if s.mean() != 0 else np.nan,\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4)  Quick look at results\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=== Variation in benchmark difficulties across model anchor pairs ===\")\n",
    "display(difficulty_stats)\n",
    "\n",
    "print(\"\\n=== Variation in benchmark slopes across model anchor pairs ===\")\n",
    "display(slope_stats)\n",
    "\n",
    "print(\"\\n=== Variation in model capabilities (all models) ===\")\n",
    "display(capability_stats_all)\n",
    "\n",
    "print(\"\\n=== Variation in model capabilities (excluding anchored models) ===\")\n",
    "display(capability_stats_free)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5)  Additional analysis: which model pairs give most stable estimates?\n",
    "# ---------------------------------------------------------------------------\n",
    "# Calculate overall stability metric for each model pair\n",
    "stability_by_pair = {}\n",
    "\n",
    "for anchor_pair in all_runs.keys():\n",
    "    # Get difficulty variations for this pair\n",
    "    diff_subset = difficulty_long[difficulty_long[\"anchor_model_pair\"] == anchor_pair]\n",
    "    cap_subset = capability_long_free[\n",
    "        capability_long_free[\"anchor_model_pair\"] == anchor_pair\n",
    "    ]\n",
    "\n",
    "    # Calculate average deviation from overall means\n",
    "    diff_devs = []\n",
    "    for bench in diff_subset[\"benchmark_name\"].unique():\n",
    "        estimate = diff_subset[diff_subset[\"benchmark_name\"] == bench][\n",
    "            \"estimated_difficulty\"\n",
    "        ].iloc[0]\n",
    "        overall_mean = difficulty_stats.loc[bench, \"mean\"]\n",
    "        diff_devs.append(abs(estimate - overall_mean))\n",
    "\n",
    "    cap_devs = []\n",
    "    for model in cap_subset[\"model\"].unique():\n",
    "        if model in capability_stats_free.index:\n",
    "            estimate = cap_subset[cap_subset[\"model\"] == model][\n",
    "                \"estimated_capability\"\n",
    "            ].iloc[0]\n",
    "            overall_mean = capability_stats_free.loc[model, \"mean\"]\n",
    "            cap_devs.append(abs(estimate - overall_mean))\n",
    "\n",
    "    stability_by_pair[anchor_pair] = {\n",
    "        \"mean_difficulty_deviation\": np.mean(diff_devs) if diff_devs else np.nan,\n",
    "        \"mean_capability_deviation\": np.mean(cap_devs) if cap_devs else np.nan,\n",
    "        \"combined_deviation\": (\n",
    "            np.mean(diff_devs + cap_devs) if (diff_devs or cap_devs) else np.nan\n",
    "        ),\n",
    "    }\n",
    "\n",
    "stability_df = pd.DataFrame(stability_by_pair).T.sort_values(\"combined_deviation\")\n",
    "\n",
    "print(\"\\n=== Model pair stability (lower deviation = more stable) ===\")\n",
    "display(stability_df.head(10))  # Show top 10 most stable pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spearman across anchor model pairs (benchmark difficulties) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>anchor</th>\n",
       "      <th>Baichuan-2-13B-Base_text-davinci-001</th>\n",
       "      <th>Baichuan-2-7B-Base_vicuna-13b-v1.1</th>\n",
       "      <th>Cerebras-GPT-13B_stablelm-tuned-alpha-7b</th>\n",
       "      <th>Chinchilla (70B)_Qwen2.5-Coder-7B</th>\n",
       "      <th>DeepSeek-R1-0528_claude-opus-4-20250514_16K</th>\n",
       "      <th>DeepSeek-R1_claude-3-7-sonnet-20250219_16K</th>\n",
       "      <th>DeepSeek-V2_Qwen2.5-72B</th>\n",
       "      <th>DeepSeek-V3-0324_gpt-4.1-2025-04-14</th>\n",
       "      <th>DeepSeek-V3_gemini-1.5-pro-002</th>\n",
       "      <th>GLaM (MoE)_LLaMA-33B</th>\n",
       "      <th>...</th>\n",
       "      <th>phi-2_Llama-2-34b</th>\n",
       "      <th>phi-4_gpt-4.1-nano-2025-04-14</th>\n",
       "      <th>qwen-max-2025-01-25_phi-4</th>\n",
       "      <th>qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Instruct</th>\n",
       "      <th>qwen3-235b-a22b_gemini-2.0-pro-exp-02-05</th>\n",
       "      <th>stablelm-tuned-alpha-7b_opt-1.3b</th>\n",
       "      <th>text-davinci-001_mpt-30b</th>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <th>vicuna-13b-v1.1_gemma-2b</th>\n",
       "      <th>xgen-7b-8k-base_Llama-2-7b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-13B-Base_text-davinci-001</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.994758</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.994758</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-7B-Base_vicuna-13b-v1.1</th>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebras-GPT-13B_stablelm-tuned-alpha-7b</th>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla (70B)_Qwen2.5-Coder-7B</th>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.997177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-R1-0528_claude-opus-4-20250514_16K</th>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.998790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b_opt-1.3b</th>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-001_mpt-30b</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.994758</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.994758</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.995968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1_gemma-2b</th>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base_Llama-2-7b</th>\n",
       "      <td>0.999597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.995968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "anchor                                       Baichuan-2-13B-Base_text-davinci-001  \\\n",
       "anchor                                                                              \n",
       "Baichuan-2-13B-Base_text-davinci-001                                     1.000000   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                       0.999597   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                 0.999194   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                        0.997984   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                              0.995161   \n",
       "...                                                                           ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                         0.999194   \n",
       "text-davinci-001_mpt-30b                                                 1.000000   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                       0.996774   \n",
       "vicuna-13b-v1.1_gemma-2b                                                 0.999597   \n",
       "xgen-7b-8k-base_Llama-2-7b                                               0.999597   \n",
       "\n",
       "anchor                                       Baichuan-2-7B-Base_vicuna-13b-v1.1  \\\n",
       "anchor                                                                            \n",
       "Baichuan-2-13B-Base_text-davinci-001                                   0.999597   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                     1.000000   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                               0.999597   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                      0.997177   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                            0.994355   \n",
       "...                                                                         ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                       0.999597   \n",
       "text-davinci-001_mpt-30b                                               0.999597   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                     0.995968   \n",
       "vicuna-13b-v1.1_gemma-2b                                               1.000000   \n",
       "xgen-7b-8k-base_Llama-2-7b                                             1.000000   \n",
       "\n",
       "anchor                                       Cerebras-GPT-13B_stablelm-tuned-alpha-7b  \\\n",
       "anchor                                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                                         0.999194   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                           0.999597   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                     1.000000   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                            0.997984   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                  0.995968   \n",
       "...                                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                             1.000000   \n",
       "text-davinci-001_mpt-30b                                                     0.999194   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                           0.997177   \n",
       "vicuna-13b-v1.1_gemma-2b                                                     0.999597   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                   0.999597   \n",
       "\n",
       "anchor                                       Chinchilla (70B)_Qwen2.5-Coder-7B  \\\n",
       "anchor                                                                           \n",
       "Baichuan-2-13B-Base_text-davinci-001                                  0.997984   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                    0.997177   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                              0.997984   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                     1.000000   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                           0.998790   \n",
       "...                                                                        ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                      0.997984   \n",
       "text-davinci-001_mpt-30b                                              0.997984   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                    0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                              0.997177   \n",
       "xgen-7b-8k-base_Llama-2-7b                                            0.997177   \n",
       "\n",
       "anchor                                       DeepSeek-R1-0528_claude-opus-4-20250514_16K  \\\n",
       "anchor                                                                                     \n",
       "Baichuan-2-13B-Base_text-davinci-001                                            0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                              0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                        0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                               0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                     1.000000   \n",
       "...                                                                                  ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                                0.995968   \n",
       "text-davinci-001_mpt-30b                                                        0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                              0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                                        0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                      0.994355   \n",
       "\n",
       "anchor                                       DeepSeek-R1_claude-3-7-sonnet-20250219_16K  \\\n",
       "anchor                                                                                    \n",
       "Baichuan-2-13B-Base_text-davinci-001                                           0.994758   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                             0.993952   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                       0.995565   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                              0.998387   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                    0.999597   \n",
       "...                                                                                 ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                               0.995565   \n",
       "text-davinci-001_mpt-30b                                                       0.994758   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                             0.999194   \n",
       "vicuna-13b-v1.1_gemma-2b                                                       0.993952   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                     0.993952   \n",
       "\n",
       "anchor                                       DeepSeek-V2_Qwen2.5-72B  \\\n",
       "anchor                                                                 \n",
       "Baichuan-2-13B-Base_text-davinci-001                        0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                          0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                    0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                           0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                 1.000000   \n",
       "...                                                              ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                            0.995968   \n",
       "text-davinci-001_mpt-30b                                    0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                          0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                    0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                  0.994355   \n",
       "\n",
       "anchor                                       DeepSeek-V3-0324_gpt-4.1-2025-04-14  \\\n",
       "anchor                                                                             \n",
       "Baichuan-2-13B-Base_text-davinci-001                                    0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                      0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                       0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                             1.000000   \n",
       "...                                                                          ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                        0.995968   \n",
       "text-davinci-001_mpt-30b                                                0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                      0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                                0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                              0.994355   \n",
       "\n",
       "anchor                                       DeepSeek-V3_gemini-1.5-pro-002  \\\n",
       "anchor                                                                        \n",
       "Baichuan-2-13B-Base_text-davinci-001                               0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                 0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                           0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                  0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                        1.000000   \n",
       "...                                                                     ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                   0.995968   \n",
       "text-davinci-001_mpt-30b                                           0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                 0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                           0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                         0.994355   \n",
       "\n",
       "anchor                                       GLaM (MoE)_LLaMA-33B  ...  \\\n",
       "anchor                                                             ...   \n",
       "Baichuan-2-13B-Base_text-davinci-001                     0.999597  ...   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                       0.999194  ...   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                 0.999597  ...   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                        0.998790  ...   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K              0.996774  ...   \n",
       "...                                                           ...  ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                         0.999597  ...   \n",
       "text-davinci-001_mpt-30b                                 0.999597  ...   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                       0.997984  ...   \n",
       "vicuna-13b-v1.1_gemma-2b                                 0.999194  ...   \n",
       "xgen-7b-8k-base_Llama-2-7b                               0.999194  ...   \n",
       "\n",
       "anchor                                       phi-2_Llama-2-34b  \\\n",
       "anchor                                                           \n",
       "Baichuan-2-13B-Base_text-davinci-001                  0.999597   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                    0.999194   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b              0.999597   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                     0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K           0.996774   \n",
       "...                                                        ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                      0.999597   \n",
       "text-davinci-001_mpt-30b                              0.999597   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                    0.997984   \n",
       "vicuna-13b-v1.1_gemma-2b                              0.999194   \n",
       "xgen-7b-8k-base_Llama-2-7b                            0.999194   \n",
       "\n",
       "anchor                                       phi-4_gpt-4.1-nano-2025-04-14  \\\n",
       "anchor                                                                       \n",
       "Baichuan-2-13B-Base_text-davinci-001                              0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                          0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                 0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                       1.000000   \n",
       "...                                                                    ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                  0.995968   \n",
       "text-davinci-001_mpt-30b                                          0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                          0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                        0.994355   \n",
       "\n",
       "anchor                                       qwen-max-2025-01-25_phi-4  \\\n",
       "anchor                                                                   \n",
       "Baichuan-2-13B-Base_text-davinci-001                          0.995161   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                            0.994355   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                      0.995968   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                             0.998790   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                   1.000000   \n",
       "...                                                                ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                              0.995968   \n",
       "text-davinci-001_mpt-30b                                      0.995161   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                            0.999597   \n",
       "vicuna-13b-v1.1_gemma-2b                                      0.994355   \n",
       "xgen-7b-8k-base_Llama-2-7b                                    0.994355   \n",
       "\n",
       "anchor                                       qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Instruct  \\\n",
       "anchor                                                                                             \n",
       "Baichuan-2-13B-Base_text-davinci-001                                                  0.996774     \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                                    0.995968     \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                              0.997177     \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                                     0.999597     \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                           0.999597     \n",
       "...                                                                                        ...     \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                                      0.997177     \n",
       "text-davinci-001_mpt-30b                                                              0.996774     \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                                    1.000000     \n",
       "vicuna-13b-v1.1_gemma-2b                                                              0.995968     \n",
       "xgen-7b-8k-base_Llama-2-7b                                                            0.995968     \n",
       "\n",
       "anchor                                       qwen3-235b-a22b_gemini-2.0-pro-exp-02-05  \\\n",
       "anchor                                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                                         0.994758   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                           0.993952   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                     0.995565   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                            0.998387   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                  0.999597   \n",
       "...                                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                             0.995565   \n",
       "text-davinci-001_mpt-30b                                                     0.994758   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                           0.999194   \n",
       "vicuna-13b-v1.1_gemma-2b                                                     0.993952   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                   0.993952   \n",
       "\n",
       "anchor                                       stablelm-tuned-alpha-7b_opt-1.3b  \\\n",
       "anchor                                                                          \n",
       "Baichuan-2-13B-Base_text-davinci-001                                 0.999194   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                   0.999597   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                             1.000000   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                    0.997984   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                          0.995968   \n",
       "...                                                                       ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                     1.000000   \n",
       "text-davinci-001_mpt-30b                                             0.999194   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                   0.997177   \n",
       "vicuna-13b-v1.1_gemma-2b                                             0.999597   \n",
       "xgen-7b-8k-base_Llama-2-7b                                           0.999597   \n",
       "\n",
       "anchor                                       text-davinci-001_mpt-30b  \\\n",
       "anchor                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                         1.000000   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                           0.999597   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                     0.999194   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                            0.997984   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                  0.995161   \n",
       "...                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                             0.999194   \n",
       "text-davinci-001_mpt-30b                                     1.000000   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                           0.996774   \n",
       "vicuna-13b-v1.1_gemma-2b                                     0.999597   \n",
       "xgen-7b-8k-base_Llama-2-7b                                   0.999597   \n",
       "\n",
       "anchor                                       text-davinci-002_Qwen2.5-Coder-14B  \\\n",
       "anchor                                                                            \n",
       "Baichuan-2-13B-Base_text-davinci-001                                   0.996774   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                     0.995968   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                               0.997177   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                      0.999597   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                            0.999597   \n",
       "...                                                                         ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                       0.997177   \n",
       "text-davinci-001_mpt-30b                                               0.996774   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                     1.000000   \n",
       "vicuna-13b-v1.1_gemma-2b                                               0.995968   \n",
       "xgen-7b-8k-base_Llama-2-7b                                             0.995968   \n",
       "\n",
       "anchor                                       vicuna-13b-v1.1_gemma-2b  \\\n",
       "anchor                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                         0.999597   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                           1.000000   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                     0.999597   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                            0.997177   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                  0.994355   \n",
       "...                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                             0.999597   \n",
       "text-davinci-001_mpt-30b                                     0.999597   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                           0.995968   \n",
       "vicuna-13b-v1.1_gemma-2b                                     1.000000   \n",
       "xgen-7b-8k-base_Llama-2-7b                                   1.000000   \n",
       "\n",
       "anchor                                       xgen-7b-8k-base_Llama-2-7b  \n",
       "anchor                                                                   \n",
       "Baichuan-2-13B-Base_text-davinci-001                           0.999597  \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                             1.000000  \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                       0.999597  \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                              0.997177  \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                    0.994355  \n",
       "...                                                                 ...  \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                               0.999597  \n",
       "text-davinci-001_mpt-30b                                       0.999597  \n",
       "text-davinci-002_Qwen2.5-Coder-14B                             0.995968  \n",
       "vicuna-13b-v1.1_gemma-2b                                       1.000000  \n",
       "xgen-7b-8k-base_Llama-2-7b                                     1.000000  \n",
       "\n",
       "[149 rows x 149 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean off-diagonal Spearman per model-pair fit (difficulties):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claude-3-opus-20240229_Llama-3.2-90B-Vision-Instruct</th>\n",
       "      <td>0.998660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Instruct</th>\n",
       "      <td>0.998660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B-v0.1_text-davinci-002</th>\n",
       "      <td>0.998660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-70B-Instruct_StableBeluga2</th>\n",
       "      <td>0.998660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.2-90B-Vision-Instruct_Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.998660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-j-6b_LLaMA-7B</th>\n",
       "      <td>0.996464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-neox-20b_open_llama_7b</th>\n",
       "      <td>0.996464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b_xgen-7b-8k-base</th>\n",
       "      <td>0.996464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1_gemma-2b</th>\n",
       "      <td>0.996464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base_Llama-2-7b</th>\n",
       "      <td>0.996464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean_rho\n",
       "anchor                                                      \n",
       "claude-3-opus-20240229_Llama-3.2-90B-Vision-Ins...  0.998660\n",
       "qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Inst...  0.998660\n",
       "Mixtral-8x7B-v0.1_text-davinci-002                  0.998660\n",
       "Llama-3.1-70B-Instruct_StableBeluga2                0.998660\n",
       "Llama-3.2-90B-Vision-Instruct_Llama-3.1-70B-Ins...  0.998660\n",
       "...                                                      ...\n",
       "gpt-j-6b_LLaMA-7B                                   0.996464\n",
       "gpt-neox-20b_open_llama_7b                          0.996464\n",
       "gemma-2b_xgen-7b-8k-base                            0.996464\n",
       "vicuna-13b-v1.1_gemma-2b                            0.996464\n",
       "xgen-7b-8k-base_Llama-2-7b                          0.996464\n",
       "\n",
       "[149 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Spearman across anchor model pairs (model capabilities) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>anchor</th>\n",
       "      <th>Baichuan-2-13B-Base_text-davinci-001</th>\n",
       "      <th>Baichuan-2-7B-Base_vicuna-13b-v1.1</th>\n",
       "      <th>Cerebras-GPT-13B_stablelm-tuned-alpha-7b</th>\n",
       "      <th>Chinchilla (70B)_Qwen2.5-Coder-7B</th>\n",
       "      <th>DeepSeek-R1-0528_claude-opus-4-20250514_16K</th>\n",
       "      <th>DeepSeek-R1_claude-3-7-sonnet-20250219_16K</th>\n",
       "      <th>DeepSeek-V2_Qwen2.5-72B</th>\n",
       "      <th>DeepSeek-V3-0324_gpt-4.1-2025-04-14</th>\n",
       "      <th>DeepSeek-V3_gemini-1.5-pro-002</th>\n",
       "      <th>GLaM (MoE)_LLaMA-33B</th>\n",
       "      <th>...</th>\n",
       "      <th>phi-2_Llama-2-34b</th>\n",
       "      <th>phi-4_gpt-4.1-nano-2025-04-14</th>\n",
       "      <th>qwen-max-2025-01-25_phi-4</th>\n",
       "      <th>qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Instruct</th>\n",
       "      <th>qwen3-235b-a22b_gemini-2.0-pro-exp-02-05</th>\n",
       "      <th>stablelm-tuned-alpha-7b_opt-1.3b</th>\n",
       "      <th>text-davinci-001_mpt-30b</th>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <th>vicuna-13b-v1.1_gemma-2b</th>\n",
       "      <th>xgen-7b-8k-base_Llama-2-7b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-13B-Base_text-davinci-001</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.993223</td>\n",
       "      <td>0.999605</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.999829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999783</td>\n",
       "      <td>0.999460</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baichuan-2-7B-Base_vicuna-13b-v1.1</th>\n",
       "      <td>0.999940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999253</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.999570</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999342</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.997987</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebras-GPT-13B_stablelm-tuned-alpha-7b</th>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999335</td>\n",
       "      <td>0.999353</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.997952</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinchilla (70B)_Qwen2.5-Coder-7B</th>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999253</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.994563</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.999424</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.999388</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.999275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-R1-0528_claude-opus-4-20250514_16K</th>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.993635</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999691</td>\n",
       "      <td>0.998176</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.999662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b_opt-1.3b</th>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999495</td>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>0.999271</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.999204</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.999243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999829</td>\n",
       "      <td>0.997678</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-001_mpt-30b</th>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.999691</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.993194</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.999431</td>\n",
       "      <td>0.999442</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.997987</td>\n",
       "      <td>0.997952</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>0.998176</td>\n",
       "      <td>0.998158</td>\n",
       "      <td>0.994421</td>\n",
       "      <td>0.998208</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>0.998435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998585</td>\n",
       "      <td>0.998315</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>0.998290</td>\n",
       "      <td>0.998108</td>\n",
       "      <td>0.997678</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997980</td>\n",
       "      <td>0.997984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.1_gemma-2b</th>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999246</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.992757</td>\n",
       "      <td>0.999509</td>\n",
       "      <td>0.999371</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>0.999335</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.997980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgen-7b-8k-base_Llama-2-7b</th>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.999662</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.992761</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>0.999388</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.999324</td>\n",
       "      <td>0.999349</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "anchor                                       Baichuan-2-13B-Base_text-davinci-001  \\\n",
       "anchor                                                                              \n",
       "Baichuan-2-13B-Base_text-davinci-001                                     1.000000   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                       0.999940   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                 0.999911   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                        0.999417   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                              0.999716   \n",
       "...                                                                           ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                         0.999844   \n",
       "text-davinci-001_mpt-30b                                                 0.999972   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                       0.998254   \n",
       "vicuna-13b-v1.1_gemma-2b                                                 0.999932   \n",
       "xgen-7b-8k-base_Llama-2-7b                                               0.999932   \n",
       "\n",
       "anchor                                       Baichuan-2-7B-Base_vicuna-13b-v1.1  \\\n",
       "anchor                                                                            \n",
       "Baichuan-2-13B-Base_text-davinci-001                                   0.999940   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                     1.000000   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                               0.999950   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                      0.999253   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                            0.999652   \n",
       "...                                                                         ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                       0.999929   \n",
       "text-davinci-001_mpt-30b                                               0.999929   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                     0.997987   \n",
       "vicuna-13b-v1.1_gemma-2b                                               0.999979   \n",
       "xgen-7b-8k-base_Llama-2-7b                                             0.999986   \n",
       "\n",
       "anchor                                       Cerebras-GPT-13B_stablelm-tuned-alpha-7b  \\\n",
       "anchor                                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                                         0.999911   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                           0.999950   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                     1.000000   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                            0.999246   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                  0.999655   \n",
       "...                                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                             0.999936   \n",
       "text-davinci-001_mpt-30b                                                     0.999897   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                           0.997952   \n",
       "vicuna-13b-v1.1_gemma-2b                                                     0.999943   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                   0.999964   \n",
       "\n",
       "anchor                                       Chinchilla (70B)_Qwen2.5-Coder-7B  \\\n",
       "anchor                                                                           \n",
       "Baichuan-2-13B-Base_text-davinci-001                                  0.999417   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                    0.999253   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                              0.999246   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                     1.000000   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                           0.999420   \n",
       "...                                                                        ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                      0.999068   \n",
       "text-davinci-001_mpt-30b                                              0.999392   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                    0.998414   \n",
       "vicuna-13b-v1.1_gemma-2b                                              0.999246   \n",
       "xgen-7b-8k-base_Llama-2-7b                                            0.999275   \n",
       "\n",
       "anchor                                       DeepSeek-R1-0528_claude-opus-4-20250514_16K  \\\n",
       "anchor                                                                                     \n",
       "Baichuan-2-13B-Base_text-davinci-001                                            0.999716   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                              0.999652   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                        0.999655   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                               0.999420   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                     1.000000   \n",
       "...                                                                                  ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                                0.999584   \n",
       "text-davinci-001_mpt-30b                                                        0.999691   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                              0.998176   \n",
       "vicuna-13b-v1.1_gemma-2b                                                        0.999644   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                      0.999662   \n",
       "\n",
       "anchor                                       DeepSeek-R1_claude-3-7-sonnet-20250219_16K  \\\n",
       "anchor                                                                                    \n",
       "Baichuan-2-13B-Base_text-davinci-001                                           0.999648   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                             0.999570   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                       0.999577   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                              0.999438   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                    0.999950   \n",
       "...                                                                                 ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                               0.999495   \n",
       "text-davinci-001_mpt-30b                                                       0.999623   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                             0.998158   \n",
       "vicuna-13b-v1.1_gemma-2b                                                       0.999563   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                     0.999580   \n",
       "\n",
       "anchor                                       DeepSeek-V2_Qwen2.5-72B  \\\n",
       "anchor                                                                 \n",
       "Baichuan-2-13B-Base_text-davinci-001                        0.993223   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                          0.992764   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                    0.992739   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                           0.994563   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                 0.993635   \n",
       "...                                                              ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                            0.992163   \n",
       "text-davinci-001_mpt-30b                                    0.993194   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                          0.994421   \n",
       "vicuna-13b-v1.1_gemma-2b                                    0.992757   \n",
       "xgen-7b-8k-base_Llama-2-7b                                  0.992761   \n",
       "\n",
       "anchor                                       DeepSeek-V3-0324_gpt-4.1-2025-04-14  \\\n",
       "anchor                                                                             \n",
       "Baichuan-2-13B-Base_text-davinci-001                                    0.999605   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                      0.999516   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                0.999520   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                       0.999438   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                             0.999932   \n",
       "...                                                                          ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                        0.999428   \n",
       "text-davinci-001_mpt-30b                                                0.999580   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                      0.998208   \n",
       "vicuna-13b-v1.1_gemma-2b                                                0.999509   \n",
       "xgen-7b-8k-base_Llama-2-7b                                              0.999524   \n",
       "\n",
       "anchor                                       DeepSeek-V3_gemini-1.5-pro-002  \\\n",
       "anchor                                                                        \n",
       "Baichuan-2-13B-Base_text-davinci-001                               0.999492   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                 0.999378   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                           0.999385   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                  0.999396   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                        0.999872   \n",
       "...                                                                     ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                   0.999271   \n",
       "text-davinci-001_mpt-30b                                           0.999467   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                 0.998275   \n",
       "vicuna-13b-v1.1_gemma-2b                                           0.999371   \n",
       "xgen-7b-8k-base_Llama-2-7b                                         0.999388   \n",
       "\n",
       "anchor                                       GLaM (MoE)_LLaMA-33B  ...  \\\n",
       "anchor                                                             ...   \n",
       "Baichuan-2-13B-Base_text-davinci-001                     0.999829  ...   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                       0.999758  ...   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                 0.999708  ...   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                        0.999488  ...   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K              0.999666  ...   \n",
       "...                                                           ...  ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                         0.999602  ...   \n",
       "text-davinci-001_mpt-30b                                 0.999808  ...   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                       0.998435  ...   \n",
       "vicuna-13b-v1.1_gemma-2b                                 0.999751  ...   \n",
       "xgen-7b-8k-base_Llama-2-7b                               0.999751  ...   \n",
       "\n",
       "anchor                                       phi-2_Llama-2-34b  \\\n",
       "anchor                                                           \n",
       "Baichuan-2-13B-Base_text-davinci-001                  0.999783   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                    0.999669   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b              0.999623   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                     0.999637   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K           0.999716   \n",
       "...                                                        ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                      0.999492   \n",
       "text-davinci-001_mpt-30b                              0.999762   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                    0.998585   \n",
       "vicuna-13b-v1.1_gemma-2b                              0.999662   \n",
       "xgen-7b-8k-base_Llama-2-7b                            0.999659   \n",
       "\n",
       "anchor                                       phi-4_gpt-4.1-nano-2025-04-14  \\\n",
       "anchor                                                                       \n",
       "Baichuan-2-13B-Base_text-davinci-001                              0.999460   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                0.999328   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                          0.999335   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                 0.999424   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                       0.999812   \n",
       "...                                                                    ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                  0.999204   \n",
       "text-davinci-001_mpt-30b                                          0.999431   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                0.998315   \n",
       "vicuna-13b-v1.1_gemma-2b                                          0.999321   \n",
       "xgen-7b-8k-base_Llama-2-7b                                        0.999339   \n",
       "\n",
       "anchor                                       qwen-max-2025-01-25_phi-4  \\\n",
       "anchor                                                                   \n",
       "Baichuan-2-13B-Base_text-davinci-001                          0.999470   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                            0.999346   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                      0.999353   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                             0.999435   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                   0.999822   \n",
       "...                                                                ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                              0.999225   \n",
       "text-davinci-001_mpt-30b                                      0.999442   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                            0.998325   \n",
       "vicuna-13b-v1.1_gemma-2b                                      0.999339   \n",
       "xgen-7b-8k-base_Llama-2-7b                                    0.999356   \n",
       "\n",
       "anchor                                       qwen2.5-72b-instruct_Llama-4-Scout-17B-16E-Instruct  \\\n",
       "anchor                                                                                             \n",
       "Baichuan-2-13B-Base_text-davinci-001                                                  0.999442     \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                                    0.999314     \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                              0.999321     \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                                     0.999388     \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                           0.999776     \n",
       "...                                                                                        ...     \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                                      0.999189     \n",
       "text-davinci-001_mpt-30b                                                              0.999413     \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                                    0.998290     \n",
       "vicuna-13b-v1.1_gemma-2b                                                              0.999307     \n",
       "xgen-7b-8k-base_Llama-2-7b                                                            0.999324     \n",
       "\n",
       "anchor                                       qwen3-235b-a22b_gemini-2.0-pro-exp-02-05  \\\n",
       "anchor                                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                                         0.999410   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                           0.999342   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                                     0.999346   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                            0.999307   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                                  0.999772   \n",
       "...                                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                             0.999243   \n",
       "text-davinci-001_mpt-30b                                                     0.999385   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                           0.998108   \n",
       "vicuna-13b-v1.1_gemma-2b                                                     0.999335   \n",
       "xgen-7b-8k-base_Llama-2-7b                                                   0.999349   \n",
       "\n",
       "anchor                                       stablelm-tuned-alpha-7b_opt-1.3b  \\\n",
       "anchor                                                                          \n",
       "Baichuan-2-13B-Base_text-davinci-001                                 0.999844   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                   0.999929   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                             0.999936   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                    0.999068   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                          0.999584   \n",
       "...                                                                       ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                     1.000000   \n",
       "text-davinci-001_mpt-30b                                             0.999829   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                   0.997678   \n",
       "vicuna-13b-v1.1_gemma-2b                                             0.999922   \n",
       "xgen-7b-8k-base_Llama-2-7b                                           0.999947   \n",
       "\n",
       "anchor                                       text-davinci-001_mpt-30b  \\\n",
       "anchor                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                         0.999972   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                           0.999929   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                     0.999897   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                            0.999392   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                  0.999691   \n",
       "...                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                             0.999829   \n",
       "text-davinci-001_mpt-30b                                     1.000000   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                           0.998226   \n",
       "vicuna-13b-v1.1_gemma-2b                                     0.999922   \n",
       "xgen-7b-8k-base_Llama-2-7b                                   0.999922   \n",
       "\n",
       "anchor                                       text-davinci-002_Qwen2.5-Coder-14B  \\\n",
       "anchor                                                                            \n",
       "Baichuan-2-13B-Base_text-davinci-001                                   0.998254   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                                     0.997987   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                               0.997952   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                                      0.998414   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                            0.998176   \n",
       "...                                                                         ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                                       0.997678   \n",
       "text-davinci-001_mpt-30b                                               0.998226   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                                     1.000000   \n",
       "vicuna-13b-v1.1_gemma-2b                                               0.997980   \n",
       "xgen-7b-8k-base_Llama-2-7b                                             0.997984   \n",
       "\n",
       "anchor                                       vicuna-13b-v1.1_gemma-2b  \\\n",
       "anchor                                                                  \n",
       "Baichuan-2-13B-Base_text-davinci-001                         0.999932   \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                           0.999979   \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                     0.999943   \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                            0.999246   \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                  0.999644   \n",
       "...                                                               ...   \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                             0.999922   \n",
       "text-davinci-001_mpt-30b                                     0.999922   \n",
       "text-davinci-002_Qwen2.5-Coder-14B                           0.997980   \n",
       "vicuna-13b-v1.1_gemma-2b                                     1.000000   \n",
       "xgen-7b-8k-base_Llama-2-7b                                   0.999979   \n",
       "\n",
       "anchor                                       xgen-7b-8k-base_Llama-2-7b  \n",
       "anchor                                                                   \n",
       "Baichuan-2-13B-Base_text-davinci-001                           0.999932  \n",
       "Baichuan-2-7B-Base_vicuna-13b-v1.1                             0.999986  \n",
       "Cerebras-GPT-13B_stablelm-tuned-alpha-7b                       0.999964  \n",
       "Chinchilla (70B)_Qwen2.5-Coder-7B                              0.999275  \n",
       "DeepSeek-R1-0528_claude-opus-4-20250514_16K                    0.999662  \n",
       "...                                                                 ...  \n",
       "stablelm-tuned-alpha-7b_opt-1.3b                               0.999947  \n",
       "text-davinci-001_mpt-30b                                       0.999922  \n",
       "text-davinci-002_Qwen2.5-Coder-14B                             0.997984  \n",
       "vicuna-13b-v1.1_gemma-2b                                       0.999979  \n",
       "xgen-7b-8k-base_Llama-2-7b                                     1.000000  \n",
       "\n",
       "[149 rows x 149 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean off-diagonal Spearman per model-pair fit (capabilities):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_rho</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>o1-mini-2024-09-12_high_claude-opus-4-1-20250805</th>\n",
       "      <td>0.999630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-3-7-sonnet-20250219_16K_o1-mini-2024-09-12_high</th>\n",
       "      <td>0.999630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grok-3-beta_claude-opus-4-20250514</th>\n",
       "      <td>0.999628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o1-mini-2024-09-12_medium_qwen3-235b-a22b</th>\n",
       "      <td>0.999627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grok-3-mini-beta_low_grok-3-beta</th>\n",
       "      <td>0.999625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613_internlm-20b</th>\n",
       "      <td>0.998529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-Coder-14B_falcon-180B</th>\n",
       "      <td>0.998440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-002_Qwen2.5-Coder-14B</th>\n",
       "      <td>0.998211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-4k-instruct_PaLM 2-L</th>\n",
       "      <td>0.997282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek-V2_Qwen2.5-72B</th>\n",
       "      <td>0.994050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean_rho\n",
       "anchor                                                      \n",
       "o1-mini-2024-09-12_high_claude-opus-4-1-20250805    0.999630\n",
       "claude-3-7-sonnet-20250219_16K_o1-mini-2024-09-...  0.999630\n",
       "grok-3-beta_claude-opus-4-20250514                  0.999628\n",
       "o1-mini-2024-09-12_medium_qwen3-235b-a22b           0.999627\n",
       "grok-3-mini-beta_low_grok-3-beta                    0.999625\n",
       "...                                                      ...\n",
       "gpt-3.5-turbo-0613_internlm-20b                     0.998529\n",
       "Qwen2.5-Coder-14B_falcon-180B                       0.998440\n",
       "text-davinci-002_Qwen2.5-Coder-14B                  0.998211\n",
       "Phi-3-mini-4k-instruct_PaLM 2-L                     0.997282\n",
       "DeepSeek-V2_Qwen2.5-72B                             0.994050\n",
       "\n",
       "[149 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Spearman rank correlation across anchor model pairs\n",
    "# - Difficulties (by benchmark)\n",
    "# - Capabilities (by model)\n",
    "# Robust to duplicates and missing values\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _spearman_corr_from_long(\n",
    "    df_long: pd.DataFrame, index_col: str, columns_col: str, values_col: str\n",
    ") -> pd.DataFrame:\n",
    "    wide = df_long.pivot_table(\n",
    "        index=index_col,\n",
    "        columns=columns_col,\n",
    "        values=values_col,\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "    wide = wide.apply(pd.to_numeric, errors=\"coerce\").dropna(axis=0, how=\"all\")\n",
    "    if wide.shape[1] < 2:\n",
    "        return pd.DataFrame()\n",
    "    ranks = wide.rank(axis=0, method=\"average\", na_option=\"keep\")\n",
    "    return ranks.corr(method=\"pearson\", min_periods=2)\n",
    "\n",
    "\n",
    "# --- Difficulties across anchor model pairs ---------------------------------\n",
    "print(\"=== Spearman across anchor model pairs (benchmark difficulties) ===\")\n",
    "spearman_difficulty_pairs = _spearman_corr_from_long(\n",
    "    summary_benchmarks.rename(columns={\"anchor_model_pair\": \"anchor\"}),\n",
    "    index_col=\"benchmark_name\",\n",
    "    columns_col=\"anchor\",\n",
    "    values_col=\"estimated_difficulty\",\n",
    ")\n",
    "if spearman_difficulty_pairs.empty:\n",
    "    print(\n",
    "        \"Not enough comparable fits to compute correlations for difficulties across model pairs.\"\n",
    "    )\n",
    "else:\n",
    "    display(spearman_difficulty_pairs)\n",
    "    mean_rho_difficulty_pairs = (\n",
    "        spearman_difficulty_pairs.apply(lambda s: s.drop(labels=s.name).mean(), axis=0)\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(\"mean_rho\")\n",
    "    )\n",
    "    print(\"\\nMean off-diagonal Spearman per model-pair fit (difficulties):\")\n",
    "    display(mean_rho_difficulty_pairs)\n",
    "\n",
    "# --- Capabilities across anchor model pairs ---------------------------------\n",
    "print(\"\\n=== Spearman across anchor model pairs (model capabilities) ===\")\n",
    "spearman_capability_pairs = _spearman_corr_from_long(\n",
    "    summary_models.rename(columns={\"anchor_model_pair\": \"anchor\"}),\n",
    "    index_col=\"model\",\n",
    "    columns_col=\"anchor\",\n",
    "    values_col=\"estimated_capability\",\n",
    ")\n",
    "if spearman_capability_pairs.empty:\n",
    "    print(\n",
    "        \"Not enough comparable fits to compute correlations for capabilities across model pairs.\"\n",
    "    )\n",
    "else:\n",
    "    display(spearman_capability_pairs)\n",
    "    mean_rho_capability_pairs = (\n",
    "        spearman_capability_pairs.apply(lambda s: s.drop(labels=s.name).mean(), axis=0)\n",
    "        .sort_values(ascending=False)\n",
    "        .to_frame(\"mean_rho\")\n",
    "    )\n",
    "    print(\"\\nMean off-diagonal Spearman per model-pair fit (capabilities):\")\n",
    "    display(mean_rho_capability_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8ab8f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
