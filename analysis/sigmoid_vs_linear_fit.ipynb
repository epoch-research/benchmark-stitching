{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02f8ab9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a733e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)\n",
    "# print(\"cwd is now:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59f7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null performances after coercion: 0\n",
      "after filter num benchmarks 1965\n",
      "after merge with model versions 1965\n",
      "after date filter (>= 2022-11-01) 1766\n",
      "after merge with benchmark dates 1766\n",
      "Original number of rows: 1766\n",
      "Number of rows after aggregation: 1324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "from data_loader import scores_df\n",
    "# from fit import fit_statistical_model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f995d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_benchmark = \"Winogrande\"\n",
    "anchor_difficulty = 0\n",
    "anchor_slope = 1\n",
    "# df1, df_cm1, df_db1 = fit_statistical_model(scores_df, anchor_benchmark, anchor_difficulty, anchor_slope)\n",
    "\n",
    "# # Convert date strings to datetime objects\n",
    "# df_cm1['date_obj'] = pd.to_datetime(df_cm1['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c236504",
   "metadata": {},
   "source": [
    "# Cross validation (Appendix E.4: Varying the statistical model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1fbd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# MODIFIED: Added anchor_slope and anchor_difficulty to the function signature.\n",
    "def split_params(params: np.ndarray, num_models: int, num_benchmarks: int, anchor_idx: int, model_type: str, anchor_slope: float, anchor_difficulty: float):\n",
    "    \"\"\"\n",
    "    Breaks the flat parameter vector into C, D, full-length α,\n",
    "    and gammas (for sigmoid_with_offsets).\n",
    "    \"\"\"\n",
    "    C = params[:num_models]\n",
    "\n",
    "    # MODIFICATION: Reconstruct D by inserting the fixed anchor difficulty.\n",
    "    # The optimizer only solves for the other N-1 difficulties.\n",
    "    D_free = params[num_models : num_models + num_benchmarks - 1]\n",
    "    D = np.insert(D_free, anchor_idx, anchor_difficulty)\n",
    "\n",
    "    # MODIFICATION: Adjust index for alpha and insert the fixed anchor slope.\n",
    "    alpha_free = params[num_models + num_benchmarks - 1 : num_models + 2 * num_benchmarks - 2]\n",
    "    alpha = np.insert(alpha_free, anchor_idx, anchor_slope)\n",
    "\n",
    "    # Handle parameters specific to the 'sigmoid_with_offsets' model\n",
    "    if model_type == 'sigmoid_with_offsets':\n",
    "        # MODIFICATION: Adjust indices for gamma parameters.\n",
    "        gamma_lower = params[num_models + 2 * num_benchmarks - 2 : num_models + 3 * num_benchmarks - 2]\n",
    "        gamma_upper = params[num_models + 3 * num_benchmarks - 2:]\n",
    "        return C, D, alpha, gamma_lower, gamma_upper\n",
    "\n",
    "    return C, D, alpha\n",
    "\n",
    "# MODIFIED: Added anchor_slope and anchor_difficulty to the function signature.\n",
    "def predict(params, model_idx, bench_idx, num_models, num_benchmarks, anchor_idx, model_type, anchor_slope, anchor_difficulty):\n",
    "    \"\"\"Calculates predictions based on the specified model type.\"\"\"\n",
    "    # Unpack the parameters based on the model type\n",
    "    # MODIFICATION: Pass anchor values to split_params\n",
    "    split_args = (params, num_models, num_benchmarks, anchor_idx, model_type, anchor_slope, anchor_difficulty)\n",
    "    if model_type == 'sigmoid_with_offsets':\n",
    "        C, D, alpha, lower_asymptote, upper_asymptote = split_params(*split_args)\n",
    "    else:\n",
    "        C, D, alpha = split_params(*split_args)\n",
    "\n",
    "    # Core prediction logic\n",
    "    x = alpha[bench_idx] * (C[model_idx] - D[bench_idx])\n",
    "\n",
    "    # Apply the correct functional form\n",
    "    if model_type == 'clipped_linear':\n",
    "        return np.clip(x, 0, 1)\n",
    "    elif model_type == 'sigmoid_with_offsets':\n",
    "        return lower_asymptote[bench_idx] + (upper_asymptote[bench_idx] - lower_asymptote[bench_idx]) / (1 + np.exp(-x))\n",
    "    else:  # Default to 'sigmoid'\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def fit_statistical_model(\n",
    "    df,\n",
    "    anchor_benchmark,\n",
    "    anchor_difficulty,\n",
    "    anchor_slope,\n",
    "    slope_init=0.05,\n",
    "    df_model=None,\n",
    "    model_type='sigmoid',\n",
    "    full_dataset_maps=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits a statistical model to benchmark performance data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with benchmark performance data.\n",
    "        anchor_benchmark (str): The name of the anchor benchmark.\n",
    "        anchor_difficulty (float): The difficulty of the anchor benchmark to be fixed.\n",
    "        anchor_slope (float): The slope of the anchor benchmark to be fixed.\n",
    "        slope_init (float, optional): Initial slope value. Defaults to 0.05.\n",
    "        df_model (pd.DataFrame, optional): DataFrame with model metadata. Defaults to None.\n",
    "        model_type (str, optional): The type of model to fit.\n",
    "        full_dataset_maps (dict, optional): Pre-computed mappings for models and benchmarks.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Mappings & data arrays\n",
    "    # ------------------------------------------------------------\n",
    "    if full_dataset_maps:\n",
    "        model_id_to_idx = full_dataset_maps['model_id_to_idx']\n",
    "        bench_id_to_idx = full_dataset_maps['bench_id_to_idx']\n",
    "        valid_model_ids = list(model_id_to_idx.keys())\n",
    "        benchmark_ids = list(bench_id_to_idx.keys())\n",
    "    else:\n",
    "        valid_model_ids = df[\"model_id\"].unique()\n",
    "        benchmark_ids = df[\"benchmark_id\"].unique()\n",
    "        model_id_to_idx = {m_id: i for i, m_id in enumerate(valid_model_ids)}\n",
    "        bench_id_to_idx = {b_id: i for i, b_id in enumerate(benchmark_ids)}\n",
    "\n",
    "    num_models = len(valid_model_ids)\n",
    "    num_benchmarks = len(benchmark_ids)\n",
    "\n",
    "    model_idx_data = np.array([model_id_to_idx[m] for m in df[\"model_id\"]])\n",
    "    bench_idx_data = np.array([bench_id_to_idx[b] for b in df[\"benchmark_id\"]])\n",
    "    observed_scores = df[\"performance\"].values\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Anchor benchmark\n",
    "    # ------------------------------------------------------------\n",
    "    try:\n",
    "        anchor_bench_id_from_df = df.loc[df[\"benchmark\"] == anchor_benchmark, \"benchmark_id\"].iloc[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"Anchor benchmark “{anchor_benchmark}” not found in the provided dataframe split.\")\n",
    "\n",
    "    anchor_idx = bench_id_to_idx[anchor_bench_id_from_df]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Residuals function for least squares\n",
    "    # ------------------------------------------------------------\n",
    "    def residuals(params, model_idx, bench_idx, y):\n",
    "        \"\"\"Calculate the residuals for the least squares fit.\"\"\"\n",
    "        # MODIFICATION: Pass anchor values to the predict function.\n",
    "        preds = predict(params, model_idx, bench_idx, num_models, num_benchmarks, anchor_idx, model_type, anchor_slope, anchor_difficulty)\n",
    "        return preds - y\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Initial guesses\n",
    "    # ------------------------------------------------------------\n",
    "    initial_C = np.zeros(num_models)\n",
    "    # MODIFICATION: We only optimize N-1 difficulties, since one is fixed.\n",
    "    initial_D = np.zeros(num_benchmarks - 1)\n",
    "    initial_alpha = np.full(num_benchmarks - 1, slope_init)\n",
    "    initial_theta = np.concatenate([initial_C, initial_D, initial_alpha])\n",
    "\n",
    "    if model_type == 'sigmoid_with_offsets':\n",
    "        # MODIFICATION: The number of gamma parameters is still num_benchmarks\n",
    "        initial_gamma_lower = np.full(num_benchmarks, 0.0)\n",
    "        initial_gamma_upper = np.full(num_benchmarks, 1.0)\n",
    "        initial_theta = np.concatenate([initial_theta, initial_gamma_lower, initial_gamma_upper])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5) Fit\n",
    "    # ------------------------------------------------------------\n",
    "    result = least_squares(\n",
    "        residuals,\n",
    "        initial_theta,\n",
    "        args=(model_idx_data, bench_idx_data, observed_scores),\n",
    "        method=\"trf\",\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6) Recover full parameter vectors and normalize\n",
    "    # ------------------------------------------------------------\n",
    "    theta_hat = result.x\n",
    "\n",
    "    # MODIFICATION: Pass anchor values to get the full parameter vectors.\n",
    "    split_args = (theta_hat, num_models, num_benchmarks, anchor_idx, model_type, anchor_slope, anchor_difficulty)\n",
    "    if model_type == 'sigmoid_with_offsets':\n",
    "        C_hat, D_hat, alpha_hat, gammas_hat_lower, gammas_hat_upper = split_params(*split_args)\n",
    "    else:\n",
    "        C_hat, D_hat, alpha_hat = split_params(*split_args)\n",
    "\n",
    "    # MODIFICATION: The normalization shift now correctly centers the capabilities\n",
    "    # around the fixed anchor difficulty. We no longer need to shift D_hat.\n",
    "    shift = D_hat[anchor_idx] # This is equal to anchor_difficulty\n",
    "    C_hat -= shift\n",
    "    # D_hat -= shift # This line is no longer needed as D is fixed.\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7) Pack tidy DataFrames for inspection / downstream use\n",
    "    # ------------------------------------------------------------\n",
    "    # (This section remains unchanged)\n",
    "    if df_model is not None:\n",
    "        id_to_name = df.drop_duplicates(\"model_id\").set_index(\"model_id\")[\"model\"].to_dict()\n",
    "        model_cap_df = (\n",
    "            pd.DataFrame({\"model_id\": valid_model_ids, \"estimated_capability\": C_hat})\n",
    "            .assign(model=lambda d: d[\"model_id\"].map(id_to_name))\n",
    "        )\n",
    "        model_cap_df = model_cap_df.merge(df_model, on=\"model\", how=\"left\")\n",
    "        model_capabilities_df = model_cap_df.sort_values(\"estimated_capability\", ascending=False)\n",
    "    else:\n",
    "        model_capabilities_df = pd.DataFrame(\n",
    "            {\"model_id\": valid_model_ids, \"estimated_capability\": C_hat}\n",
    "        ).sort_values(\"estimated_capability\", ascending=False)\n",
    "\n",
    "    release_date_map = (\n",
    "        df.drop_duplicates(\"benchmark_id\")\n",
    "          .set_index(\"benchmark_id\")[\"benchmark_release_date\"]\n",
    "          .to_dict()\n",
    "    )\n",
    "\n",
    "    benchmark_params_df = pd.DataFrame({\n",
    "        \"benchmark_id\": benchmark_ids,\n",
    "        \"estimated_difficulty\": D_hat,\n",
    "        \"estimated_slope\": alpha_hat,\n",
    "    })\n",
    "\n",
    "    if model_type == 'sigmoid_with_offsets':\n",
    "        benchmark_params_df['estimated_lower_asymptote'] = gammas_hat_lower\n",
    "        benchmark_params_df['estimated_upper_asymptote'] = gammas_hat_upper\n",
    "\n",
    "    benchmark_params_df = benchmark_params_df.assign(\n",
    "        benchmark_name=lambda d: d[\"benchmark_id\"].map(\n",
    "            dict(zip(df[\"benchmark_id\"], df[\"benchmark\"]))\n",
    "        ),\n",
    "        benchmark_release_date=lambda d: d[\"benchmark_id\"].map(release_date_map),\n",
    "    ).sort_values(\"estimated_difficulty\")\n",
    "\n",
    "    return theta_hat, model_capabilities_df, benchmark_params_df\n",
    "\n",
    "# ==============================================================================\n",
    "# NEW MODEL COMPARISON AND CROSS-VALIDATION CODE\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, num_params):\n",
    "    \"\"\"\n",
    "    Calculates R-squared, MSE, AIC, and BIC.\n",
    "    \"\"\"\n",
    "    n_obs = len(y_true)\n",
    "    rss = np.sum((y_true - y_pred) ** 2)\n",
    "    mse = rss / n_obs\n",
    "    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (rss / tss) if tss > 0 else 0\n",
    "    aic = n_obs * np.log(mse) + 2 * num_params if mse > 0 else -np.inf\n",
    "    bic = n_obs * np.log(mse) + num_params * np.log(n_obs) if mse > 0 else -np.inf\n",
    "    return {'r2': r2, 'mse': mse, 'aic': aic, 'bic': bic}\n",
    "\n",
    "\n",
    "def compare_models_cross_validation(df, anchor_benchmark, anchor_difficulty, anchor_slope, n_splits=10):\n",
    "    \"\"\"\n",
    "    Compares sigmoid and clipped_linear models using k-fold cross-validation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The full dataset.\n",
    "        anchor_benchmark (str): The name of the anchor benchmark.\n",
    "        anchor_difficulty (float): The difficulty value for the anchor benchmark.\n",
    "        anchor_slope (float): The slope value for the anchor benchmark.\n",
    "        n_splits (int): The number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    model_types = ['sigmoid', 'clipped_linear']\n",
    "    results = []\n",
    "\n",
    "    # Pre-calculate mappings from the FULL dataset for consistency across folds\n",
    "    valid_model_ids = df['model_id'].unique()\n",
    "    benchmark_ids = df['benchmark_id'].unique()\n",
    "    model_id_to_idx = {m_id: i for i, m_id in enumerate(valid_model_ids)}\n",
    "    bench_id_to_idx = {b_id: i for i, b_id in enumerate(benchmark_ids)}\n",
    "    num_models = len(valid_model_ids)\n",
    "    num_benchmarks = len(benchmark_ids)\n",
    "\n",
    "    try:\n",
    "        anchor_bench_id = df.loc[df[\"benchmark\"] == anchor_benchmark, \"benchmark_id\"].iloc[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"Anchor benchmark '{anchor_benchmark}' not found in the dataset.\")\n",
    "\n",
    "    anchor_idx = bench_id_to_idx[anchor_bench_id]\n",
    "\n",
    "    full_dataset_maps = {\n",
    "        'model_id_to_idx': model_id_to_idx,\n",
    "        'bench_id_to_idx': bench_id_to_idx,\n",
    "    }\n",
    "\n",
    "    print(f\"Running {n_splits}-fold cross-validation...\")\n",
    "    for model_type in model_types:\n",
    "        print(f\"  Evaluating model: {model_type}\")\n",
    "        y_pred_out_of_sample = np.zeros(len(df))\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "            # print(f\"    - Fold {fold+1}/{n_splits}\") # Optional: uncomment for more verbose output\n",
    "            df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "            # --- FIX 1: Pass anchor_difficulty and anchor_slope to the fitting function ---\n",
    "            theta_hat, _, _ = fit_statistical_model(\n",
    "                df=df_train,\n",
    "                anchor_benchmark=anchor_benchmark,\n",
    "                anchor_difficulty=anchor_difficulty, # Corrected\n",
    "                anchor_slope=anchor_slope,         # Corrected\n",
    "                model_type=model_type,\n",
    "                full_dataset_maps=full_dataset_maps\n",
    "            )\n",
    "\n",
    "            # Get indices for the test data using the full dataset mapping\n",
    "            model_idx_test = np.array([model_id_to_idx[m] for m in df_test[\"model_id\"]])\n",
    "            bench_idx_test = np.array([bench_id_to_idx[b] for b in df_test[\"benchmark_id\"]])\n",
    "\n",
    "            # --- FIX 2: Pass anchor_difficulty and anchor_slope to the predict function ---\n",
    "            y_pred_test = predict(\n",
    "                theta_hat,\n",
    "                model_idx_test,\n",
    "                bench_idx_test,\n",
    "                num_models,\n",
    "                num_benchmarks,\n",
    "                anchor_idx,\n",
    "                model_type,\n",
    "                anchor_slope=anchor_slope,         # Corrected\n",
    "                anchor_difficulty=anchor_difficulty  # Corrected\n",
    "            )\n",
    "            y_pred_out_of_sample[test_index] = y_pred_test\n",
    "\n",
    "        # --- Calculate metrics based on out-of-sample CV predictions ---\n",
    "        y_true = df['performance'].values\n",
    "        cv_mse = np.mean((y_true - y_pred_out_of_sample) ** 2)\n",
    "        cv_tss = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        cv_rss = np.sum((y_true - y_pred_out_of_sample) ** 2)\n",
    "        cv_r2 = 1 - (cv_rss / cv_tss)\n",
    "\n",
    "        # --- Fit on full data to get AIC/BIC ---\n",
    "        print(f\"  Fitting {model_type} on full data for AIC/BIC...\")\n",
    "        # --- FIX 3: Use the passed anchor values instead of hardcoded ones ---\n",
    "        theta_full, _, _ = fit_statistical_model(\n",
    "            df=df,\n",
    "            anchor_benchmark=anchor_benchmark,\n",
    "            anchor_difficulty=anchor_difficulty, # Corrected\n",
    "            anchor_slope=anchor_slope,         # Corrected\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        model_idx_full = np.array([model_id_to_idx[m] for m in df[\"model_id\"]])\n",
    "        bench_idx_full = np.array([bench_id_to_idx[b] for b in df[\"benchmark_id\"]])\n",
    "\n",
    "        # --- FIX 4: Pass anchor values to the final predict call ---\n",
    "        y_pred_full = predict(\n",
    "            theta_full,\n",
    "            model_idx_full,\n",
    "            bench_idx_full,\n",
    "            num_models,\n",
    "            num_benchmarks,\n",
    "            anchor_idx,\n",
    "            model_type,\n",
    "            anchor_slope=anchor_slope,         # Corrected\n",
    "            anchor_difficulty=anchor_difficulty  # Corrected\n",
    "        )\n",
    "\n",
    "        num_params = len(theta_full)\n",
    "        full_data_metrics = calculate_metrics(y_true, y_pred_full, num_params)\n",
    "\n",
    "        results.append({\n",
    "            'model_type': model_type,\n",
    "            'cv_r2': cv_r2,\n",
    "            'cv_mse': cv_mse,\n",
    "            'aic': full_data_metrics['aic'],\n",
    "            'bic': full_data_metrics['bic'],\n",
    "            'num_params': num_params\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844071e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10-fold cross-validation...\n",
      "  Evaluating model: sigmoid\n",
      "  Fitting sigmoid on full data for AIC/BIC...\n",
      "  Evaluating model: clipped_linear\n",
      "  Fitting clipped_linear on full data for AIC/BIC...\n",
      "\n",
      "==================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "==================================================\n",
      "CV R2 / CV MSE are from 5-fold cross-validation (higher is better for R2, lowe r for MSE).\n",
      "AIC / BIC are from fitting on the full dataset (lower is better for both).\n",
      "--------------------------------------------------\n",
      "       model_type   cv_r2  cv_mse        aic        bic  num_params\n",
      "0         sigmoid  0.8641  0.0098 -6490.3797 -5177.7113         253\n",
      "1  clipped_linear  0.8683  0.0095 -6472.1491 -5159.4807         253\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "comparison_results = compare_models_cross_validation(\n",
    "    df=scores_df,\n",
    "    anchor_benchmark=anchor_benchmark,\n",
    "    anchor_difficulty=anchor_difficulty,\n",
    "    anchor_slope=anchor_slope\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"CV R2 / CV MSE are from 5-fold cross-validation (higher is better for R2, lowe r for MSE).\")\n",
    "print(\"AIC / BIC are from fitting on the full dataset (lower is better for both).\")\n",
    "print(\"-\" * 50)\n",
    "print(comparison_results.round(4))\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_stitching_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
